{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e386b9",
   "metadata": {},
   "source": [
    "# Training a Speech Tokenizer with WavLM and K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c4754",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73de2d",
   "metadata": {},
   "source": [
    "This notebook was created by [Jupyter AI](https://github.com/jupyterlab/jupyter-ai) with the following prompt:\n",
    "\n",
    "> /generate I want to train a speech tokenizer based on WavLM and k-means algorithm. The dataset is the LibriSpeech. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd82d20",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This Jupyter notebook details the step-by-step process of training a speech tokenizer utilizing the WavLM model and the k-means algorithm on the LibriSpeech dataset. It begins with the setup and installation of necessary libraries, followed by the loading and preprocessing of the dataset. The notebook then describes the extraction of audio features, the implementation of the k-means clustering algorithm, and the training of the tokenizer using these features. Subsequent sections evaluate the tokenizer's performance using various metrics and visualization techniques, save the trained model for future use, and demonstrate how to apply the tokenizer to new audio data, complete with examples and visualizations of the tokenization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd39a44",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b534c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a026b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torchaudio\n",
    "from torchaudio.datasets import LIBRISPEECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load the LibriSpeech dataset\n",
    "def load_librispeech_data(root_dir, url='train-clean-100', download=True):\n",
    "    \"\"\"\n",
    "    Load the LibriSpeech dataset from the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Parameters:\n",
    "    root_dir (str): The root directory where the dataset will be stored.\n",
    "    url (str): The specific dataset subset to load (e.g., 'train-clean-100').\n",
    "    download (bool): If True, download the dataset if not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Returns:\n",
    "    list: A list of tuples containing audio tensors and their corresponding transcripts.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = LIBRISPEECH(root=root_dir, url=url, download=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Prepare a list to hold the audio data and transcripts\n",
    "    data = []\n",
    "    \n",
    "    # Loop through the dataset and extract audio and transcript\n",
    "    for waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id in dataset:\n",
    "        data.append((waveform, sample_rate, utterance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3387c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe66b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root directory for the dataset\n",
    "root_directory = './librispeech_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058be4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "librispeech_data = load_librispeech_data(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of samples loaded and an example entry\n",
    "print(f\"Number of audio samples loaded: {len(librispeech_data)}\")\n",
    "print(\"Example entry (waveform shape, sample rate, transcript):\")\n",
    "print(librispeech_data[0])  # Display the first entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f459f2",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c69363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import WavLMModel, WavLMProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained WavLM model and processor\n",
    "model_name = \"microsoft/wavlm-base-960h\"\n",
    "processor = WavLMProcessor.from_pretrained(model_name)\n",
    "model = WavLMModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488546d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from audio files\n",
    "def extract_features(audio_file):\n",
    "    # Load the audio file\n",
    "    audio_input, sample_rate = torchaudio.load(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20241f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Resample if necessary (WavLM expects 16kHz)\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        audio_input = resampler(audio_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e80b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Process the audio input for WavLM\n",
    "    inputs = processor(audio_input.squeeze(0), sampling_rate=16000, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e68aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Move inputs to the same device as the model (CPU/GPU)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Extract features using WavLM\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        features = model(**inputs).last_hidden_state\n",
    "    \n",
    "    # Return the features\n",
    "    return features.squeeze(0)  # Remove the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c802311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Extract features from an audio file\n",
    "audio_file_path = \"path/to/your/audio/file.wav\"  # Replace with your audio file path\n",
    "features = extract_features(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eeaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the extracted features\n",
    "print(f\"Extracted features shape: {features.shape}\")  # Should print (sequence_length, feature_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4012dc5e",
   "metadata": {},
   "source": [
    "## 4. K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a975a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize centroids randomly from the dataset\n",
    "def initialize_centroids(X, k):\n",
    "    \"\"\"\n",
    "    Initialize k centroids randomly from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    X : np.ndarray\n",
    "        The input dataset of shape (n_samples, n_features).\n",
    "    k : int\n",
    "        The number of clusters (centroids) to initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Returns:\n",
    "    centroids : np.ndarray\n",
    "        Initialized centroids of shape (k, n_features).\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    random_indices = np.random.choice(n_samples, size=k, replace=False)\n",
    "    centroids = X[random_indices]\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign clusters based on the nearest centroid\n",
    "def assign_clusters(X, centroids):\n",
    "    \"\"\"\n",
    "    Assign each sample to the nearest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caacd6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Parameters:\n",
    "    X : np.ndarray\n",
    "        The input dataset of shape (n_samples, n_features).\n",
    "    centroids : np.ndarray\n",
    "        Current centroids of shape (k, n_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8226940",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Returns:\n",
    "    labels : np.ndarray\n",
    "        Array of cluster labels for each sample, shape (n_samples,).\n",
    "    \"\"\"\n",
    "    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)  # Compute distances to centroids\n",
    "    labels = np.argmin(distances, axis=1)  # Assign clusters based on the nearest centroid\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27348a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update centroids based on the assigned clusters\n",
    "def update_centroids(X, labels, k):\n",
    "    \"\"\"\n",
    "    Update centroids to be the mean of the samples assigned to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Parameters:\n",
    "    X : np.ndarray\n",
    "        The input dataset of shape (n_samples, n_features).\n",
    "    labels : np.ndarray\n",
    "        Array of cluster labels for each sample, shape (n_samples,).\n",
    "    k : int\n",
    "        The number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c253026",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Returns:\n",
    "    centroids : np.ndarray\n",
    "        Updated centroids of shape (k, n_features).\n",
    "    \"\"\"\n",
    "    centroids = np.zeros((k, X.shape[1]))  # Initialize centroids array\n",
    "    for i in range(k):\n",
    "        if np.any(labels == i):\n",
    "            centroids[i] = X[labels == i].mean(axis=0)  # Compute mean of the samples in each cluster\n",
    "        else:\n",
    "            centroids[i] = X[np.random.choice(X.shape[0])]  # Reinitialize centroid if cluster is empty\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d39f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main K-means algorithm function\n",
    "def kmeans(X, k, max_iters=100):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Parameters:\n",
    "    X : np.ndarray\n",
    "        The input dataset of shape (n_samples, n_features).\n",
    "    k : int\n",
    "        The number of clusters.\n",
    "    max_iters : int\n",
    "        The maximum number of iterations to run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13722b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Returns:\n",
    "    centroids : np.ndarray\n",
    "        Final centroids of shape (k, n_features).\n",
    "    labels : np.ndarray\n",
    "        Array of cluster labels for each sample, shape (n_samples,).\n",
    "    \"\"\"\n",
    "    centroids = initialize_centroids(X, k)  # Step 1: Initialize centroids\n",
    "    for _ in range(max_iters):\n",
    "        labels = assign_clusters(X, centroids)  # Step 2: Assign clusters\n",
    "        new_centroids = update_centroids(X, labels, k)  # Step 3: Update centroids\n",
    "        # Check for convergence: if centroids do not change, break the loop\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids  # Update centroids for next iteration\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df53cf",
   "metadata": {},
   "source": [
    "## 5. Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82172a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe09eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2003c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming features and labels are already prepared from previous steps\n",
    "# features: a tensor of shape (num_samples, feature_dim)\n",
    "# labels: the cluster labels obtained from k-means (as long tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab53dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901934a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset and DataLoader for training\n",
    "dataset = TensorDataset(features, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4581b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network for the tokenizer training\n",
    "class TokenizerNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_clusters):\n",
    "        super(TokenizerNN, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_clusters)  # Map features to cluster space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b172c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cbafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "num_clusters = labels.max().item() + 1  # Number of clusters from k-means\n",
    "input_dim = features.shape[1]  # Feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b2a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TokenizerNN(input_dim, num_clusters)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad63c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ac574",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for batch_features, batch_labels in dataloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(batch_features)  # Forward pass\n",
    "        loss = criterion(outputs, batch_labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item() * batch_features.size(0)  # Accumulate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a031bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "    epoch_loss = running_loss / len(dataset)  # Average loss for the epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'tokenizer_model.pth')\n",
    "print(\"Tokenizer model trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41b3f9",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0178176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c88549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `X_test` is the test feature set obtained from the tokenizer\n",
    "# and `y_test` is the true label set for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e907e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer's output on the test dataset\n",
    "# For example, this could be the output of the WavLM model\n",
    "# Here, we just assume `X_test` and `y_test` are already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7926ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering on the test dataset using k-means\n",
    "n_clusters = 10  # Set the number of clusters based on the training phase\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1268036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the clustering performance using accuracy\n",
    "# Map the predicted clusters to the true labels\n",
    "# Create a mapping from cluster labels to true labels\n",
    "def map_clusters_to_labels(y_true, y_pred):\n",
    "    mapping = {}\n",
    "    for label in np.unique(y_pred):\n",
    "        mask = (y_pred == label)\n",
    "        mapping[label] = mode(y_true[mask])[0][0]  # Most common true label in the cluster\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee36c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from predicted cluster labels to true labels\n",
    "label_mapping = map_clusters_to_labels(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map predicted labels to true labels\n",
    "y_pred_mapped = np.array([label_mapping[label] for label in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70566371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_mapped)\n",
    "print(f\"Accuracy of the tokenizer: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate silhouette score for clustering quality\n",
    "silhouette_avg = silhouette_score(X_test, y_pred)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e89d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of clustering results using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7876e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='viridis', alpha=0.5)\n",
    "plt.title('PCA of Test Set with K-Means Clustering')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a66090",
   "metadata": {},
   "source": [
    "## 7. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'trained_tokenizer.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f271909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(tokenizer, save_path):\n",
    "    \"\"\"\n",
    "    Saves the trained tokenizer model to a specified path using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "    tokenizer: The trained tokenizer model to be saved.\n",
    "    save_path: The file path where the model should be saved.\n",
    "    \"\"\"\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'tokenizer' is defined and is a trained tokenizer model\n",
    "if 'tokenizer' in locals():\n",
    "    save_model(tokenizer, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if os.path.exists(model_save_path):\n",
    "        print(\"Model saved successfully!\")\n",
    "    else:\n",
    "        print(\"Model saving failed.\")\n",
    "else:\n",
    "    print(\"Tokenizer model is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0431943",
   "metadata": {},
   "source": [
    "## 8. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Inference\n",
    "# This section demonstrates how to use the trained tokenizer on new audio data.\n",
    "# We will visualize the tokenization process using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd84265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained tokenizer model\n",
    "# Assuming 'tokenizer' is the trained tokenizer object and 'wavlm_model' is the WavLM model\n",
    "# (These should be defined in previous sections of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01edcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and define these variables in your actual code\n",
    "# tokenizer = ...  # Load your trained tokenizer\n",
    "# wavlm_model = ...  # Load your WavLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path):\n",
    "    \"\"\"Load an audio file and return the waveform and sample rate.\"\"\"\n",
    "    waveform, sample_rate = librosa.load(file_path, sr=None)  # Load audio file\n",
    "    return waveform, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f241418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_waveform(waveform, sample_rate):\n",
    "    \"\"\"Plot the audio waveform.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.waveshow(waveform, sr=sample_rate)\n",
    "    plt.title('Audio Waveform')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ab710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_audio(waveform):\n",
    "    \"\"\"Tokenize the audio waveform using the trained tokenizer.\"\"\"\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        input_tensor = torch.FloatTensor(waveform).unsqueeze(0)  # Add batch dimension\n",
    "        tokens = tokenizer(input_tensor)  # Tokenize the audio\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcc69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokens(tokens):\n",
    "    \"\"\"Visualize the tokenized output as a sequence.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.imshow(tokens.cpu().numpy(), aspect='auto', cmap='viridis')  # Ensure tokens are on CPU\n",
    "    plt.title('Tokenized Output')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Token Indices')\n",
    "    plt.colorbar(label='Token Value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71db22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "audio_file_path = 'path/to/new/audio/file.wav'  # Replace with the path to your audio file\n",
    "waveform, sample_rate = load_audio(audio_file_path)  # Load the new audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the waveform\n",
    "visualize_waveform(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad5091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the audio waveform\n",
    "tokens = tokenize_audio(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6845fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tokenization process\n",
    "visualize_tokens(tokens)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
