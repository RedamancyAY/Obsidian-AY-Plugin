{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52ff5cf-7be5-447d-946e-999212a148d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:28.740922Z",
     "iopub.status.busy": "2024-11-26T03:05:28.739589Z",
     "iopub.status.idle": "2024-11-26T03:05:28.781513Z",
     "shell.execute_reply": "2024-11-26T03:05:28.780154Z",
     "shell.execute_reply.started": "2024-11-26T03:05:28.740857Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5fe5336-2047-4df6-a4f1-1a18d02d979f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:28.785277Z",
     "iopub.status.busy": "2024-11-26T03:05:28.784062Z",
     "iopub.status.idle": "2024-11-26T03:05:30.472590Z",
     "shell.execute_reply": "2024-11-26T03:05:30.471685Z",
     "shell.execute_reply.started": "2024-11-26T03:05:28.785218Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from typing import Any, NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c8ced8c-3d71-4e5b-aed7-c37f7fbd65d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:30.473501Z",
     "iopub.status.busy": "2024-11-26T03:05:30.473283Z",
     "iopub.status.idle": "2024-11-26T03:05:30.569741Z",
     "shell.execute_reply": "2024-11-26T03:05:30.568870Z",
     "shell.execute_reply.started": "2024-11-26T03:05:30.473485Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5becef-67a0-4478-965e-93ba6c8986bd",
   "metadata": {},
   "source": [
    "# Load LibriSpeech dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9967e3-05ec-4563-a199-17f9d57df3a4",
   "metadata": {},
   "source": [
    "The LibriSpeech dataset is a popular resource in the field of speech recognition and is widely used for training and evaluating machine learning models. It consists of approximately 1,000 hours of English speech, derived from audiobooks that are part of the LibriVox project. The dataset is divided into different subsets based on the amount of data and the quality of the recordings, making it suitable for various tasks in automatic speech recognition (ASR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15439e5-f087-4cf5-8020-21653d7ab4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T01:39:00.716153Z",
     "iopub.status.busy": "2024-11-25T01:39:00.714616Z",
     "iopub.status.idle": "2024-11-25T01:39:00.764745Z",
     "shell.execute_reply": "2024-11-25T01:39:00.762980Z",
     "shell.execute_reply.started": "2024-11-25T01:39:00.716080Z"
    }
   },
   "source": [
    "Dataset Structure:\n",
    "- The dataset is organized into several parts:\n",
    "    - train-clean-100: 100 hours of clean speech for training.\n",
    "    - train-clean-360: 360 hours of clean speech for training.\n",
    "    - train-other-500: 500 hours of speech from various sources that may include background noise, making it less clean.\n",
    "    - dev-clean: A development set containing clean speech.\n",
    "    - dev-other: A development set containing more diverse and noisy speech.\n",
    "    - test-clean: A test set of clean speech.\n",
    "    - test-other: A test set with more diverse and noisy speech.\n",
    "- Audio Format:\n",
    "    - The audio files are typically in the `flac` format, sampled at 16 kHz, and stored as mono channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2bcbc-12d8-4cbd-8875-8195b758e46e",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "We use all the train data for training, not just a subset. Thus, **Total 960 hours** are used to train the Kmeans model.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299b4669-5de6-4fc9-ab20-7ba398b8ed5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:30.571210Z",
     "iopub.status.busy": "2024-11-26T03:05:30.571032Z",
     "iopub.status.idle": "2024-11-26T03:05:30.586073Z",
     "shell.execute_reply": "2024-11-26T03:05:30.585304Z",
     "shell.execute_reply.started": "2024-11-26T03:05:30.571194Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_root_path = \"/home/ay/data2/datasets/Lib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84cee9-aa56-40f5-8474-347c03aca92d",
   "metadata": {},
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d383272-af0a-4a45-aa00-acc3b19bbc74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:30.586760Z",
     "iopub.status.busy": "2024-11-26T03:05:30.586625Z",
     "iopub.status.idle": "2024-11-26T03:05:30.602830Z",
     "shell.execute_reply": "2024-11-26T03:05:30.602080Z",
     "shell.execute_reply.started": "2024-11-26T03:05:30.586746Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to pad audio waveforms for batching.\n",
    "\n",
    "    This function takes a batch of audio waveforms and their associated\n",
    "    metadata and pads the waveforms to ensure they all have the same length\n",
    "    for efficient processing in neural networks.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of tuples, where each tuple contains:\n",
    "            - waveform (Tensor): The audio waveform tensor.\n",
    "            - sample_rate (int): The sample rate of the audio.\n",
    "            - utterance (str): The transcription of the audio.\n",
    "            - speaker_id (int): The ID of the speaker.\n",
    "            - chapter_id (int): The ID of the chapter.\n",
    "            - file_id (int): The ID of the file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - padded_waveforms (Tensor): A tensor of padded audio waveforms.\n",
    "            - sample_rate (int): The sample rate of the audio.\n",
    "            - transcriptions (list): A list of transcriptions for the audio.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input batch is empty.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the maximum length of the waveforms in the batch\n",
    "    max_length = max(waveform.shape[1] for waveform, *_ in batch)\n",
    "\n",
    "    # Pad the waveforms and create a tensor for the batch\n",
    "    padded_waveforms = []\n",
    "    transcriptions = []\n",
    "\n",
    "    for waveform, sample_rate, utterance, speaker_id, chapter_id, file_id in batch:\n",
    "        # Pad the waveform with zeros\n",
    "        padded_waveform = torch.nn.functional.pad(waveform, (0, max_length - waveform.shape[1]))\n",
    "        padded_waveforms.append(padded_waveform)\n",
    "        transcriptions.append(utterance)\n",
    "\n",
    "    # Stack the waveforms into a single tensor\n",
    "    return torch.concat(padded_waveforms), sample_rate, transcriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204e8f5-ea70-46d2-a1a9-6e72d0955b95",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "We use a `percentage` of 960 hours for training.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d412af-8a34-41ba-9639-277a2a5e85db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:30.603539Z",
     "iopub.status.busy": "2024-11-26T03:05:30.603399Z",
     "iopub.status.idle": "2024-11-26T03:05:30.618769Z",
     "shell.execute_reply": "2024-11-26T03:05:30.618015Z",
     "shell.execute_reply.started": "2024-11-26T03:05:30.603525Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_librispeech_dataset(\n",
    "    root_path: str, split=\"train\", batch_size: int = 32, num_workers: int = 2, percentage=0.1\n",
    ") -> DataLoader:\n",
    "    \"\"\"Load the LibriSpeech dataset and return a DataLoader.\n",
    "\n",
    "    This function initializes the LibriSpeech dataset and creates a DataLoader\n",
    "    with a custom collate function to handle variable-length audio samples.\n",
    "\n",
    "    Args:\n",
    "        root_path (str): The path to the directory where the dataset will be stored.\n",
    "        split (str): train or val .\n",
    "        batch_size (int, optional): The number of samples per batch. Defaults to 32.\n",
    "        num_workers (int, optional): The number of subprocesses to use for data loading.\n",
    "            Defaults to 2.\n",
    "        percentage (float, optional): The percentage that used to generate features\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A DataLoader object that provides batches of audio waveforms\n",
    "        and their corresponding metadata from the LibriSpeech dataset.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the dataset cannot be downloaded or accessed.\n",
    "    \"\"\"\n",
    "\n",
    "    if split == \"train\":\n",
    "        # Load individual datasets\n",
    "        train_clean_100 = torchaudio.datasets.LIBRISPEECH(root=root_path, url=\"train-clean-100\", download=True)\n",
    "        train_clean_360 = torchaudio.datasets.LIBRISPEECH(root=root_path, url=\"train-clean-360\", download=True)\n",
    "        train_other_500 = torchaudio.datasets.LIBRISPEECH(root=root_path, url=\"train-other-500\", download=True)\n",
    "        # Combine the datasets\n",
    "        dataset = ConcatDataset([train_clean_100, train_clean_360, train_other_500])\n",
    "    else:\n",
    "        dev_clean = torchaudio.datasets.LIBRISPEECH(root=root_path, url=\"dev-clean\", download=True)\n",
    "        dev_other = torchaudio.datasets.LIBRISPEECH(root=root_path, url=\"dev-other\", download=True)\n",
    "        dataset = ConcatDataset([dev_clean, dev_other])\n",
    "\n",
    "    # Randomly select 10% of the dataset,  and then create a Subset using the selected indices\n",
    "    subset_size = int(len(dataset) * percentage)\n",
    "    indices = torch.randperm(len(dataset))[:subset_size]\n",
    "    dataset = Subset(dataset, indices)\n",
    "\n",
    "    # Create a DataLoader with the custom collate function\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37fb41-4826-4091-8ea2-5520237a6dad",
   "metadata": {},
   "source": [
    "## Load dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116f9140-0814-45b5-ba1e-7ba01dca5247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:30.619330Z",
     "iopub.status.busy": "2024-11-26T03:05:30.619190Z",
     "iopub.status.idle": "2024-11-26T03:05:30.634310Z",
     "shell.execute_reply": "2024-11-26T03:05:30.633546Z",
     "shell.execute_reply.started": "2024-11-26T03:05:30.619316Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataloader = load_librispeech_dataset(dataset_root_path, split=\"train\", percentage=0.2, num_workers=4)\n",
    "\n",
    "# # Iterate through the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     waveform, sample_rate, utterance = batch\n",
    "#     print(f\"Waveform shape: {waveform.shape}, Sample Rate: {sample_rate}\")\n",
    "#     # print(f\"Utterance: {utterance}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9809e1a-dd1b-414c-9cf1-4d3d4a8592ba",
   "metadata": {},
   "source": [
    "# WavLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9e800-c038-4c5c-8198-8b4f94799c23",
   "metadata": {},
   "source": [
    "## Intro of WavLM\n",
    "WavLM is a state-of-the-art model introduced for automatic speech recognition (ASR) and other speech-related tasks. Developed by researchers at Microsoft, WavLM builds upon the success of prior models like Wav2Vec 2.0, enhancing the ability to learn representations from raw audio and improving performance across various speech processing applications.\n",
    "\n",
    "### Key Features of WavLM\n",
    "\n",
    "1. **Architecture**:\n",
    "   - WavLM utilizes a transformer-based architecture similar to Wav2Vec 2.0 but incorporates several innovations to handle speech signals more effectively.\n",
    "   - It employs a feature extraction layer that processes raw audio inputs into a more manageable format for the transformer layers.\n",
    "\n",
    "2. **Self-Supervised Learning**:\n",
    "   - WavLM is trained using self-supervised learning techniques, which allow it to learn from unlabeled data. This is particularly beneficial in scenarios where labeled data is scarce.\n",
    "   - The model is pre-trained on a large corpus of unlabeled audio, enabling it to capture various acoustic characteristics and linguistic features.\n",
    "\n",
    "3. **Multi-Task Learning**:\n",
    "   - WavLM is designed to perform multiple tasks, including speech recognition, speaker identification, and emotion recognition, making it versatile for different applications.\n",
    "   - The model can be fine-tuned on specific tasks, allowing it to adapt to various domains and improve its performance.\n",
    "\n",
    "4. **Robustness**:\n",
    "   - One of the significant advancements of WavLM is its robustness to different acoustic conditions and noise levels, making it suitable for real-world applications where audio quality may vary.\n",
    "\n",
    "5. **Fine-Tuning Capabilities**:\n",
    "   - After pre-training, WavLM can be fine-tuned on specific datasets, allowing it to achieve high accuracy on specific tasks like ASR or speaker verification.\n",
    "   - Fine-tuning can involve supervised data, enabling the model to adapt to the nuances of the target domain effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad899f73-607c-4a36-9fb7-d74173f14905",
   "metadata": {},
   "source": [
    "\n",
    "We use the [WavLMModel](https://github.com/huggingface/transformers/blob/052e652d6d53c2b26ffde87e039b723949a53493/src/transformers/models/wavlm/modeling_wavlm.py#L1105) to extract audio features and then train the Kmeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ad4fb6-2393-44e0-84d6-f41a57ac11f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:30.635037Z",
     "iopub.status.busy": "2024-11-26T03:05:30.634902Z",
     "iopub.status.idle": "2024-11-26T03:05:31.395656Z",
     "shell.execute_reply": "2024-11-26T03:05:31.394623Z",
     "shell.execute_reply.started": "2024-11-26T03:05:30.635023Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import WavLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c01bee-6a91-4415-91be-9885278e08d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:31.396942Z",
     "iopub.status.busy": "2024-11-26T03:05:31.396758Z",
     "iopub.status.idle": "2024-11-26T03:05:31.421486Z",
     "shell.execute_reply": "2024-11-26T03:05:31.420635Z",
     "shell.execute_reply.started": "2024-11-26T03:05:31.396923Z"
    }
   },
   "outputs": [],
   "source": [
    "class WavLM(nn.Module):\n",
    "    \"\"\"\n",
    "    https://huggingface.co/docs/transformers/model_doc/wavlm#transformers.WavLMForCTC\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_name):\n",
    "        super().__init__()\n",
    "        print(\"Load WavLM model!!!!!!!\")\n",
    "        self.model = WavLMModel.from_pretrained(pretrained_name)\n",
    "        self.model.lm_head = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.model(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c81c1f4a-5847-41c5-970a-5b64fb8673a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:31.423494Z",
     "iopub.status.busy": "2024-11-26T03:05:31.423311Z",
     "iopub.status.idle": "2024-11-26T03:05:31.816640Z",
     "shell.execute_reply": "2024-11-26T03:05:31.815724Z",
     "shell.execute_reply.started": "2024-11-26T03:05:31.423478Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at /home/ay/.cache/huggingface/hub/models--microsoft--wavlm-base/snapshots/efa81aae7ff777e464159e0f877d54eac5b84f81/ were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at /home/ay/.cache/huggingface/hub/models--microsoft--wavlm-base/snapshots/efa81aae7ff777e464159e0f877d54eac5b84f81/ and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load WavLM model!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "pretrained_name = (\n",
    "    \"/home/ay/.cache/huggingface/hub/models--microsoft--wavlm-base\"\n",
    "    \"/snapshots/efa81aae7ff777e464159e0f877d54eac5b84f81/\"\n",
    ")\n",
    "\n",
    "model = WavLM(pretrained_name).cuda(0)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0922cb0f-9447-4608-b23c-345fe3df6079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:31.817697Z",
     "iopub.status.busy": "2024-11-26T03:05:31.817458Z",
     "iopub.status.idle": "2024-11-26T03:05:31.837879Z",
     "shell.execute_reply": "2024-11-26T03:05:31.837073Z",
     "shell.execute_reply.started": "2024-11-26T03:05:31.817680Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     res = model.model(waveform.to(next(model.parameters()).device))\n",
    "# res.hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2aad3-97cd-429d-986c-695671be23c1",
   "metadata": {},
   "source": [
    "## Load Validation Split and obtain val features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fef05bf-c0dc-48aa-a845-91f25eb1d1b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:05:31.838703Z",
     "iopub.status.busy": "2024-11-26T03:05:31.838550Z",
     "iopub.status.idle": "2024-11-26T03:06:53.736705Z",
     "shell.execute_reply": "2024-11-26T03:06:53.735827Z",
     "shell.execute_reply.started": "2024-11-26T03:05:31.838688Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Audio: 100%|██████████████████████████████████████████████████████████████████████████████| 348/348 [01:15<00:00,  4.60it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataloader = load_librispeech_dataset(dataset_root_path, split=\"val\", percentage=1.0, num_workers=4, batch_size=16)\n",
    "val_feat = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(val_dataloader, desc=\"Processing Audio\")):\n",
    "        waveform, sample_rate, utterance = batch\n",
    "        outputs = model(waveform.cuda(0))\n",
    "        last_hidden_states = outputs.last_hidden_state.reshape(\n",
    "            -1, outputs.last_hidden_state.shape[-1]\n",
    "        )  # [batch, frames, features]\n",
    "        x = last_hidden_states.cpu().numpy()\n",
    "        val_feat.append(x)\n",
    "\n",
    "val_feat = np.concatenate(val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abc31035-efe0-49b2-87ed-7c36a74c23d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:06:53.738266Z",
     "iopub.status.busy": "2024-11-26T03:06:53.738075Z",
     "iopub.status.idle": "2024-11-26T03:06:53.762989Z",
     "shell.execute_reply": "2024-11-26T03:06:53.762220Z",
     "shell.execute_reply.started": "2024-11-26T03:06:53.738249Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate_kmeans(kmeans, features, metric=\"inertia\"):\n",
    "    \"\"\"\n",
    "    Validate a trained KMeans model using features from a validation DataLoader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pretrained feature extraction model (e.g., WavLM).\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
    "        processor: Processor for preprocessing input data for the model.\n",
    "        kmeans (MiniBatchKMeans): Trained MiniBatchKMeans model.\n",
    "        metric (str): Metric to compute. Options: \"inertia\" (default) or \"silhouette\".\n",
    "\n",
    "    Returns:\n",
    "        float: Computed evaluation metric (e.g., inertia or silhouette score).\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.metrics import silhouette_score\n",
    "\n",
    "    features_list = []\n",
    "    cluster_assignments = []\n",
    "\n",
    "    # Predict cluster assignments\n",
    "    cluster_labels = kmeans.predict(features)\n",
    "\n",
    "    # Compute the specified metric\n",
    "    if metric == \"inertia\":\n",
    "        # Inertia: Sum of squared distances of samples to their closest cluster center\n",
    "        return kmeans.inertia_\n",
    "\n",
    "    elif metric == \"silhouette\":\n",
    "        # Silhouette score: Measures the separation between clusters\n",
    "        return silhouette_score(features, cluster_labels)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric '{metric}'. Supported metrics: 'inertia', 'silhouette'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b2a9dc2-e42c-4e42-95a3-9051765804f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:06:53.763780Z",
     "iopub.status.busy": "2024-11-26T03:06:53.763641Z",
     "iopub.status.idle": "2024-11-26T03:06:53.786982Z",
     "shell.execute_reply": "2024-11-26T03:06:53.786036Z",
     "shell.execute_reply.started": "2024-11-26T03:06:53.763765Z"
    }
   },
   "outputs": [],
   "source": [
    "# validate_kmeans(kmeans_model, val_feat, metric=\"inertia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c26166-0ee6-4c37-9087-c1b2e302f2cd",
   "metadata": {},
   "source": [
    "# Train Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73652cd4-4b91-4672-9d5a-054b8a298d76",
   "metadata": {},
   "source": [
    "## Introduction of Kmeans\n",
    "MiniBatchKMeans is a variant of the traditional K-Means clustering algorithm that is designed to handle large datasets more efficiently. It was introduced to overcome some of the scalability issues associated with standard K-Means, particularly when dealing with very large datasets that may not fit into memory. Here's an overview of its key features, advantages, and applications:\n",
    "\n",
    "### Key Features of MiniBatchKMeans\n",
    "\n",
    "1. **Batch Processing**:\n",
    "   - Unlike the standard K-Means algorithm, which uses the entire dataset to update the cluster centroids in each iteration, MiniBatchKMeans processes small, random subsets (mini-batches) of the data.\n",
    "   - This approach significantly reduces the computational burden and memory requirements, allowing the algorithm to scale to larger datasets.\n",
    "\n",
    "2. **Algorithm Steps**:\n",
    "   - **Initialization**: Similar to K-Means, the algorithm starts by randomly initializing the centroids of the clusters.\n",
    "   - **Mini-Batch Selection**: In each iteration, a small random sample of the data is selected as a mini-batch.\n",
    "   - **Centroid Update**: The centroids are updated based on the mini-batch rather than the entire dataset, which leads to faster convergence.\n",
    "   - **Iteration**: This process is repeated for a specified number of iterations or until the centroids converge to stable values.\n",
    "\n",
    "3. **Reduced Memory Usage**:\n",
    "   - Because MiniBatchKMeans only needs to store and process the mini-batch at any given time, it is more memory-efficient compared to the standard K-Means approach.\n",
    "\n",
    "4. **Faster Convergence**:\n",
    "   - The use of mini-batches leads to quicker updates to the centroids, often resulting in faster convergence times, especially for large datasets.\n",
    "\n",
    "5. **Flexibility**:\n",
    "   - MiniBatchKMeans allows for the adjustment of the mini-batch size, which can be tuned based on the specific requirements of the dataset and application.\n",
    "\n",
    "### Advantages of MiniBatchKMeans\n",
    "\n",
    "- **Scalability**: It is particularly suited for applications where the dataset is too large to fit into memory, allowing clustering on large-scale data.\n",
    "- **Speed**: The algorithm runs faster than traditional K-Means, especially with large datasets, making it suitable for real-time applications.\n",
    "- **Reduced Variance**: By using random mini-batches, the algorithm can help reduce the variance in the updates, which can lead to a more stable convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "572affe9-f1fd-4411-a886-8d053f7f6706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:06:53.787711Z",
     "iopub.status.busy": "2024-11-26T03:06:53.787561Z",
     "iopub.status.idle": "2024-11-26T03:06:54.279376Z",
     "shell.execute_reply": "2024-11-26T03:06:54.278420Z",
     "shell.execute_reply.started": "2024-11-26T03:06:53.787695Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import joblib\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5b40a9-11a5-46b2-a251-3a5ceee15437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:06:54.280573Z",
     "iopub.status.busy": "2024-11-26T03:06:54.280264Z",
     "iopub.status.idle": "2024-11-26T03:06:54.309283Z",
     "shell.execute_reply": "2024-11-26T03:06:54.308444Z",
     "shell.execute_reply.started": "2024-11-26T03:06:54.280555Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_kmeans_model(args: Namespace) -> MiniBatchKMeans:\n",
    "    \"\"\"Create and return a MiniBatch KMeans clustering model.\n",
    "\n",
    "    This function initializes a MiniBatch KMeans model with the specified\n",
    "    parameters from an args.Namespace object, which is suitable for large\n",
    "    datasets and can handle mini-batch updates for efficiency.\n",
    "\n",
    "    Args:\n",
    "        args (Namespace): A Namespace object containing the following parameters:\n",
    "            - n_clusters (int): The number of clusters to form.\n",
    "            - init (str): Method for initialization.\n",
    "            - max_iter (int): Maximum number of iterations for a single run.\n",
    "            - batch_size (int): Size of the mini-batches.\n",
    "            - tol (float): Relative tolerance with regards to the inertia to declare convergence.\n",
    "            - max_no_improvement (int): Maximum number of consecutive iterations with no improvement in the inertia.\n",
    "            - n_init (int): Number of time the k-means algorithm will be run with different centroid seeds.\n",
    "            - reassignment_ratio (float): The proportion of the previous cluster centers that must change to consider a clustering solution stable.\n",
    "            - random_state (int): Random seed for initialization.\n",
    "\n",
    "    Returns:\n",
    "        MiniBatchKMeans: An initialized MiniBatchKMeans model ready for fitting to data.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the input parameters are invalid or inconsistent.\n",
    "    \"\"\"\n",
    "\n",
    "    return MiniBatchKMeans(\n",
    "        n_clusters=args.n_clusters,\n",
    "        init=args.init,\n",
    "        max_iter=args.max_iter,\n",
    "        batch_size=args.batch_size,\n",
    "        tol=args.tol,\n",
    "        max_no_improvement=args.max_no_improvement,\n",
    "        n_init=args.n_init,\n",
    "        reassignment_ratio=args.reassignment_ratio,\n",
    "        random_state=args.random_state,\n",
    "        verbose=0,\n",
    "        compute_labels=True,\n",
    "        init_size=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7623718f-c00a-4a69-9216-0319f8638f2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:06:54.310086Z",
     "iopub.status.busy": "2024-11-26T03:06:54.309936Z",
     "iopub.status.idle": "2024-11-26T03:06:54.334400Z",
     "shell.execute_reply": "2024-11-26T03:06:54.333290Z",
     "shell.execute_reply.started": "2024-11-26T03:06:54.310070Z"
    }
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    n_clusters=200,\n",
    "    init=\"k-means++\",\n",
    "    max_iter=150,\n",
    "    batch_size=1000,\n",
    "    tol=0.0001,\n",
    "    max_no_improvement=100,\n",
    "    n_init=20,\n",
    "    reassignment_ratio=0.5,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e62f7c4-da95-4149-88e2-de8fda4184e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:06:54.335235Z",
     "iopub.status.busy": "2024-11-26T03:06:54.335085Z",
     "iopub.status.idle": "2024-11-26T03:06:55.073156Z",
     "shell.execute_reply": "2024-11-26T03:06:55.072323Z",
     "shell.execute_reply.started": "2024-11-26T03:06:54.335220Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = load_librispeech_dataset(dataset_root_path, split=\"train\", percentage=0.5, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7993c3b-0d6e-4430-9f0f-9b6ae0902217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T03:06:55.074222Z",
     "iopub.status.busy": "2024-11-26T03:06:55.074030Z",
     "iopub.status.idle": "2024-11-26T03:06:55.108413Z",
     "shell.execute_reply": "2024-11-26T03:06:55.107581Z",
     "shell.execute_reply.started": "2024-11-26T03:06:55.074204Z"
    }
   },
   "outputs": [],
   "source": [
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f6679-f5ef-4a69-acb9-fc47857345aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.add(f\"kmeans2.log\", rotation=\"5 MB\", retention=\"7 days\", level=\"DEBUG\")  # Save to a file with rotation\n",
    "\n",
    "\n",
    "for n_clusters in [200, 250]:\n",
    "# for n_clusters in [50]:\n",
    "    args.n_clusters = n_clusters\n",
    "    kmeans_model = get_kmeans_model(args)\n",
    "    n_epochs = 10\n",
    "    best_score = 100000000000000000\n",
    "    tol = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for e in range(n_epochs):\n",
    "            for i, batch in enumerate(tqdm(dataloader, desc=\"Processing Audio\")):\n",
    "                waveform, sample_rate, utterance = batch\n",
    "                outputs = model(waveform.cuda(0))\n",
    "                last_hidden_states = outputs.last_hidden_state.reshape(\n",
    "                    -1, outputs.last_hidden_state.shape[-1]\n",
    "                )  # [batch, frames, features]\n",
    "                x = last_hidden_states.cpu().numpy()\n",
    "                kmeans_model.partial_fit(x)\n",
    "                score = -kmeans_model.score(x) / len(x)\n",
    "                if i % 500 == 0:\n",
    "                    logger.info(f\"{n_clusters}, Epoch {e}, current score: {kmeans_model.inertia_}\")\n",
    "                \n",
    "            \n",
    "            val_score = validate_kmeans(kmeans_model, val_feat, metric=\"inertia\")\n",
    "            logger.info(f\"{n_clusters}, Epoch {e}, val score: {val_score}\")\n",
    "            \n",
    "            if val_score < best_score:\n",
    "                best_score = val_score\n",
    "                tol = 0\n",
    "                joblib.dump(kmeans_model, os.path.join(dataset_root_path, f\"kmeans_model-{n_clusters}.pkl\"))\n",
    "            else:\n",
    "                tol += 1\n",
    "                if tol >= 2:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b76bc4-c5bb-4d76-88bb-5d4be079cef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
