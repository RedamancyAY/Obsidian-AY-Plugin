{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad68866-6284-48a6-b5a6-1d5322c0b171",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-07-11T07:03:47.759714Z",
     "iopub.status.busy": "2023-07-11T07:03:47.759143Z",
     "iopub.status.idle": "2023-07-11T07:03:47.786739Z",
     "shell.execute_reply": "2023-07-11T07:03:47.785549Z",
     "shell.execute_reply.started": "2023-07-11T07:03:47.759668Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a8c335-a8d5-4953-a5a7-9a3b2b69fbc0",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-07-11T07:03:48.606048Z",
     "iopub.status.busy": "2023-07-11T07:03:48.605572Z",
     "iopub.status.idle": "2023-07-11T07:03:49.705130Z",
     "shell.execute_reply": "2023-07-11T07:03:49.703895Z",
     "shell.execute_reply.started": "2023-07-11T07:03:48.606004Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Common preprocessing functions for audio data.\"\"\"\n",
    "import functools\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.functional import apply_codec\n",
    "import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef34468-7e74-4088-93e5-707aa6392658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:03:50.564593Z",
     "iopub.status.busy": "2023-07-11T07:03:50.563236Z",
     "iopub.status.idle": "2023-07-11T07:03:50.595010Z",
     "shell.execute_reply": "2023-07-11T07:03:50.593960Z",
     "shell.execute_reply.started": "2023-07-11T07:03:50.564542Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Optional, Tuple, Union\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b68559-128e-4d66-9717-8d4e9e1634a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:03:51.392559Z",
     "iopub.status.busy": "2023-07-11T07:03:51.391405Z",
     "iopub.status.idle": "2023-07-11T07:03:51.424527Z",
     "shell.execute_reply": "2023-07-11T07:03:51.423346Z",
     "shell.execute_reply.started": "2023-07-11T07:03:51.392512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from .dataset import BaseDataset\n",
    "from .utils import ioa_with_anchors, iou_with_anchors, padding_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0d8882-ace7-41e9-809e-2bf9d6605195",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:03:53.204553Z",
     "iopub.status.busy": "2023-07-11T07:03:53.204061Z",
     "iopub.status.idle": "2023-07-11T07:03:53.245418Z",
     "shell.execute_reply": "2023-07-11T07:03:53.244684Z",
     "shell.execute_reply.started": "2023-07-11T07:03:53.204509Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Localization_DS(BaseDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        sample_rate: int = 16_000,\n",
    "        normalize: bool = True,\n",
    "        trim: bool = False,\n",
    "        # custome args\n",
    "        max_wave_length: int = 16_000 * 20,\n",
    "        transform=torch.nn.Identity(),\n",
    "        is_training=False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            data=data, sample_rate=sample_rate, normalize=normalize, trim=trim\n",
    "        )\n",
    "\n",
    "        self.data = data\n",
    "        self.max_wave_length = max_wave_length\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        self.max_duration = 64\n",
    "        \n",
    "        \n",
    "        \n",
    "    def read_metadata(self, index: int) -> dict:\n",
    "        item = self.data.iloc[index]\n",
    "        keys = item.keys()\n",
    "        res = {\"sample_rate\": self.sample_rate}\n",
    "\n",
    "        label_path = item[\"path\"].replace(\".wav\", \".npz\")\n",
    "        if os.path.exists(label_path):\n",
    "            label = np.load(label_path)\n",
    "            bm_label = torch.from_numpy(label['bm_label'])\n",
    "            frame_label = torch.from_numpy(label['frame_label'])\n",
    "        else:\n",
    "            bm_label, frame_label = self.gen_label(index)\n",
    "\n",
    "        res['bm_label'] = bm_label\n",
    "        res['frame_label'] = frame_label\n",
    "        res['frames'] = int(item[\"audio_frames\"] / 16000 * 40)\n",
    "        res[\"name\"] = item[\"file\"]\n",
    "        res['fake_periods'] = item['fake_periods']\n",
    "        return res\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        waveform = self.read_audio(index)\n",
    "        waveform = padding_audio(waveform, target=self.max_wave_length)\n",
    "        waveform = self.transform(waveform)\n",
    "\n",
    "        waveform = self._get_log_mel_spectrogram(waveform)\n",
    "        \n",
    "        res = self.read_metadata(index)\n",
    "        res[\"audio\"] = waveform\n",
    "        return res\n",
    "\n",
    "    def _get_log_mel_spectrogram(self, audio: Tensor) -> Tensor:\n",
    "        ms = torchaudio.transforms.MelSpectrogram(n_fft=201, n_mels=64)\n",
    "        spec = torch.log(ms(audio[0, :]) + 0.01)\n",
    "        assert spec.shape == (64, 3200), \"Wrong log mel-spectrogram setup in Dataset\"\n",
    "        return spec\n",
    "\n",
    "    \n",
    "    def _get_audio_label(\n",
    "        self, audio_length, fake_periods\n",
    "    ) -> Union[Tensor, Tuple[Tensor, Tensor, Tensor]]:\n",
    "        corrected_second = audio_length / self.sample_rate  # number of audio seconds\n",
    "        audio_frames = int(\n",
    "            audio_length / 16000 * 40\n",
    "        )  # number of audio clips (25ms per clip, thus 40 frames/second)\n",
    "        temporal_gap = 1 / audio_frames\n",
    "\n",
    "        #############################################################################\n",
    "        # change the measurement from second to percentage\n",
    "        gt_bbox = []\n",
    "        for j in range(len(fake_periods)):\n",
    "            tmp_start = max(min(1, fake_periods[j][0] / corrected_second), 0)\n",
    "            tmp_end = max(min(1, fake_periods[j][1] / corrected_second), 0)\n",
    "            gt_bbox.append([tmp_start, tmp_end])\n",
    "\n",
    "        ###########################################################################\n",
    "        # generate R_s and R_e\n",
    "        gt_bbox = torch.tensor(gt_bbox)\n",
    "        if len(gt_bbox) > 0:\n",
    "            gt_xmins = gt_bbox[:, 0]\n",
    "            gt_xmaxs = gt_bbox[:, 1]\n",
    "        else:\n",
    "            gt_xmins = np.array([])\n",
    "            gt_xmaxs = np.array([])\n",
    "\n",
    "        ###########################################################################\n",
    "\n",
    "        gt_iou_map = torch.zeros([self.max_duration, audio_frames])\n",
    "\n",
    "        if len(gt_bbox) > 0:\n",
    "            for begin in range(audio_frames):\n",
    "                for duration in range(self.max_duration):\n",
    "                    end = begin + duration\n",
    "                    if end > audio_frames:\n",
    "                        break\n",
    "                    gt_iou_map[duration, begin] = torch.max(\n",
    "                        iou_with_anchors(\n",
    "                            begin * temporal_gap,\n",
    "                            (end + 1) * temporal_gap,\n",
    "                            gt_xmins,\n",
    "                            gt_xmaxs,\n",
    "                        )\n",
    "                    )\n",
    "                    # [i, j]: Start in i, end in j.\n",
    "\n",
    "        ############################################################################\n",
    "        max_wave_frames = int(self.max_wave_length / 16000 * 40)\n",
    "        gt_iou_map = F.pad(\n",
    "            gt_iou_map.float(),\n",
    "            pad=[0, max_wave_frames - audio_frames, 0, 0],\n",
    "        )\n",
    "        \n",
    "        \n",
    "        bm_label = gt_iou_map\n",
    "        frame_label = torch.ones(max_wave_frames)\n",
    "        for begin, end in fake_periods:\n",
    "            begin = int(begin * 40)\n",
    "            end = int(end * 40)\n",
    "            frame_label[begin: end] = 0\n",
    "        \n",
    "        return bm_label, frame_label\n",
    "    \n",
    "    def gen_label(self, index:int, overwrite=False)->Tuple[Tensor, Tensor]:\n",
    "        item = self.data.iloc[index]\n",
    "        label_path = item[\"path\"].replace(\".wav\", \".npz\")\n",
    "        if not overwrite and os.path.exists(label_path):\n",
    "            label = np.load(label_path)\n",
    "            bm_label = torch.from_numpy(label['bm_label'])\n",
    "            frame_label = torch.from_numpy(label['frame_label'])\n",
    "            return bm_label, frame_label\n",
    "        \n",
    "        bm_label, frame_label = self._get_audio_label(\n",
    "                item[\"audio_frames\"], fake_periods=item[\"fake_periods\"]\n",
    "            )\n",
    "        np.savez_compressed(label_path, bm_label=bm_label, frame_label=frame_label)\n",
    "        return bm_label, frame_label\n",
    "    \n",
    "    \n",
    "    def gen_labels(self, overwrite=False):\n",
    "        from pandarallel import pandarallel\n",
    "\n",
    "        pandarallel.initialize(progress_bar=True, nb_workers=15)\n",
    "        data = pd.DataFrame()\n",
    "        data['id'] = list(range(len(self.data)))\n",
    "        data['id'].parallel_apply(lambda x: self.gen_label(x, overwrite=overwrite))\n",
    "        # for _id in tqdm(range(len(self.data))):\n",
    "            # self.gen_label(_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30694e-91be-4000-813c-c7df9c231c1e",
   "metadata": {},
   "source": [
    "`gt_iou_map[duration, begin]` 表示 以 `begin`作为帧数的开始，以`begin + duration`作为帧数的结束，这个范围里帧和真实fake区间的IoU分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff1a80-6f83-4595-8967-9a3926387e58",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-07-11T07:04:29.725993Z",
     "iopub.status.busy": "2023-07-11T07:04:29.725461Z",
     "iopub.status.idle": "2023-07-11T07:04:29.766492Z",
     "shell.execute_reply": "2023-07-11T07:04:29.765735Z",
     "shell.execute_reply.started": "2023-07-11T07:04:29.725947Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/ay/zky/Coding/0-Audio/data/datasets\")\n",
    "\n",
    "from LAV_DF import LAV_DF_Audio\n",
    "\n",
    "lav_df = LAV_DF_Audio(root_path=\"/home/ay/data/0-原始数据集/LAV-DF-Audio\")\n",
    "datas = lav_df.get_splits()\n",
    "\n",
    "ds = LAVDF(datas.val)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
