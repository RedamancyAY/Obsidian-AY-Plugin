{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22f5e04-839d-4db7-9a24-702445f8d907",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-02-07T01:12:14.015605Z",
     "iopub.status.busy": "2024-02-07T01:12:14.015017Z",
     "iopub.status.idle": "2024-02-07T01:12:15.974591Z",
     "shell.execute_reply": "2024-02-07T01:12:15.973287Z",
     "shell.execute_reply.started": "2024-02-07T01:12:14.015538Z"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Common preprocessing functions for audio data.\"\"\"\n",
    "import functools\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.functional import apply_codec\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "SOX_SILENCE = [\n",
    "    # trim all silence that is longer than 0.2s and louder than 1% volume (relative to the file)\n",
    "    # from beginning and middle/end\n",
    "    [\"silence\", \"1\", \"0.2\", \"1%\", \"-1\", \"0.2\", \"1%\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f357204-19dd-4ddb-849b-2f9bf4522deb",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871a3960-f714-4461-a5d3-032caf94c599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T02:00:32.152877Z",
     "iopub.status.busy": "2023-08-11T02:00:32.152173Z",
     "iopub.status.idle": "2023-08-11T02:00:32.173626Z",
     "shell.execute_reply": "2023-08-11T02:00:32.172340Z",
     "shell.execute_reply.started": "2023-08-11T02:00:32.152801Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Torch dataset to load data from a provided directory.\n",
    "\n",
    "    Args:\n",
    "        data: A pandas dataframe, whose 'path' column is the path for each audio file.\n",
    "        sample_rate: The used sample rate for the audio\n",
    "        amount: default None. If not none, it means the number of used audio\n",
    "        normalize: default True.\n",
    "        trim: default True. trim all silence that is longer than 0.2s and louder than 1% volume\n",
    "        phone_call: default False.\n",
    "    Raises:\n",
    "        IOError: If the directory does ot exists or the directory did not contain any wav files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        sample_rate: int = 16_000,\n",
    "        normalize: bool = True,\n",
    "        trim: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.sample_rate = sample_rate\n",
    "        self.normalize = normalize\n",
    "        self.trim = trim\n",
    "\n",
    "    def read_metadata(self, index: int) -> dict:\n",
    "        item = self.data.iloc[index]\n",
    "        keys = item.keys()\n",
    "        res = {\"sample_rate\": self.sample_rate}\n",
    "        if \"label\" in keys:\n",
    "            res[\"label\"] = item[\"label\"]\n",
    "        if \"name\" in keys:\n",
    "            res[\"name\"] = item[\"name\"]\n",
    "        else:\n",
    "            res[\"name\"] = item[\"audio_path\"]\n",
    "        if \"vocoder_label\" in keys:\n",
    "            res[\"vocoder_label\"] = item[\"vocoder_label\"]\n",
    "        else:\n",
    "            res[\"vocoder_label\"] = 0\n",
    "\n",
    "        res[\"speed\"] = 0\n",
    "\n",
    "        if \"emotion_label\" in keys:\n",
    "            res[\"emotion_label\"] = item[\"emotion_label\"]\n",
    "        return res\n",
    "\n",
    "    def read_audio(self, index: int) -> Tuple[torch.Tensor, int, int]:\n",
    "        item = self.data.iloc[index]\n",
    "\n",
    "        path = item[\"audio_path\"]\n",
    "        fps = item[\"audio_fps\"]\n",
    "\n",
    "        # read audio ath self.sampling_rate\n",
    "        # if fps != self.sample_rate:\n",
    "        #     waveform, sample_rate = torchaudio.sox_effects.apply_effects_file(\n",
    "        #         path, [[\"rate\", f\"{self.sample_rate}\"]], normalize=self.normalize\n",
    "        #     )\n",
    "        # else:\n",
    "        #     waveform, sample_rate = torchaudio.load(path, normalize=self.normalize)\n",
    "\n",
    "        waveform, sample_rate = librosa.load(path, sr=self.sample_rate)\n",
    "\n",
    "        # trim the salience of audio\n",
    "        # if self.trim:\n",
    "        #     (\n",
    "        #         waveform_trimmed,\n",
    "        #         sample_rate_trimmed,\n",
    "        #     ) = torchaudio.sox_effects.apply_effects_tensor(\n",
    "        #         waveform, sample_rate, SOX_SILENCE\n",
    "        #     )\n",
    "\n",
    "        #     if waveform_trimmed.size()[1] > 0:\n",
    "        #         waveform = waveform_trimmed\n",
    "        #         sample_rate = sample_rate_trimmed\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        waveform = self.read_audio(index)\n",
    "        res = self.read_metadata(index)\n",
    "        res[\"audio\"] = waveform\n",
    "        return res\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b17ce9a-a02e-4f55-a293-3d5621e61fa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-11T02:00:33.312987Z",
     "iopub.status.busy": "2023-08-11T02:00:33.311692Z",
     "iopub.status.idle": "2023-08-11T02:00:33.327680Z",
     "shell.execute_reply": "2023-08-11T02:00:33.326388Z",
     "shell.execute_reply.started": "2023-08-11T02:00:33.312924Z"
    }
   },
   "outputs": [],
   "source": [
    "class WaveDataset(BaseDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        sample_rate: int = 16_000,\n",
    "        normalize: bool = True,\n",
    "        trim: bool = False,\n",
    "        # custome args\n",
    "        max_wave_length: int = 64600,\n",
    "        transform=None,\n",
    "        is_training=False,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            data=data, sample_rate=sample_rate, normalize=normalize, trim=trim\n",
    "        )\n",
    "        self.is_training = is_training\n",
    "        self.transform = transform\n",
    "        self.max_wave_length = max_wave_length\n",
    "\n",
    "    def check_length(self, waveform):\n",
    "        waveform_len = waveform.shape[-1]\n",
    "\n",
    "        if self.max_wave_length == -1:\n",
    "            return waveform\n",
    "\n",
    "        # don't need to pad\n",
    "        if waveform_len >= self.max_wave_length:\n",
    "            if self.is_training:\n",
    "                start = random.randint(0, waveform_len - self.max_wave_length)\n",
    "            else:\n",
    "                start = (waveform_len - self.max_wave_length) // 2\n",
    "            return waveform[:, start : start + self.max_wave_length]\n",
    "\n",
    "        # need to pad\n",
    "        num_repeats = int(math.ceil(self.max_wave_length / waveform_len))\n",
    "        padded_waveform = torch.tile(waveform, (1, num_repeats))[\n",
    "            :, : self.max_wave_length\n",
    "        ]\n",
    "        return padded_waveform\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        waveform = self.read_audio(index)\n",
    "        res = self.read_metadata(index)\n",
    "\n",
    "        # waveform = self.check_length(waveform)\n",
    "\n",
    "        if (\n",
    "            self.transform is not None\n",
    "            and self.transform[\"Before_trim_audio\"] is not None\n",
    "        ):\n",
    "            for t in self.transform[\"Before_trim_audio\"]:\n",
    "                waveform = t(waveform, metadata=res)\n",
    "                \n",
    "        waveform = self.check_length(waveform)\n",
    "\n",
    "        if (\n",
    "            self.transform is not None\n",
    "            and self.transform[\"After_trim_audio\"] is not None\n",
    "        ):\n",
    "            for t in self.transform[\"After_trim_audio\"]:\n",
    "                waveform = t(waveform, metadata=res)\n",
    "        \n",
    "        res[\"audio\"] = waveform\n",
    "\n",
    "        # print(res['speed'])\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f57bd-83ba-4f70-9b47-b6998694666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Torch dataset to load data from a provided directory.\n",
    "\n",
    "    Args:\n",
    "        data: A pandas dataframe, whose 'path' column is the path for each audio file.\n",
    "        sample_rate: The used sample rate for the audio\n",
    "        amount: default None. If not none, it means the number of used audio\n",
    "        normalize: default True.\n",
    "        trim: default True. trim all silence that is longer than 0.2s and louder than 1% volume\n",
    "        phone_call: default False.\n",
    "    Raises:\n",
    "        IOError: If the directory does ot exists or the directory did not contain any wav files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        sample_rate: int = 16_000,\n",
    "        normalize: bool = True,\n",
    "        trim: bool = False,\n",
    "        phone_call: bool = False,\n",
    "        audio_feature=None,\n",
    "        max_feature_frames=None,\n",
    "        # post processing\n",
    "        len_clip: int = 64600,\n",
    "        len_sep: int = 48000,\n",
    "        audio_split=False,\n",
    "        over_sample=False,\n",
    "        random_cut=False,\n",
    "        transform=None,\n",
    "        test=False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if audio_split:\n",
    "            self.data = audio_data_split(data, len_clip, len_sep)\n",
    "        else:\n",
    "            self.data = data\n",
    "\n",
    "        self.trim = trim\n",
    "        self.sample_rate = sample_rate\n",
    "        self.normalize = normalize\n",
    "        self.phone_call = phone_call\n",
    "\n",
    "        # post processing\n",
    "        self.len_clip = len_clip\n",
    "        self.len_sep = len_sep\n",
    "        self.random_cut = random_cut\n",
    "        self.transform = transform\n",
    "\n",
    "    def read_audio(self, index: int) -> Tuple[torch.Tensor, int, int]:\n",
    "        item = self.data.iloc[index]\n",
    "\n",
    "        path = item[\"audio_path\"]\n",
    "        fps = item[\"audio_fps\"]\n",
    "\n",
    "        if fps != self.sample_rate:\n",
    "            waveform, sample_rate = torchaudio.sox_effects.apply_effects_file(\n",
    "                path, [[\"rate\", f\"{self.sample_rate}\"]], normalize=self.normalize\n",
    "            )\n",
    "        else:\n",
    "            waveform, sample_rate = torchaudio.load(path, normalize=self.normalize)\n",
    "\n",
    "        if self.trim:\n",
    "            (\n",
    "                waveform_trimmed,\n",
    "                sample_rate_trimmed,\n",
    "            ) = torchaudio.sox_effects.apply_effects_tensor(\n",
    "                waveform, sample_rate, SOX_SILENCE\n",
    "            )\n",
    "\n",
    "            if waveform_trimmed.size()[1] > 0:\n",
    "                waveform = waveform_trimmed\n",
    "                sample_rate = sample_rate_trimmed\n",
    "\n",
    "        if self.phone_call:\n",
    "            waveform, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n",
    "                waveform,\n",
    "                sample_rate,\n",
    "                effects=[\n",
    "                    [\"lowpass\", \"4000\"],\n",
    "                    [\n",
    "                        \"compand\",\n",
    "                        \"0.02,0.05\",\n",
    "                        \"-60,-60,-30,-10,-20,-8,-5,-8,-2,-8\",\n",
    "                        \"-8\",\n",
    "                        \"-7\",\n",
    "                        \"0.05\",\n",
    "                    ],\n",
    "                    [\"rate\", \"8000\"],\n",
    "                ],\n",
    "            )\n",
    "            waveform = apply_codec(waveform, sample_rate, format=\"gsm\")\n",
    "\n",
    "        if \"start\" in item.keys():\n",
    "            s = int(item[\"start\"])\n",
    "            e = int(item[\"end\"])\n",
    "            waveform = waveform[:, s:e]\n",
    "\n",
    "        res = {\n",
    "            \"audio\": waveform,\n",
    "            \"sample_rate\": sample_rate,\n",
    "            \"name\": item[\"audio_path\"],\n",
    "        }\n",
    "        if \"label\" in item.keys():\n",
    "            res[\"label\"] = item[\"label\"]\n",
    "        return res\n",
    "\n",
    "    def cut_audio(self, waveform):\n",
    "        waveform_len = waveform.shape[-1]\n",
    "\n",
    "        # don't need to pad\n",
    "        if waveform_len >= self.len_clip:\n",
    "            if self.random_cut:\n",
    "                start = random.randint(0, waveform_len - self.len_clip)\n",
    "            else:\n",
    "                start = 0\n",
    "            return waveform[:, start : start + self.len_clip]\n",
    "\n",
    "        # need to pad\n",
    "        num_repeats = int(self.len_clip / waveform_len) + 1\n",
    "        padded_waveform = torch.tile(waveform, (1, num_repeats))[:, : self.len_clip]\n",
    "        return padded_waveform\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        res = self.read_audio(index)  # 1. read audio\n",
    "\n",
    "        # 2. post precessing\n",
    "        if self.len_clip > 0:\n",
    "            res[\"audio\"] = self.cut_audio(res[\"audio\"])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            res[\"audio\"] = self.transform(res[\"audio\"])\n",
    "\n",
    "        return res\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d9e7b-9a1a-4e50-a55d-b1a85cbd5ec8",
   "metadata": {
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import WaveFake\n",
    "\n",
    "wave = WaveFake(\"/usr/local/ay_data/dataset/0-deepfake/WaveFake\")\n",
    "ds = AudioDataset(wave.data, cut=500000)\n",
    "ds[0][0].shape"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
