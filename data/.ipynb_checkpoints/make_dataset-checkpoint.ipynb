{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5f1d3c-81a1-4f8c-97f2-a9357ff39874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T02:20:19.941247Z",
     "iopub.status.busy": "2024-11-04T02:20:19.940562Z",
     "iopub.status.idle": "2024-11-04T02:20:19.994272Z",
     "shell.execute_reply": "2024-11-04T02:20:19.992337Z",
     "shell.execute_reply.started": "2024-11-04T02:20:19.941184Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42caf32d-f8a4-495d-bc37-528c47257ce6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T02:20:21.465829Z",
     "iopub.status.busy": "2024-11-04T02:20:21.464793Z",
     "iopub.status.idle": "2024-11-04T02:20:23.779615Z",
     "shell.execute_reply": "2024-11-04T02:20:23.778399Z",
     "shell.execute_reply.started": "2024-11-04T02:20:21.465723Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, default_collate\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61313dd-61d5-4c53-83e4-a406d85c0ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T02:20:30.266575Z",
     "iopub.status.busy": "2024-11-04T02:20:30.266232Z",
     "iopub.status.idle": "2024-11-04T02:20:39.105968Z",
     "shell.execute_reply": "2024-11-04T02:20:39.104925Z",
     "shell.execute_reply.started": "2024-11-04T02:20:30.266545Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01may2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     ASV2019LA_AudioDs,\n\u001b[1;32m      3\u001b[0m     ASV2021_AudioDs,\n\u001b[1;32m      4\u001b[0m     ASV2021LA_AudioDs,\n\u001b[1;32m      5\u001b[0m     Codecfake_AudioDs,\n\u001b[1;32m      6\u001b[0m     DECRO_AudioDs,\n\u001b[1;32m      7\u001b[0m     InTheWild_AudioDs,\n\u001b[1;32m      8\u001b[0m     LibriSeVoc_AudioDs,\n\u001b[1;32m      9\u001b[0m     MLAAD_AudioDs,\n\u001b[1;32m     10\u001b[0m     VGGSound_AudioDs,\n\u001b[1;32m     11\u001b[0m     WaveFake_AudioDs,\n\u001b[1;32m     12\u001b[0m     Codecfake_AudioDs\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01may2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m color_print\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01may2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioRawBoost, SpecAugmentTransform_Wave\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/ay_data/packages2/ay2/datasets/audio/__init__.py:32\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     30\u001b[0m py_path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m cls_name \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpy_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, cls_name)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/ay_data/packages2/ay2/datasets/audio/_codecfake.py:312\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Namespace(\n\u001b[1;32m    305\u001b[0m             train\u001b[38;5;241m=\u001b[39msub_datas[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    306\u001b[0m             val\u001b[38;5;241m=\u001b[39msub_datas[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    307\u001b[0m             test\u001b[38;5;241m=\u001b[39mtest_datas,\n\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mCodecfake_AudioDs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mROOT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m data \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    314\u001b[0m splits \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mget_splits()\n",
      "File \u001b[0;32m/usr/local/ay_data/packages2/ay2/datasets/base.py:47\u001b[0m, in \u001b[0;36mBase.__init__\u001b[0;34m(self, root_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03mWhen crate a entry of WaveFake, it will read all the metadatas from the root_path\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    root_path: the path of WaveFake dataset. Note that the path must contain \"/WaveFake/\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_path \u001b[38;5;241m=\u001b[39m root_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m root_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m root_path[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_property(root_path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess()\n",
      "File \u001b[0;32m/usr/local/ay_data/packages2/ay2/datasets/base.py:80\u001b[0m, in \u001b[0;36mBase.read_metadata\u001b[0;34m(self, root_path, re_generate)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_metadata(data)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/ay_data/packages2/ay2/datasets/audio/_codecfake.py:272\u001b[0m, in \u001b[0;36mCodecfake_AudioDs._read_metadata\u001b[0;34m(self, root_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03mRead metadata for the audio dataset.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    pd.DataFrame: The processed metadata.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m _realdata \u001b[38;5;241m=\u001b[39m read_wav_paths(root_path)\n\u001b[0;32m--> 272\u001b[0m _metadata \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(_realdata, _metadata, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelative_path\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_audio_path(data, root_path)\n",
      "File \u001b[0;32m/usr/local/ay_data/packages2/ay2/datasets/audio/_codecfake.py:173\u001b[0m, in \u001b[0;36mgenerate_metadata\u001b[0;34m(root_path)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m#### process labels and vocoder methods\u001b[39;00m\n\u001b[1;32m    172\u001b[0m _data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 173\u001b[0m _data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocoder\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m7\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmethod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m _data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocoder_label\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocoder\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: METHODS\u001b[38;5;241m.\u001b[39mindex(x))\n\u001b[1;32m    180\u001b[0m datas\u001b[38;5;241m.\u001b[39mappend(_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10033\u001b[0m )\n\u001b[0;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 963\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/pandas/core/apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 979\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    981\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    982\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    983\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ay2.datasets.audio import (\n",
    "    ASV2019LA_AudioDs,\n",
    "    ASV2021_AudioDs,\n",
    "    ASV2021LA_AudioDs,\n",
    "    Codecfake_AudioDs,\n",
    "    DECRO_AudioDs,\n",
    "    InTheWild_AudioDs,\n",
    "    LibriSeVoc_AudioDs,\n",
    "    MLAAD_AudioDs,\n",
    "    VGGSound_AudioDs,\n",
    "    WaveFake_AudioDs,\n",
    "    Codecfake_AudioDs\n",
    ")\n",
    "from ay2.tools import color_print\n",
    "from ay2.torch.transforms.audio import AudioRawBoost, SpecAugmentTransform_Wave\n",
    "from ay2.torchaudio.transforms import LFCC, RandomBackgroundNoise, RandomNoise, RawBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1865e9-d5be-4944-85f2-bea42076993a",
   "metadata": {
    "editable": true,
    "lines_to_end_of_cell_marker": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # from .datasets import ADD2023, LAV_DF_Audio, LibriSeVoc, WaveFake, DECRO\n",
    "    from .tools import WaveDataset\n",
    "except ImportError:\n",
    "    # from datasets import ADD2023, LAV_DF_Audio, LibriSeVoc, WaveFake, DECRO\n",
    "    from tools import WaveDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00d727-ff90-4381-b860-a0b1a3c65f50",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Make audio splits (DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3efdf-83e4-411d-b943-4a285fe0deff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_emotion_labels(\n",
    "    data: pd.DataFrame,\n",
    "    emotion_df_path=\"/home/ay/data/DATA/2-datasets/1-df-audio/emotions.csv\",\n",
    "):\n",
    "    emotions = pd.read_csv(emotion_df_path)\n",
    "    emotions[\"emotion_label\"] = emotions[\"index\"]\n",
    "    emotions = emotions[[\"audio_path\", \"emotion_label\"]]\n",
    "    data = pd.merge(data, emotions, how=\"left\", on=\"audio_path\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7539c5f-879d-410f-a3d5-7429f75baecb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88800f5-a634-4e11-a72a-6f32faf87826",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_InTheWild_data(\n",
    "    root_path=\"/home/ay/data/DATA/2-datasets/1-df-audio/release_in_the_wild\",\n",
    "):\n",
    "    dataset = InTheWild_AudioDs(root_path=root_path)\n",
    "    return dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1079fa-61d3-4f09-8da7-04206123e30b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset = InTheWild_AudioDs(root_path=\"/home/ay/data/DATA/2-datasets/1-df-audio/release_in_the_wild\")\n",
    "# dataset.data.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dffe00-730e-4f56-934b-c4129cbfcd1d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### WaveFake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a8ac3-0ee7-4b8e-ac76-c3f36f9ec872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WaveFake_JP(root_path=\"/home/ay/data/DATA/2-datasets/1-df-audio/WaveFake\"):\n",
    "    dataset = WaveFake_AudioDs(root_path)\n",
    "    data_train = dataset.get_sub_data(corpus=1, methods=[1, 2])\n",
    "    train, val = dataset.split_data(data_train, splits=[0.8, 0.2], return_list=True, refer=\"id\")\n",
    "    data_splits = Namespace(train=train, val=val)\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66f636-2721-443f-ba84-58d4e471fa3d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_WaveFake(cfg):\n",
    "    dataset = WaveFake_AudioDs(root_path=cfg.root_path)\n",
    "    # dataset.data = get_emotion_labels(dataset.data)\n",
    "\n",
    "    if cfg.task == \"inner_eval\":\n",
    "        color_print(\"WaveFake task: inner evaluation\")\n",
    "        data = dataset.get_sub_data(corpus=cfg.corpus, methods=cfg.methods)\n",
    "        data_splits = dataset.split_data(data, splits=cfg.splits, refer=\"id\")\n",
    "    elif cfg.task == \"cross_lang\":\n",
    "        color_print(\"WaveFake task: cross language evaluation\")\n",
    "\n",
    "        task_cfg = cfg.task_cfg\n",
    "        data_train = dataset.get_sub_data(corpus=task_cfg.train.corpus, methods=task_cfg.train.methods)\n",
    "        train, val = dataset.split_data(data_train, splits=task_cfg.train.splits, return_list=True, refer=\"id\")\n",
    "        test = dataset.get_sub_data(corpus=task_cfg.test.corpus, methods=task_cfg.test.methods)\n",
    "        data_splits = Namespace(train=train, val=val, test=test)\n",
    "    elif cfg.task == \"cross_method\":\n",
    "        color_print(\"WaveFake task: cross method\")\n",
    "\n",
    "        task_cfg = cfg.task_cfg\n",
    "        # get real data, and split it into train/val/test\n",
    "        data_real = dataset._get_sub_data(task_cfg.train.corpus, \"real\")\n",
    "        real_train, real_val, real_test = dataset.split_data(\n",
    "            data_real, splits=[0.6, 0.2, 0.2], return_list=True, refer=\"id\"\n",
    "        )\n",
    "\n",
    "        data_train = dataset.get_sub_data(\n",
    "            corpus=task_cfg.train.corpus,\n",
    "            methods=task_cfg.train.methods,\n",
    "            contain_real=False,\n",
    "        )\n",
    "        train, val = dataset.split_data(data_train, splits=task_cfg.train.splits, return_list=True, refer=\"id\")\n",
    "        test = [\n",
    "            dataset.get_sub_data(corpus=_cfg.corpus, methods=_cfg.methods, contain_real=False) for _cfg in task_cfg.test\n",
    "        ]\n",
    "        train = pd.concat([train, real_train], ignore_index=True)\n",
    "        val = pd.concat([val, real_val], ignore_index=True)\n",
    "        test = [pd.concat([_test, real_test], ignore_index=True) for _test in test]\n",
    "        data_splits = Namespace(train=train, val=val, test=test)\n",
    "\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c8e07-5790-4f0b-b932-699edda65fae",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### LibriSeVoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f170c69-1973-44b4-9718-ec687d356a4c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_LibriSeVoc(cfg):\n",
    "    dataset = LibriSeVoc_AudioDs(root_path=cfg.ROOT_PATHs.LibriSeVoc)\n",
    "    # dataset.data = get_emotion_labels(dataset.data)\n",
    "\n",
    "    if cfg.task == \"inner_eval\":\n",
    "        color_print(\"LibriSeVoc task: inner evaluation\")\n",
    "\n",
    "        data = dataset.get_sub_data(methods=cfg.methods)\n",
    "        data_splits = dataset.split_data(data, splits=cfg.splits, refer=\"id\")\n",
    "    elif cfg.task == \"cross_method\":\n",
    "        color_print(\"LibriSeVoc task: cross method evaluation\")\n",
    "        task_cfg = cfg.task_cfg\n",
    "\n",
    "        # get real data, and split it into train/val/test\n",
    "        data_real = dataset.get_sub_data([], contain_real=True)\n",
    "        real_train, real_val, real_test = dataset.split_data(\n",
    "            data_real, splits=[0.6, 0.2, 0.2], return_list=True, refer=\"id\"\n",
    "        )\n",
    "\n",
    "        data_train = dataset.get_sub_data(methods=task_cfg.train.methods, contain_real=False)\n",
    "        train, val = dataset.split_data(data_train, splits=task_cfg.train.splits, return_list=True, refer=\"id\")\n",
    "        test = [dataset.get_sub_data(methods=_cfg.methods, contain_real=False) for _cfg in task_cfg.test]\n",
    "        train = pd.concat([train, real_train], ignore_index=True)\n",
    "        val = pd.concat([val, real_val], ignore_index=True)\n",
    "        test = [pd.concat([_test, real_test], ignore_index=True) for _test in test]\n",
    "\n",
    "        data_splits = Namespace(train=train, val=val, test=test)\n",
    "    elif cfg.task == \"cross_dataset\":\n",
    "        color_print(\"LibriSeVoc task: cross dataset evaluation\")\n",
    "        task_cfg = cfg.task_cfg\n",
    "        data_train = dataset.get_sub_data(methods=task_cfg.train.methods)\n",
    "        train, val = dataset.split_data(data_train, splits=task_cfg.train.splits, return_list=True, refer=\"id\")\n",
    "        test = []\n",
    "        for _cfg in task_cfg.test:\n",
    "            if _cfg.dataset.lower() == \"wavefake\":\n",
    "                dataset2 = WaveFake_AudioDs(root_path=cfg.ROOT_PATHs.WaveFake)\n",
    "                _data = dataset2.get_sub_data(corpus=_cfg.corpus, methods=_cfg.methods)\n",
    "                test.append(_data)\n",
    "        test.append(get_InTheWild_data())\n",
    "        test.append(get_DECRO_test_splits(language=\"en\"))\n",
    "        test.append(get_DECRO_test_splits(language=\"cn\"))\n",
    "        # test += get_ASV2021_test_splits()\n",
    "        data_splits = Namespace(train=train, val=val, test=test)\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1e4b1-9a40-44b9-b228-a193b2aa6a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T11:35:09.247018Z",
     "iopub.status.busy": "2024-01-04T11:35:09.246332Z",
     "iopub.status.idle": "2024-01-04T11:35:09.255194Z",
     "shell.execute_reply": "2024-01-04T11:35:09.253131Z",
     "shell.execute_reply.started": "2024-01-04T11:35:09.246951Z"
    },
    "lines_to_next_cell": 2
   },
   "source": [
    "### DECRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133a8207-1494-47bc-9698-a54b623afef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DECRO_test_splits(root_path=\"/home/ay/data/DATA/2-datasets/1-df-audio/DECRO\", language=\"en\"):\n",
    "    dataset = DECRO_AudioDs(root_path=root_path)\n",
    "    en_splits = dataset.get_splits(language=\"en\")\n",
    "    ch_splits = dataset.get_splits(language=\"ch\")\n",
    "    if language == \"en\":\n",
    "        data = en_splits.test\n",
    "    else:\n",
    "        data = ch_splits.test\n",
    "\n",
    "    data[\"vocoder_label_org\"] = data[\"vocoder_label\"]\n",
    "    data[\"vocoder_label\"] = 0\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_DECRO_splits(root_path=\"/home/ay/data/DATA/2-datasets/1-df-audio/DECRO\"):\n",
    "    dataset = DECRO_AudioDs(root_path=root_path)\n",
    "    en_splits = dataset.get_splits(language=\"en\")\n",
    "    ch_splits = dataset.get_splits(language=\"ch\")\n",
    "    return en_splits, ch_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c656a-c11b-4de0-8f50-09331199458a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_DECRO(cfg):\n",
    "    dataset = DECRO_AudioDs(root_path=cfg.root_path)\n",
    "    # dataset.data = get_emotion_labels(dataset.data)\n",
    "\n",
    "    en_splits = dataset.get_splits(language=\"en\")\n",
    "    ch_splits = dataset.get_splits(language=\"ch\")\n",
    "\n",
    "    if cfg.task == \"en->ch\":\n",
    "        color_print(\"DECRO task: en->ch\")\n",
    "        train, val, test = (\n",
    "            en_splits.train,\n",
    "            en_splits.val,\n",
    "            [en_splits.train, ch_splits.test, get_InTheWild_data()],\n",
    "        )\n",
    "    else:\n",
    "        color_print(\"DECRO task: ch->en\")\n",
    "        train, val, test = (\n",
    "            ch_splits.train,\n",
    "            ch_splits.val,\n",
    "            [ch_splits.test, en_splits.test, get_InTheWild_data()],\n",
    "        )\n",
    "    data_splits = Namespace(train=train, val=val, test=test)\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da1971-2cac-4905-ae1d-be183b3d3185",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### ASV 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20707505-f865-49d0-9768-4f468ebf63fc",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "def make_ASV2019(cfg):\n",
    "    dataset = ASV2019LA_AudioDs(root_path=cfg.root_path)\n",
    "    if cfg.task == \"inner_eval\":\n",
    "        color_print(\"ASVspoof 2021 task: inner evaluation\")\n",
    "        data_splits = dataset.get_splits()\n",
    "\n",
    "    data_splits.test = [data_splits.test]\n",
    "    data_splits.test += get_ASV2021_whole_test_split(cfg=cfg)\n",
    "    data_splits.test += get_ASV2021_test_splits(cfg=cfg)\n",
    "    return data_splits\n",
    "\n",
    "\n",
    "def get_ASV2019_test_split(root_path=\"/home/ay/data/0-原始数据集/ASV2019\"):\n",
    "    dataset = ASV2019LA_AudioDs(root_path=root_path)\n",
    "    data_splits = dataset.get_splits()\n",
    "    return data_splits.test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99ad85-835b-4afb-a813-5f4620c3800e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### ASV 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a196609-312c-4f04-9f2b-09b852ecd71d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_ASV2021_test_splits(root_path=\"/home/ay/ASV2021\", cfg=None):\n",
    "    dataset = ASV2021_AudioDs(root_path=root_path)\n",
    "    data_splits = dataset.get_test_splits()\n",
    "\n",
    "    \n",
    "    if cfg is None:\n",
    "        return data_splits\n",
    "\n",
    "    \n",
    "    args = eval(cfg.runtime_args)\n",
    "    if args.filter_ASV2021:\n",
    "        AA = [\"A07\", \"A08\", \"A09\", \"A10\", \"A11\", \"A12\", \"A13\", \"A14\", \"A15\", \"A16\", \"A17\", \"A18\", \"A19\"]\n",
    "        data_splits = [data.query(f\"attack not in {AA}\").reset_index(drop=True) for data in data_splits]\n",
    "\n",
    "\n",
    "    return data_splits\n",
    "\n",
    "\n",
    "def get_ASV2021_whole_test_split(root_path=\"/home/ay/ASV2021\", cfg=None):\n",
    "    dataset = ASV2021_AudioDs(root_path=root_path)\n",
    "    test = dataset.get_whole_test_split()\n",
    "    data_splits = [test]\n",
    "\n",
    "    if cfg is None:\n",
    "        return data_splits\n",
    "    \n",
    "    args = eval(cfg.runtime_args)\n",
    "    if args.filter_ASV2021:\n",
    "        AA = [\"A07\", \"A08\", \"A09\", \"A10\", \"A11\", \"A12\", \"A13\", \"A14\", \"A15\", \"A16\", \"A17\", \"A18\", \"A19\"]\n",
    "        data_splits = [data.query(f\"attack not in {AA}\").reset_index(drop=True) for data in data_splits]\n",
    "    return data_splits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d2c19-c46e-4753-8e09-21fdc3ba906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ASV2021(cfg):\n",
    "    dataset = ASV2021_AudioDs(root_path=cfg.root_path)\n",
    "    if cfg.task == \"inner_eval\":\n",
    "        color_print(\"ASVspoof 2021 DF task: inner evaluation\")\n",
    "        data_splits = dataset.get_splits()\n",
    "\n",
    "    data_splits.test += get_ASV2021_whole_test_split(root_path=cfg.root_path)\n",
    "\n",
    "    args = eval(cfg.runtime_args)\n",
    "    if args.filter_ASV2021:\n",
    "        AA = [\"A07\", \"A08\", \"A09\", \"A10\", \"A11\", \"A12\", \"A13\", \"A14\", \"A15\", \"A16\", \"A17\", \"A18\", \"A19\"]\n",
    "        data_splits.test[1:] = [data.query(f\"attack not in {AA}\").reset_index(drop=True) for data in data_splits.test[1:]]\n",
    "    \n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043c98c-7453-47e3-b485-f6b2ed639acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ASV2021_LA(cfg):\n",
    "    dataset = ASV2021LA_AudioDs(root_path=cfg.root_path)\n",
    "    if cfg.task == \"inner_eval\":\n",
    "        color_print(\"ASVspoof 2021 LA task: inner evaluation\")\n",
    "        data_splits = dataset.get_splits()\n",
    "\n",
    "    data_splits.test = [data_splits.test]\n",
    "    data_splits.test.append(get_ASV2019_test_split())\n",
    "    data_splits.test += get_ASV2021_whole_test_split(cfg=cfg)\n",
    "    data_splits.test += get_ASV2021_test_splits(cfg=cfg)\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0ecaa-75e2-4882-91da-2dd0759f2872",
   "metadata": {},
   "source": [
    "### Codecfake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef9482-8634-441f-9fb9-579c451c3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_Codecfake(cfg):\n",
    "    dataset = Codecfake_AudioDs(root_path=cfg.root_path)\n",
    "    color_print(\"Load dataset splits of Codecfake\")\n",
    "    data_splits = dataset.get_splits()\n",
    "\n",
    "    data_splits.test.append(get_ASV2019_test_split())\n",
    "    data_splits.test += get_ASV2021_whole_test_split(cfg=cfg)\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81eb14-bef1-49ad-92fd-c6683e067ae3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### VGG Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea58f36-7028-4dde-b7e2-258f8a4d5989",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_VGGSound(cfg):\n",
    "    dataset = VGGSound_AudioDs(root_path=cfg.root_path)\n",
    "    color_print(\"VGGSound: load splits\")\n",
    "    data_splits = dataset.get_splits()  # only train and test splits\n",
    "    data_splits.val = data_splits.test\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc075fe-66aa-4e6a-a773-49057a3d7ced",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### MLAAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f605fa-4c18-4f8c-a1e3-e153e87fcfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_MLAAD(cfg):\n",
    "    def append_test(_data_splits):\n",
    "        _data_splits.test.append(get_InTheWild_data())\n",
    "        _data_splits.test.append(get_DECRO_test_splits(language=\"en\"))\n",
    "        _data_splits.test.append(get_DECRO_test_splits(language=\"cn\"))\n",
    "\n",
    "    dataset = MLAAD_AudioDs(root_path=cfg.root_path)\n",
    "    color_print(\"MLAAD: load splits\")\n",
    "    if cfg.task == \"cross_lang\":\n",
    "        data_splits = dataset.get_splits(language_list=[\"en\", \"de\", \"es\"])\n",
    "        append_test(data_splits)\n",
    "    elif cfg.task == \"de_es_ru\":\n",
    "        print(\"MLDDA, load DE, ES, RU subsets for training!!!!!!!\")\n",
    "        data_splits = dataset.get_splits(language_list=[\"de\", \"es\", \"ru\"])\n",
    "        append_test(data_splits)\n",
    "    elif cfg.task == \"DECRO\":\n",
    "        print(\"MLDDA, load DECRO and WaveFake subsets for training!!!!!!!\")\n",
    "        data_splits = dataset.get_splits(language_list=[])\n",
    "        append_test(data_splits)\n",
    "        en_splits, ch_splits = get_DECRO_splits()\n",
    "        jp_splits = get_WaveFake_JP()\n",
    "        data_splits.train = pd.concat([en_splits.train, ch_splits.train, jp_splits.train], ignore_index=True)\n",
    "        data_splits.val = pd.concat([en_splits.val, ch_splits.val, jp_splits.val], ignore_index=True)\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d636c27-9e62-4247-8e23-583c8d4769fe",
   "metadata": {},
   "source": [
    "### Codecfake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a09e410a-287d-44eb-a2af-4f329ec4f7bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T02:21:58.478572Z",
     "iopub.status.busy": "2024-11-04T02:21:58.478165Z",
     "iopub.status.idle": "2024-11-04T02:21:58.533991Z",
     "shell.execute_reply": "2024-11-04T02:21:58.532065Z",
     "shell.execute_reply.started": "2024-11-04T02:21:58.478537Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_Codecfake(cfg):\n",
    "\n",
    "    dataset = Codecfake_AudioDs(root_path=cfg.root_path)\n",
    "    data_splits = dataset.get_splits()\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a370d0-db7c-4ae5-8246-3cb50cd0c649",
   "metadata": {},
   "source": [
    "## Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740837e8-c1e4-4585-90b4-c600fd42c74a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "MAKE_DATASETS = {\n",
    "    \"WaveFake\": make_WaveFake,\n",
    "    \"LibriSeVoc\": make_LibriSeVoc,\n",
    "    \"DECRO\": make_DECRO,\n",
    "    \"ASV2021\": make_ASV2021,\n",
    "    \"ASV2021_LA\": make_ASV2021_LA,\n",
    "    \"ASV2019_LA\": make_ASV2019,\n",
    "    \"VGGSound\": make_VGGSound,\n",
    "    \"MLAAD\": make_MLAAD,\n",
    "    \"Codecfake\": make_Codecfake,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea73119-1996-4b96-9dee-6c8a85f153f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20eba88-536b-4127-8bf6-344b348bc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature(cfg):\n",
    "    if cfg.audio_feature == \"LFCC\":\n",
    "        return LFCC()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad84e5-2537-4e33-aa1b-ae8ae57ecfec",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb75dcd0-5e2f-42a6-9ec6-9c6033c8fdfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T02:47:21.382320Z",
     "iopub.status.busy": "2024-10-16T02:47:21.381959Z",
     "iopub.status.idle": "2024-10-16T02:47:23.760387Z",
     "shell.execute_reply": "2024-10-16T02:47:23.759761Z",
     "shell.execute_reply.started": "2024-10-16T02:47:21.382278Z"
    }
   },
   "outputs": [],
   "source": [
    "from ay2.torchaudio.transforms import RandomAudioCompression,RandomAudioCompressionSpeedChanging\n",
    "from ay2.torchaudio.transforms.self_operation import (\n",
    "    AudioToTensor,\n",
    "    CentralAudioClip,\n",
    "    RandomAudioClip,\n",
    "    RandomPitchShift,\n",
    "    RandomSpeed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c1127-a38f-41e4-ab69-1a267cbe1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_augmentations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a57c5-fa33-4b54-95ba-0ee0753e9a94",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def build_transforms(cfg=None, args=None):\n",
    "    # t1 = RandomNoise(snr_min_db=10.0, snr_max_db=120.0, p=1.0)\n",
    "    # # t = RawBoost(algo=[5], p=0.5)\n",
    "    # t2 = RandomSpeed(min_speed=0.5, max_speed=2.0, p=1.0)\n",
    "    # t3 = RandomPitchShift(p=1.0)\n",
    "\n",
    "    # sr = 16000\n",
    "    # num_samples=48000\n",
    "    # transforms = [\n",
    "    #     RandomResizedCrop(n_samples=num_samples),\n",
    "    #     RandomApply([PolarityInversion()], p=0.8),\n",
    "    #     RandomApply([Noise(min_snr=0.001, max_snr=0.005)], p=0.3),\n",
    "    #     RandomApply([Gain()], p=0.2),\n",
    "    #     HighLowPass(sample_rate=sr), # this augmentation will always be applied in this aumgentation chain!\n",
    "    #     RandomApply([Delay(sample_rate=sr)], p=0.5),\n",
    "    #     RandomApply([PitchShift(\n",
    "    #         n_samples=num_samples,\n",
    "    #         sample_rate=sr\n",
    "    #     )], p=0.4),\n",
    "    #     RandomApply([Reverb(sample_rate=sr)], p=0.3)\n",
    "    # ]\n",
    "\n",
    "    # return {\n",
    "    #     \"train\": transforms,\n",
    "    #     \"val\": [\n",
    "    #         CentralAudioClip(length=48000),\n",
    "    #         AudioToTensor(),\n",
    "    #     ],\n",
    "    # }\n",
    "\n",
    "    res = {\n",
    "        \"train\": [\n",
    "            # RandomSpeed(min_speed=0.5, max_speed=2.0, p=0.5),\n",
    "            # RandomAudioCompression(p=0.9),\n",
    "            # RandomSpeed(min_speed=0.5, max_speed=2.0, p=1.0),\n",
    "            RandomAudioClip(length=48000),\n",
    "            RandomNoise(snr_min_db=10.0, snr_max_db=120.0, p=1.0),\n",
    "            AudioToTensor(),\n",
    "            # RandomApply([PitchShift(n_samples=48000, sample_rate=16000)], p=0.5),\n",
    "            # RandomPitchShift(p=0.5),\n",
    "        ],\n",
    "        \"val\": [\n",
    "            CentralAudioClip(length=48000),\n",
    "            AudioToTensor(),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    if args is not None and args.cfg.startswith(\"Ours/ResNet\"):\n",
    "        if 'ASV2021' in args.cfg:\n",
    "            res[\"train\"] = [\n",
    "                RandomSpeed(min_speed=0.5, max_speed=2.0, p=1.0),\n",
    "            ] + res[\"train\"]\n",
    "        else:\n",
    "            res[\"train\"] = [\n",
    "                # RandomAudioCompression(p=0.9),\n",
    "                # RandomSpeed(min_speed=0.5, max_speed=2.0, p=1.0),\n",
    "                RandomAudioCompressionSpeedChanging(p_compression=0.9, sample_rate=16000, p_speed=1.0, min_speed=0.5, max_speed=2.0)\n",
    "            ] + res[\"train\"]\n",
    "    else:\n",
    "        res[\"train\"].append(RandomApply([PitchShift(n_samples=48000, sample_rate=16000)], p=0.5))\n",
    "        \n",
    "    \n",
    "\n",
    "    if args is not None and args.test_noise:\n",
    "        res[\"test_noise\"] = [\n",
    "            CentralAudioClip(length=48000),\n",
    "            RandomBackgroundNoise(\n",
    "                16000,\n",
    "                noise_dir=\"/home/ay/data/0-原始数据集/musan/noise\",\n",
    "                p=1.0,\n",
    "                min_snr_db=args.test_noise_level,\n",
    "                max_snr_db=args.test_noise_level,\n",
    "                noise_type=args.test_noise_type,\n",
    "            ),\n",
    "            AudioToTensor(),\n",
    "        ]\n",
    "\n",
    "    # if args.cfg.startswith('MPE_LCNN'):\n",
    "    #     from ay2.torchaudio.transforms import MPE_LFCC\n",
    "    #     for key in res:\n",
    "    #         res[key].append(MPE_LFCC())\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d22ee-9e5f-48cc-8ec8-6ea89dca2f6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Common Opeations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4accf-4165-4fb7-af15-a4afea7a5ad0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def build_dataloader(data: pd.DataFrame, cfg, is_training: bool = True, args=None):\n",
    "    transforms = build_transforms(cfg.transforms, args=args)\n",
    "    transform = transforms[\"train\"] if is_training else transforms[\"val\"]\n",
    "\n",
    "    _ds = WaveDataset(\n",
    "        data,\n",
    "        sample_rate=cfg.sample_rate,\n",
    "        normalize=True,\n",
    "        transform=transform,\n",
    "        dtype=\"tensor\",\n",
    "    )\n",
    "\n",
    "    if not is_training and cfg.test_batch_size > 0:\n",
    "        batch_size = cfg.test_batch_size\n",
    "    else:\n",
    "        batch_size = cfg.batch_size\n",
    "\n",
    "    _dl = DataLoader(\n",
    "        _ds,\n",
    "        batch_size=batch_size,\n",
    "        # num_workers=cfg.num_workers,\n",
    "        num_workers=10,\n",
    "        pin_memory=True,\n",
    "        shuffle=True if is_training else False,\n",
    "        # shuffle=True,\n",
    "        prefetch_factor=2,\n",
    "        drop_last=True if is_training else bool(args.drop_last),\n",
    "    )\n",
    "    return _ds, _dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1546f1-3d1a-4329-8eb7-adb483e9cd9b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Door"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001bf0b4-2f78-4dfc-b90c-911436def412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sample_dataset(data, column=\"label\"):\n",
    "    n_fake = len(data[data[column] == 0])\n",
    "    n_real = len(data[data[column] == 1])\n",
    "    if n_fake == n_real:\n",
    "        return data\n",
    "    if n_fake > n_real:\n",
    "        sampled = data[data[column] == 1].sample(n=n_fake - n_real, replace=True)\n",
    "        balanced_data = pd.concat([data, sampled])\n",
    "    else:\n",
    "        sampled = data[data[column] == 0].sample(n=n_real - n_fake, replace=True)\n",
    "        balanced_data = pd.concat([data, sampled])\n",
    "\n",
    "    balanced_data = balanced_data.copy().reset_index(drop=True)\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793fc65-14a3-4beb-b611-5ec66e2e18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_audio_splits_label_distribution(audio_splits):\n",
    "    res = {}\n",
    "    for _split in [\"train\", \"val\", \"test\"]:\n",
    "        _data = getattr(audio_splits, _split)\n",
    "        res[_split] = \"\"\n",
    "        if isinstance(_data, list):\n",
    "            for _data2 in _data:\n",
    "                tmp = _data2.groupby(\"label\").count()\n",
    "                num_0 = tmp.loc[0][0] if 0 in tmp.index else 0\n",
    "                num_1 = tmp.loc[1][0] if 1 in tmp.index else 0\n",
    "                res[_split] += f\" {num_0}/{num_1}\"\n",
    "        else:\n",
    "            tmp = _data.groupby(\"label\").count()\n",
    "            num_0 = tmp.loc[0][0] if 0 in tmp.index else 0\n",
    "            num_1 = tmp.loc[1][0] if 1 in tmp.index else 0\n",
    "            res[_split] += f\" {num_0}/{num_1}\"\n",
    "\n",
    "    color_print(f\"Fake/Real label distribution in train/val/test: {res['train']}, {res['val']}, {res['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1e8ee-a313-4bdc-82d1-3de2ed4a39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(cfg, args=None):\n",
    "\n",
    "    cfg.dataset_cfg.runtime_args = str(args)\n",
    "    \n",
    "    # make audio splits (pd.DataFrame)\n",
    "    audio_splits = MAKE_DATASETS[cfg.name](cfg.dataset_cfg)\n",
    "    audio_splits.train = over_sample_dataset(audio_splits.train, column=\"label\")\n",
    "\n",
    "    print_audio_splits_label_distribution(audio_splits)\n",
    "\n",
    "    # make dataset and dataloaders\n",
    "    train_ds, train_dl = build_dataloader(audio_splits.train, cfg, is_training=True, args=args)\n",
    "    train_ds2, train_dl2 = build_dataloader(audio_splits.train, cfg, is_training=False, args=args)\n",
    "    val_ds, val_dl = build_dataloader(audio_splits.val, cfg, is_training=False, args=args)\n",
    "    if isinstance(audio_splits.test, list):\n",
    "        test_ds, test_dl = [], []\n",
    "        for _test in audio_splits.test:\n",
    "            _ds, _dl = build_dataloader(_test, cfg, is_training=False, args=args)\n",
    "            test_ds.append(_ds)\n",
    "            test_dl.append(_dl)\n",
    "    else:\n",
    "        test_ds, test_dl = build_dataloader(audio_splits.test, cfg, is_training=False, args=args)\n",
    "\n",
    "    # collect all dataloaders\n",
    "    ds = Namespace(train=train_ds, val=val_ds, test=test_ds, train_wo_transform=train_ds2)\n",
    "    dl = Namespace(train=train_dl, val=val_dl, test=test_dl, train_wo_transform=train_dl2)\n",
    "\n",
    "    print(args)\n",
    "    if args is not None and args.test_noise:\n",
    "        color_print(\"!!!!Test robustness: Load audio with background noise\")\n",
    "        test_noise = build_transforms(args=args)[\"test_noise\"]\n",
    "        if isinstance(dl.test, list):\n",
    "            for _dl in dl.test:\n",
    "                _dl.dataset.transform = test_noise\n",
    "            # dl.test = dl.test[1]\n",
    "        else:\n",
    "            dl.test.transform = test_noise\n",
    "\n",
    "    return ds, dl"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
