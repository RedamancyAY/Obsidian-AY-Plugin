{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import statistics\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ay2.torch.deepfake_detection import DeepfakeAudioClassification\n",
    "from ay2.torch.losses import (\n",
    "    BinaryTokenContrastLoss,\n",
    "    Focal_loss,\n",
    "    LabelSmoothingBCE,\n",
    "    MultiClass_ContrastLoss,\n",
    ")\n",
    "from ay2.torch.optim import Adam_GC\n",
    "from ay2.torch.optim.selective_weight_decay import (\n",
    "    Optimizers_with_selective_weight_decay,\n",
    "    Optimizers_with_selective_weight_decay_for_modulelist,\n",
    ")\n",
    "from ay2.torchaudio.transforms import AddGaussianSNR\n",
    "from tqdm.auto import tqdm\n",
    "from ay2.torchaudio.transforms.self_operation import (\n",
    "    AudioToTensor,\n",
    "    CentralAudioClip,\n",
    "    RandomAudioClip,\n",
    "    RandomPitchShift,\n",
    "    RandomSpeed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61438986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ay2.tools import (\n",
    "    freeze_modules,\n",
    "    rich_bar,\n",
    "    unfreeze_modules,\n",
    "    find_unsame_name_for_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ay2.torchaudio.transforms import SpecAugmentBatchTransform\n",
    "from ay2.torchaudio.transforms.self_operation import RandomSpeed\n",
    "\n",
    "random_speed = RandomSpeed(min_speed=0.5, max_speed=2.0, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26707bb7",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from .model import AudioModel\n",
    "    from .utils import OrthogonalProjectionLoss\n",
    "    from .mi_estimator import CLUBSample_group\n",
    "    from ._cs_models import AudioCSModule\n",
    "except ImportError:\n",
    "    from model import AudioModel\n",
    "    from utils import OrthogonalProjectionLoss\n",
    "    from mi_estimator import CLUBSample_group\n",
    "    from _cs_models import AudioCSModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ee686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_grad(var):\n",
    "    def hook(grad):\n",
    "        var.grad = grad\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "def reduce_similarity(feature1, feature2):\n",
    "    # 计算余弦相似度\n",
    "    cosine_similarity = F.cosine_similarity(feature1, feature2)\n",
    "\n",
    "    # 对余弦相似度进行处理以减小相似性\n",
    "    reduced_similarity = torch.mean(1 - cosine_similarity)\n",
    "\n",
    "    return reduced_similarity\n",
    "\n",
    "\n",
    "def mixup(data, targets, alpha=0.8):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    new_data = data * lam + shuffled_data * (1 - lam)\n",
    "    new_targets = [targets, shuffled_targets, lam]\n",
    "    return new_data, new_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64654bcc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class AudioModel_lit(DeepfakeAudioClassification):\n",
    "    def __init__(self, cfg=None, args=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = AudioModel(\n",
    "            feature_extractor=cfg.feature_extractor,\n",
    "            dims=cfg.dims,\n",
    "            n_blocks=cfg.n_blocks,\n",
    "            vocoder_classes=cfg.method_classes,\n",
    "            cfg=cfg,\n",
    "            args=args,\n",
    "        )\n",
    "        self.cfg = cfg\n",
    "        self.beta1, self.beta2, self.beta3 = cfg.beta\n",
    "\n",
    "        self.one_stem = cfg.one_stem\n",
    "\n",
    "        # self.transform = AddGaussianSNR(snr_max_db=20)\n",
    "        self.audio_transform = SpecAugmentBatchTransform.from_policy(cfg.aug_policy)\n",
    "        self.ttt_transform = [\n",
    "            RandomSpeed(min_speed=0.5, max_speed=2.0, p=1),\n",
    "            CentralAudioClip(length=48000),\n",
    "        ]\n",
    "        self.ttt_transform = AddGaussianSNR(snr_max_db=5)\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.mixup = False\n",
    "        # freeze_modules(self.model.feature_model.get_main_stem())\n",
    "\n",
    "\n",
    "        # self.cs_model = AudioCSModule(1600, 0.25)\n",
    "        \n",
    "        self.configure_loss_fn()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_loss_fn(\n",
    "        self,\n",
    "    ):\n",
    "        self.bce_loss = LabelSmoothingBCE(label_smoothing=0.1)\n",
    "        self.contrast_loss2 = BinaryTokenContrastLoss(alpha=0.1)\n",
    "        self.contrast_lossN = MultiClass_ContrastLoss(alpha=2.5, distance=\"l2\")\n",
    "        # self.contrast_lossN = MultiClass_ContrastLoss(alpha=0.1)\n",
    "        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "        from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "        self.focal_loss = partial(sigmoid_focal_loss, reduction=\"mean\")\n",
    "        self.op_loss = OrthogonalProjectionLoss(gamma=0.5)\n",
    "        self.feat_mi_esti = CLUBSample_group(512, 512, 512)\n",
    "    \n",
    "    def calcuate_loss_one_stem(self, batch_res, batch, stage=\"train\"):\n",
    "        B = batch_res[\"logit\"].shape[0]\n",
    "\n",
    "        batch_res[\"logit\"] = batch_res[\"content_logit\"]\n",
    "        label = batch[\"label\"]\n",
    "        label_32 = label.type(torch.float32)\n",
    "        losses = {}\n",
    "\n",
    "        losses[\"cls_loss\"] = self.bce_loss(batch_res[\"logit\"].squeeze(), label_32)\n",
    "        losses[\"content_contrast_loss\"] = self.contrast_loss2(\n",
    "            batch_res[\"content_feature\"], label_32\n",
    "        )\n",
    "\n",
    "        # loss = sum(losses.values()) / (len(losses))\n",
    "        losses[\"loss\"] = (losses[\"cls_loss\"] + losses[\"content_contrast_loss\"]) / 2\n",
    "        return losses\n",
    "\n",
    "    # Copied and edited from https://www.kaggle.com/code/riadalmadani/fastai-effb0-base-model-birdclef2023\n",
    "    def binary_classification_loss(self, logit, label, mixup=False):\n",
    "        logit = logit.squeeze()\n",
    "        if not mixup:\n",
    "            return self.bce_loss(logit, label.type(torch.float32))\n",
    "        else:\n",
    "            targets1, targets2, lam = label[0:3]\n",
    "            return lam * self.bce_loss(logit, targets1.type(torch.float32)) + (\n",
    "                1 - lam\n",
    "            ) * self.bce_loss(logit, targets2.type(torch.float32))\n",
    "\n",
    "    def get_vocoder_stem_loss(self, losses, batch_res, batch, stage=\"train\"):\n",
    "        if not \"vocoder_label\" in batch.keys():\n",
    "            return 0.0\n",
    "\n",
    "        losses[\"voc_cls_loss\"] = self.ce_loss(\n",
    "            batch_res[\"vocoder_logit\"], batch[\"vocoder_label\"]\n",
    "        )\n",
    "\n",
    "        losses[\"voc_contrast_loss\"] = self.contrast_lossN(\n",
    "            batch_res[\"vocoder_feature\"], batch[\"vocoder_label\"].type(torch.float32)\n",
    "        )\n",
    "        vocoder_stem_loss = losses[\"voc_cls_loss\"] + 0.5 * losses[\"voc_contrast_loss\"]\n",
    "\n",
    "        return vocoder_stem_loss\n",
    "\n",
    "    def get_content_stem_loss(self, losses, batch_res, batch, stage=\"train\"):\n",
    "        label_32 = batch[\"label\"].type(torch.float32)\n",
    "        batch_size = len(label_32)\n",
    "\n",
    "        losses[\"content_cls_loss\"] = self.binary_classification_loss(\n",
    "            batch_res[\"content_logit\"], label_32\n",
    "        )\n",
    "        losses[\"content_contrast_loss\"] = self.contrast_loss2(\n",
    "            batch_res[\"content_feature\"], label_32\n",
    "        )\n",
    "        content_stem_loss = losses[\"content_cls_loss\"] + losses[\"content_contrast_loss\"]\n",
    "        return content_stem_loss\n",
    "\n",
    "    def get_content_adv_loss(self, batch_res, batch, stage=\"train\"):\n",
    "        vocoder_label = torch.ones_like(batch_res[\"content_voc_logit\"]) * (\n",
    "            1 / batch_res[\"content_voc_logit\"].shape[-1]\n",
    "        )\n",
    "        loss = self.ce_loss(\n",
    "            batch_res[\"content_voc_logit\"],\n",
    "            vocoder_label,\n",
    "        )\n",
    "        return loss\n",
    "    def Entropy_loss(self, logit):\n",
    "        def entropy(predict_prob):\n",
    "            l = -predict_prob * torch.log(predict_prob)\n",
    "            return l.sum(-1)\n",
    "        if logit.ndim == 1:\n",
    "            prob = torch.sigmoid(logit)\n",
    "            prob = torch.stack([prob, 1-prob], dim=0)\n",
    "        else:\n",
    "            prob = torch.softmax(logit, dim=-1)\n",
    "        return torch.mean(entropy(prob))\n",
    "\n",
    "    \n",
    "    def calcuate_loss(self, batch_res, batch, stage=\"train\"):\n",
    "        B = batch_res[\"logit\"].shape[0]\n",
    "        label = batch[\"label\"]\n",
    "        losses = {}\n",
    "\n",
    "        losses[\"cls_loss\"] = self.binary_classification_loss(batch_res[\"logit\"], label)\n",
    "        if stage == \"train\":\n",
    "            losses[\"cls_loss\"] += self.focal_loss(\n",
    "                batch_res[\"shuffle_logit\"], batch[\"shuffle_label\"].type(torch.float32)\n",
    "            )\n",
    "\n",
    "        losses[\"vocoder_stem_loss\"] = self.get_vocoder_stem_loss(\n",
    "            losses, batch_res, batch, stage\n",
    "        )\n",
    "        # content_stem_loss = self.get_content_stem_loss(losses, batch_res, batch, stage)\n",
    "\n",
    "        losses[\"speed_loss\"] = self.ce_loss(\n",
    "            batch_res[\"speed_logit\"],\n",
    "            batch[\"speed_label\"].long(),\n",
    "        ) \n",
    "        losses[\"compression_loss\"] = self.ce_loss(\n",
    "            batch_res[\"compression_logit\"],\n",
    "            batch[\"compression_label\"].long(),\n",
    "        )\n",
    "        # print(batch[\"speed_label\"], batch[\"compression_label\"])\n",
    "        # + 0.5 * self.contrast_lossN(\n",
    "            # batch_res[\"content_feature\"], batch[\"speed\"].type(torch.float32)\n",
    "        # )\n",
    "\n",
    "        losses[\"op_loss\"] = 0.0\n",
    "        if self.cfg.use_op_loss:\n",
    "            # losses[\"op_loss\"] = self.op_loss(batch_res[\"feature\"], label)\n",
    "            losses[\"op_loss\"] = self.contrast_loss2(\n",
    "                batch_res[\"feature\"], label\n",
    "            )\n",
    "        # losses['mi_loss'] = self.feat_mi_esti.mi_est(batch_res[\"content_feature\"], batch_res[\"vocoder_feature\"][:, None, :])\n",
    "        losses['entropy_loss'] = self.Entropy_loss(batch_res[\"logit\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.trainer.current_epoch < 0:\n",
    "            losses[\"loss\"] = (\n",
    "                losses[\"vocoder_stem_loss\"]\n",
    "                + 1.0 * (losses[\"speed_loss\"] + losses[\"compression_loss\"])\n",
    "            )\n",
    "        else:\n",
    "            losses[\"loss\"] = (\n",
    "                losses[\"cls_loss\"]\n",
    "                # + 0.5 * content_stem_loss\n",
    "                + 0.5 * losses[\"vocoder_stem_loss\"]\n",
    "                + 0.5 * (losses[\"speed_loss\"] + losses[\"compression_loss\"])\n",
    "                + 0.5 * losses[\"op_loss\"]\n",
    "                # + 0.5 * losses['entropy_loss']\n",
    "            )\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def loss_adjustment(self, auxiliary_loss, main_loss, sigma=0.5):\n",
    "        while auxiliary_loss > main_loss * sigma:\n",
    "            auxiliary_loss = auxiliary_loss * 0.9\n",
    "        return auxiliary_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = Adam_GC  # or torch.optim.Adam\n",
    "        # optim = torch.optim.Adam\n",
    "        # optimizer = optim(self.model.parameters(), lr=0.00001, weight_decay=0.0001)\n",
    "\n",
    "        optimizer = Optimizers_with_selective_weight_decay_for_modulelist(\n",
    "            [self.model, self.feat_mi_esti], optimizer=\"Adam\", lr=0.0001, weight_decay=0.01\n",
    "        )\n",
    "        # optimizer = Optimizers_with_selective_weight_decay(\n",
    "        #     self.model, optimizer=\"SGD\", lr=0.001, weight_decay=0.01\n",
    "        # )\n",
    "        return [optimizer]\n",
    "\n",
    "    def _shared_pred(self, batch, batch_idx, stage=\"train\"):\n",
    "        \"\"\"common predict step for train/val/test\n",
    "\n",
    "        Note that the data augmenation is done in the self.model.feature_extractor.\n",
    "\n",
    "        \"\"\"\n",
    "        audio, sample_rate = batch[\"audio\"], batch[\"sample_rate\"]\n",
    "        # audio_re, _, _ = self.cs_model(audio)\n",
    "        audio = self.model.feature_model.preprocess(audio, stage=stage)\n",
    "        if stage == \"train\" and self.cfg.feature_extractor == 'ResNet':\n",
    "            audio = self.audio_transform.batch_apply(audio)\n",
    "            # print('hello')\n",
    "        batch_res = self.model(\n",
    "            audio, stage=stage, batch=batch if stage == \"train\" else None\n",
    "        )\n",
    "\n",
    "        # batch_res[\"pred\"] = (torch.sigmoid(batch_res[\"logit\"]) + 0.5).int()\n",
    "        batch_res[\"pred\"] = batch_res[\"logit\"]\n",
    "\n",
    "        return batch_res\n",
    "\n",
    "    def _shared_eval_step(\n",
    "        self,\n",
    "        batch,\n",
    "        batch_idx,\n",
    "        stage=\"train\",\n",
    "        loss=True,\n",
    "        dataloader_idx=0,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"common evaluation step for train/val/test\n",
    "\n",
    "        In contrast to the predict step, this evaluation step calculates the losses and logs\n",
    "        them to logger.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_res = self._shared_pred(batch, batch_idx, stage=stage)\n",
    "        label = batch[\"label\"]\n",
    "\n",
    "        if not loss:\n",
    "            return batch_res\n",
    "\n",
    "        if not self.one_stem:\n",
    "            loss = self.calcuate_loss(batch_res, batch, stage=stage)\n",
    "        else:\n",
    "            loss = self.calcuate_loss_one_stem(batch_res, batch, stage=stage)\n",
    "\n",
    "        suffix = \"\" if dataloader_idx == 0 else f\"-dl{dataloader_idx}\"\n",
    "        self.log_dict(\n",
    "            {f\"{stage}-{key}{suffix}\": loss[key] for key in loss},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "            prog_bar=True,\n",
    "            add_dataloader_idx=False,\n",
    "            batch_size=batch[\"label\"].shape[0],\n",
    "        )\n",
    "        batch_res.update(loss)\n",
    "        return batch_res\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"custom training step for twp-step parameter updating.\"\"\"\n",
    "\n",
    "        opt1 = self.optimizers()\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            batch_res = self._shared_eval_step(batch, batch_idx, stage=\"train\")\n",
    "    \n",
    "            opt1.zero_grad()\n",
    "            self.manual_backward(batch_res[\"loss\"], retain_graph=True)\n",
    "            # opt1.step()\n",
    "    \n",
    "            freeze_modules([self.model.cls_voc, self.model.feature_model.get_main_stem()])\n",
    "            batch_res[\"content_adv_loss\"] = 0.5 * self.get_content_adv_loss(\n",
    "                batch_res, batch\n",
    "            )\n",
    "            self.manual_backward(batch_res[\"content_adv_loss\"])\n",
    "            unfreeze_modules([self.model.cls_voc, self.model.feature_model.get_main_stem()])\n",
    "    \n",
    "            opt1.step()\n",
    "\n",
    "        return batch_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(3, 10)\n",
    "# torch.std(x, dim=1)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
