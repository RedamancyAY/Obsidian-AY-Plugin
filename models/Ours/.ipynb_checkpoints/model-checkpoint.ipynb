{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f183a79-d07e-4e9f-9606-520ada0bedf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:59:30.798027Z",
     "iopub.status.busy": "2023-08-09T13:59:30.797438Z",
     "iopub.status.idle": "2023-08-09T13:59:30.941570Z",
     "shell.execute_reply": "2023-08-09T13:59:30.940863Z",
     "shell.execute_reply.started": "2023-08-09T13:59:30.797978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1caf9846-a1a5-49b5-b121-fb92bac2868c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T07:23:00.713490Z",
     "iopub.status.busy": "2024-03-15T07:23:00.712684Z",
     "iopub.status.idle": "2024-03-15T07:23:02.234762Z",
     "shell.execute_reply": "2024-03-15T07:23:02.233503Z",
     "shell.execute_reply.started": "2024-03-15T07:23:00.713403Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from ay2.torch.nn import LambdaFunctionModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311849e6-480b-41a6-8a5d-9dc525a294fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T06:43:46.308844Z",
     "iopub.status.busy": "2024-01-11T06:43:46.308203Z",
     "iopub.status.idle": "2024-01-11T06:43:46.315643Z",
     "shell.execute_reply": "2024-01-11T06:43:46.314387Z",
     "shell.execute_reply.started": "2024-01-11T06:43:46.308773Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4a77b-985e-4aac-a4c7-67dad9261ac5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "from .feature_extractor import LCNN, MSFM, RawNet2, ResNet\n",
    "\n",
    "# from .gradient_reversal import GradientReversal\n",
    "# from .modules.classifier import Classifier\n",
    "# from .modules.feature_extractor import FeatureExtractor, FeatureExtractor2D\n",
    "# from .modules.model_RawNet2 import LayerNorm, RawNet_FeatureExtractor, SincConv_fast\n",
    "from .utils import weight_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8486292c-054f-4225-854e-12604b309f8a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-09T13:59:33.978716Z",
     "iopub.status.busy": "2023-08-09T13:59:33.978231Z",
     "iopub.status.idle": "2023-08-09T13:59:34.014060Z",
     "shell.execute_reply": "2023-08-09T13:59:34.013014Z",
     "shell.execute_reply.started": "2023-08-09T13:59:33.978671Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [],
   "source": [
    "from gradient_reversal import GradientReversal\n",
    "from modules.classifier import Classifier\n",
    "from modules.feature_extractor import FeatureExtractor, FeatureExtractor2D\n",
    "from modules.model_RawNet2 import LayerNorm, RawNet_FeatureExtractor, SincConv_fast\n",
    "from utils import weight_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e54144-4e52-4854-b20d-a5b779afe0de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-15T10:25:23.364938Z",
     "iopub.status.busy": "2024-03-15T10:25:23.364284Z",
     "iopub.status.idle": "2024-03-15T10:25:23.380432Z",
     "shell.execute_reply": "2024-03-15T10:25:23.378165Z",
     "shell.execute_reply.started": "2024-03-15T10:25:23.364878Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        h, w = x.shape[2:4]\n",
    "        short_cut = x\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        y = rearrange(y, 'b c h w -> b (h w) c')\n",
    "        x, _ = self.multihead_attn(y, x, x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h,w=w)\n",
    "        return x + short_cut\n",
    "        # return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "416b831f-95fd-4320-b30c-7aa01061e40a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-09T14:58:15.556580Z",
     "iopub.status.busy": "2023-08-09T14:58:15.555664Z",
     "iopub.status.idle": "2023-08-09T14:58:16.363719Z",
     "shell.execute_reply": "2023-08-09T14:58:16.361220Z",
     "shell.execute_reply.started": "2023-08-09T14:58:15.556463Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor: str,\n",
    "        dims=[32, 64, 64, 64, 128],\n",
    "        n_blocks=[1, 1, 1, 2, 1],\n",
    "        n_heads=[1, 2, 2, 4, 1, 1],\n",
    "        samples_per_frame=640,\n",
    "        gru_node=128,\n",
    "        gru_layers=3,\n",
    "        fc_node=128,\n",
    "        num_classes=1,\n",
    "        vocoder_classes=8,\n",
    "        adv_vocoder=False,\n",
    "        cfg=None,\n",
    "        args=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # self.norm = LayerNorm(48000)\n",
    "        self.dims = dims\n",
    "        # self.feature_model = FeatureExtractor2D(\n",
    "        #     dims=dims,\n",
    "        #     n_blocks=n_blocks,\n",
    "        #     n_heads=n_heads,\n",
    "        #     samples_per_frame=samples_per_frame,\n",
    "        #     use_gru_head=False,\n",
    "        #     gru_node=gru_node,\n",
    "        #     gru_layers=gru_layers,\n",
    "        # )\n",
    "\n",
    "        self.feature_extractor = feature_extractor\n",
    "        if feature_extractor == \"LCNN\":\n",
    "            self.feature_model = LCNN()\n",
    "            final_dim = 64\n",
    "        elif feature_extractor == \"RawNet\":\n",
    "            self.feature_model = RawNet2()\n",
    "            final_dim = 1024\n",
    "        elif feature_extractor == \"ResNet\":\n",
    "            self.feature_model = ResNet()\n",
    "            final_dim = 512\n",
    "        elif feature_extractor == \"MSFM\":\n",
    "            self.feature_model = MSFM(\n",
    "                dims=dims, n_blocks=n_blocks, n_heads=n_heads, args=args, cfg=cfg\n",
    "            )\n",
    "            final_dim = dims[-1]\n",
    "\n",
    "        self.feature_model.copy_final_stage()\n",
    "\n",
    "\n",
    "        self.conv_module = nn.Sequential(\n",
    "            nn.Conv2d(final_dim * 2, final_dim // 2, 3, bias=False, padding=1),\n",
    "            nn.BatchNorm2d(final_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(final_dim // 2, final_dim * 2, 3, bias=False, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.cls_content = nn.utils.weight_norm(nn.Linear(final_dim, 1, bias=False))\n",
    "        # self.content_based_cls = nn.utils.weight_norm(nn.Linear(final_dim, 1, bias=False))\n",
    "        # self.vocoder_based_cls = nn.utils.weight_norm(nn.Linear(final_dim, 1, bias=False))\n",
    "        self.cls_voc = nn.utils.weight_norm(\n",
    "            nn.Linear(final_dim, vocoder_classes + 1, bias=False)\n",
    "        )\n",
    "        self.cls_final = nn.Sequential(\n",
    "            # nn.utils.weight_norm(nn.Linear(final_dim * 2, final_dim * 2, bias=False)),\n",
    "            # nn.BatchNorm1d(final_dim * 2),\n",
    "            # nn.ReLU(),\n",
    "            nn.utils.weight_norm(nn.Linear(final_dim * 2, 1, bias=False)),\n",
    "        )\n",
    "\n",
    "        self.cls_speed = nn.utils.weight_norm(nn.Linear(final_dim, 16, bias=False))\n",
    "        self.cls_compression = nn.utils.weight_norm(\n",
    "            nn.Linear(final_dim, 10, bias=False)\n",
    "        )\n",
    "\n",
    "        self.debug = 0\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.cross_attention1 = CrossAttention(512)\n",
    "        self.cross_attention2 = CrossAttention(512)\n",
    "        self.transform = v2.RandomErasing()\n",
    "\n",
    "    def weight_init(self):\n",
    "        \"\"\"initialize all the weights\n",
    "\n",
    "        If use this initialization, should call this funciton in the __init__ function.\n",
    "\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.normal_(m.weight, mean=1, std=0.02)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def get_content_stream_modules(\n",
    "        self,\n",
    "    ):\n",
    "        return self.feature_model.get_content_stream_modules() + [self.cls_content]\n",
    "\n",
    "    def module_similaryity(self):\n",
    "        loss = []\n",
    "        for p1, p2 in zip(\n",
    "            self.feature_model.get_final_block_parameters(),\n",
    "            self.feature_model.get_copied_final_block_parameters(),\n",
    "        ):\n",
    "            _loss = 1 - F.cosine_similarity(p1.view(1, -1), p2.view(1, -1))[0]\n",
    "            loss.append(_loss)\n",
    "        loss = sum(loss) / len(loss)\n",
    "        return loss\n",
    "\n",
    "    def ttt(self, x):\n",
    "        res = {}\n",
    "        res[\"hidden_states\"] = self.feature_model.get_hidden_state(x)\n",
    "\n",
    "    \n",
    "    def feature_norm(self, code):\n",
    "        code_norm = code.norm(p=2,dim=1, keepdim=True) / 10.\n",
    "        code = torch.div(code, code_norm)\n",
    "        return code\n",
    "        \n",
    "    def fuse_stem_featurs(self, feat1, feat2):\n",
    "        ### feat1 and feat2 's dim is 2\n",
    "        if feat1.ndim == 2:\n",
    "            feat = torch.concat([feat1, feat2], dim=-1)\n",
    "            return feat\n",
    "\n",
    "        ### dim is 4\n",
    "        feat1 = self.cross_attention1(feat1, feat2)\n",
    "        feat2 = self.cross_attention2(feat2, feat1)\n",
    "        feat = torch.concat([feat1, feat2], dim=1)\n",
    "        # feat = self.conv_module(feat) + feat\n",
    "        # feat = self.conv_module(feat)\n",
    "        feat = self.avgpool(feat)\n",
    "        feat = feat.reshape(feat.size(0), -1)\n",
    "        # feat = self.feature_norm(feat)\n",
    "        return feat\n",
    "    \n",
    "    def forward(self, x, stage=\"test\", batch=None):\n",
    "        batch_size = x.shape[0]\n",
    "        res = {}\n",
    "\n",
    "        res[\"hidden_states\"] = self.feature_model.get_hidden_state(x)\n",
    "        if self.feature_extractor == \"ResNet\":\n",
    "            res[\"content_feature\"], conv_feat1 = self.feature_model.get_final_feature(\n",
    "                res[\"hidden_states\"]\n",
    "            )\n",
    "        else:\n",
    "            res[\"content_feature\"] = self.feature_model.get_final_feature(\n",
    "                res[\"hidden_states\"]\n",
    "            )\n",
    "        \n",
    "        # res[\"content_logit\"] = self.cls_content(\n",
    "        #     self.dropout(res[\"content_feature\"])\n",
    "        # ).squeeze(-1)\n",
    "        # res[\"content_based_cls_logit\"] = self.content_based_cls(\n",
    "        #     self.dropout(res[\"content_feature\"])\n",
    "        # ).squeeze(-1)\n",
    "        res[\"speed_logit\"] = self.cls_speed(self.dropout(res[\"content_feature\"]))\n",
    "        res[\"compression_logit\"] = self.cls_compression(\n",
    "            self.dropout(res[\"content_feature\"])\n",
    "        )\n",
    "\n",
    "        # learn a vocoder feature extractor and classifier\n",
    "\n",
    "        hidden_states = res[\"hidden_states\"]\n",
    "        # if stage == 'train':\n",
    "        #     B, C = res[\"hidden_states\"].shape[0:2]\n",
    "        #     L = 100\n",
    "        #     feat_clone = res[\"hidden_states\"].clone()\n",
    "        #     shuffle_id = torch.randperm(B)\n",
    "        #     s = np.random.randint(0, C - 100, 1)[0]\n",
    "        #     feat_clone[:B//2, s:s+L] = 0.5 * feat_clone[:B//2, s:s+L] + 0.5 * res[\"hidden_states\"][shuffle_id[:B//2], s:s+L]\n",
    "        #     feat_clone[B//2:, s:s+L] = 0\n",
    "        #     # print(res[\"hidden_states\"].shape)\n",
    "        #     hidden_states = feat_clone\n",
    "\n",
    "        \n",
    "        if self.feature_extractor == \"ResNet\":\n",
    "            (\n",
    "                res[\"vocoder_feature\"],\n",
    "                conv_feat2,\n",
    "            ) = self.feature_model.get_final_feature_copyed(hidden_states)\n",
    "        else:\n",
    "            res[\"vocoder_feature\"] = self.feature_model.get_final_feature_copyed(hidden_states)\n",
    "            \n",
    "        res[\"vocoder_logit\"] = self.cls_voc(self.dropout(res[\"vocoder_feature\"]))\n",
    "        # res[\"vocoder_based_cls_logit\"] = self.vocoder_based_cls(self.dropout(res[\"vocoder_feature\"])).squeeze(-1)\n",
    "        res[\"voc_based_speed_logit\"] = self.cls_speed(self.dropout(res[\"vocoder_feature\"]))\n",
    "        res[\"content_voc_logit\"] = self.cls_voc(self.dropout(res[\"content_feature\"]))\n",
    "\n",
    "        \n",
    "        # print(feature_aug, res[\"vocoder_feature\"])\n",
    "        voc_feat = res[\"vocoder_feature\"]\n",
    "        content_feat = res[\"content_feature\"]\n",
    "        if stage == 'train' and self.cfg.style_shuffle:\n",
    "            shuffle_id = torch.randperm(batch_size)\n",
    "            voc_feat =  exchange_mu_std(res[\"vocoder_feature\"], res[\"vocoder_feature\"][shuffle_id], dim=-1)\n",
    "            shuffle_id = torch.randperm(batch_size)\n",
    "            content_feat =  exchange_mu_std(res[\"content_feature\"], res[\"content_feature\"][shuffle_id], dim=-1)\n",
    "        res[\"feature\"] = self.fuse_stem_featurs(res[\"content_feature\"], res[\"vocoder_feature\"])\n",
    "        final_feat = self.fuse_stem_featurs(content_feat, voc_feat)\n",
    "        # res[\"feature\"] = self.fuse_stem_featurs(conv_feat1, conv_feat2)\n",
    "        res[\"logit\"] = self.cls_final(self.dropout(final_feat)).squeeze(-1)\n",
    "\n",
    "        \n",
    "        \n",
    "        if stage == \"train\" and self.cfg.feat_shuffle:\n",
    "            shuffle_id = torch.randperm(batch_size)\n",
    "            res[\"shuffle_logit\"] = self.cls_final(\n",
    "                self.dropout(\n",
    "                    # self.fuse_stem_featurs(conv_feat1, conv_feat2[shuffle_id])\n",
    "                    # self.fuse_stem_featurs(res[\"content_feature\"], res[\"vocoder_feature\"][shuffle_id])\n",
    "                    self.fuse_stem_featurs(content_feat, voc_feat[shuffle_id])\n",
    "                )\n",
    "            ).squeeze(-1)\n",
    "            batch[\"shuffle_label\"] = deepcopy(batch[\"label\"])\n",
    "            for i in range(batch_size):\n",
    "                if batch[\"label\"][shuffle_id[i]] == 0 or batch[\"label\"][i] == 0:\n",
    "                    batch[\"shuffle_label\"][i] = 0\n",
    "                else:\n",
    "                    batch[\"shuffle_label\"][i] = 1\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48146a05-6034-4d67-b788-45acb6700ffb",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-09T14:59:47.738745Z",
     "iopub.status.busy": "2023-08-09T14:59:47.738145Z",
     "iopub.status.idle": "2023-08-09T14:59:50.482245Z",
     "shell.execute_reply": "2023-08-09T14:59:50.481094Z",
     "shell.execute_reply.started": "2023-08-09T14:59:47.738693Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "model = AudioModel(vocoder_classes=7)\n",
    "x = torch.randn(32, 1, 48000)\n",
    "_ = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78813c98-620f-4189-adef-6ab216a760bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T14:00:42.856950Z",
     "iopub.status.busy": "2023-08-09T14:00:42.856267Z",
     "iopub.status.idle": "2023-08-09T14:00:42.962920Z",
     "shell.execute_reply": "2023-08-09T14:00:42.961630Z",
     "shell.execute_reply.started": "2023-08-09T14:00:42.856875Z"
    },
    "scrolled": true,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckpt = torch.load(\n",
    "#     \"/home/ay/data/DATA/1-model_save/0-Audio/Ours/LibriSeVoc_cross_dataset/version_7/checkpoints/best-epoch=3-val-auc=0.99.ckpt\"\n",
    "# )\n",
    "\n",
    "# state_dict = ckpt[\"state_dict\"]\n",
    "\n",
    "# state_dict2 = {key.replace(\"model.\", \"\", 1): state_dict[key] for key in state_dict}\n",
    "\n",
    "# model.load_state_dict(state_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b4a5e4-fd82-4cb3-9ac7-2b9a5b04fc65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-18T13:34:38.572852Z",
     "iopub.status.busy": "2024-03-18T13:34:38.572257Z",
     "iopub.status.idle": "2024-03-18T13:34:38.593395Z",
     "shell.execute_reply": "2024-03-18T13:34:38.590563Z",
     "shell.execute_reply.started": "2024-03-18T13:34:38.572793Z"
    }
   },
   "outputs": [],
   "source": [
    "def exchange_mu_std(x, y, dim=None):\n",
    "    mu_x = torch.mean(x, dim=dim, keepdims=True)\n",
    "    mu_y = torch.mean(y, dim=dim, keepdims=True)\n",
    "    std_x = torch.std(x, dim=dim, keepdims=True)\n",
    "    std_y = torch.std(y, dim=dim, keepdims=True)\n",
    "\n",
    "    alpha = np.random.rand()\n",
    "    target_mu = alpha * mu_x + (1-alpha) * mu_y \n",
    "    target_std = alpha * std_x + (1-alpha) * std_y\n",
    "    z = target_std * ( (x - mu_x) / std_x) + target_mu\n",
    "\n",
    "    noise_level = 20\n",
    "    add_noise_level=np.random.randint(0, noise_level) / 100\n",
    "    mult_noise_level=np.random.randint(0, noise_level) / 100\n",
    "    z = _noise(x, add_noise_level=add_noise_level, mult_noise_level=mult_noise_level)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a1165a4-bd59-442c-a06c-688a5c7cd6b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-19T07:08:39.395272Z",
     "iopub.status.busy": "2024-03-19T07:08:39.394583Z",
     "iopub.status.idle": "2024-03-19T07:08:39.408065Z",
     "shell.execute_reply": "2024-03-19T07:08:39.405947Z",
     "shell.execute_reply.started": "2024-03-19T07:08:39.395210Z"
    }
   },
   "outputs": [],
   "source": [
    "def _noise(x, add_noise_level=0.0, mult_noise_level=0.0):\n",
    "    add_noise = 0.0\n",
    "    mult_noise = 1.0\n",
    "    if add_noise_level > 0.0:\n",
    "        add_noise = add_noise_level * np.random.beta(2, 5) * torch.FloatTensor(x.shape).normal_().to(x.device)\n",
    "    if mult_noise_level > 0.0:\n",
    "        mult_noise = mult_noise_level * np.random.beta(2, 5) * (2*torch.FloatTensor(x.shape).uniform_()-1).to(x.device) + 1 \n",
    "    return mult_noise * x + add_noise"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
