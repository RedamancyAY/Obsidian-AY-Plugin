{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f183a79-d07e-4e9f-9606-520ada0bedf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T02:21:45.984324Z",
     "iopub.status.busy": "2023-08-02T02:21:45.983213Z",
     "iopub.status.idle": "2023-08-02T02:21:46.011547Z",
     "shell.execute_reply": "2023-08-02T02:21:46.010771Z",
     "shell.execute_reply.started": "2023-08-02T02:21:45.984276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1caf9846-a1a5-49b5-b121-fb92bac2868c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T02:21:46.898101Z",
     "iopub.status.busy": "2023-08-02T02:21:46.897031Z",
     "iopub.status.idle": "2023-08-02T02:21:46.923044Z",
     "shell.execute_reply": "2023-08-02T02:21:46.922260Z",
     "shell.execute_reply.started": "2023-08-02T02:21:46.898057Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ay.torch.nn import LambdaFunctionModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4a77b-985e-4aac-a4c7-67dad9261ac5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from .gradient_reversal import GradientReversal\n",
    "from .model_RawNet2 import LayerNorm, SincConv_fast\n",
    "from .utils import AdaptiveConv1d, DepthwiseSeparableConv1d, Multi_Head_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8486292c-054f-4225-854e-12604b309f8a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-02T02:21:47.970210Z",
     "iopub.status.busy": "2023-08-02T02:21:47.969152Z",
     "iopub.status.idle": "2023-08-02T02:21:48.459066Z",
     "shell.execute_reply": "2023-08-02T02:21:48.458346Z",
     "shell.execute_reply.started": "2023-08-02T02:21:47.970165Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [],
   "source": [
    "from gradient_reversal import GradientReversal\n",
    "from model_RawNet2 import LayerNorm, SincConv_fast\n",
    "from utils import AdaptiveConv1d, DepthwiseSeparableConv1d, Multi_Head_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36cb802c-4caa-4e7f-9358-4ec0758a03cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T02:21:49.564792Z",
     "iopub.status.busy": "2023-08-02T02:21:49.563747Z",
     "iopub.status.idle": "2023-08-02T02:21:49.597648Z",
     "shell.execute_reply": "2023-08-02T02:21:49.596931Z",
     "shell.execute_reply.started": "2023-08-02T02:21:49.564745Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        fan_out //= m.groups\n",
    "        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, (nn.Conv3d, nn.Conv1d)):\n",
    "        nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2.0))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d, nn.BatchNorm3d, nn.LayerNorm)):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b10b1-44df-4b9f-a8f2-5be02d82cc4f",
   "metadata": {},
   "source": [
    "# Feature Model\n",
    "\n",
    "## Multi-Scale Fusion Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0dd7314-a750-470a-aade-d8e2ca6b6101",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-02T02:25:41.116869Z",
     "iopub.status.busy": "2023-08-02T02:25:41.116486Z",
     "iopub.status.idle": "2023-08-02T02:25:41.145118Z",
     "shell.execute_reply": "2023-08-02T02:25:41.144648Z",
     "shell.execute_reply.started": "2023-08-02T02:25:41.116847Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiScaleFusion(nn.Module):\n",
    "    def __init__(self, n_dim, n_head=1, samples_per_frame=400):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_dim = n_dim\n",
    "        self.samples_per_frame = samples_per_frame\n",
    "        self.norm = nn.BatchNorm1d(n_dim)\n",
    "\n",
    "        scales = [1, 5, 10]\n",
    "        assert samples_per_frame % scales[-1] == 0, samples_per_frame\n",
    "\n",
    "        self.down_samples = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.AvgPool1d(scales[i] * 3, stride=scales[i], padding=scales[i])\n",
    "                    if i > 0\n",
    "                    else nn.Identity(),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # nn.GELU(),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(3)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.up_samples = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Upsample(scale_factor=scales[i]) if i > 0 else nn.Identity(),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                    # nn.GELU(),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(3)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.conv_fusion = nn.Sequential(\n",
    "            nn.Conv1d(n_dim * 3, n_dim, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(n_dim, n_dim * 3, 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.mha = Multi_Head_Attention(\n",
    "            max_k=80, embed_dim=n_dim, num_heads=n_head, dropout=0.1\n",
    "        )\n",
    "        self.attn_upsamples = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Upsample(scale_factor=samples_per_frame // scales[i]),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(3)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.register_parameter(\"alpha\", nn.Parameter(torch.ones(1, n_dim, 1)))\n",
    "        self.register_parameter(\"beta\", nn.Parameter(torch.ones(1, n_dim * 3, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        short_cut = x\n",
    "        x = self.norm(x)\n",
    "        n_frames = x.shape[-1] // self.samples_per_frame\n",
    "        avg_pool = partial(F.adaptive_avg_pool1d, output_size=n_frames)\n",
    "        max_pool = partial(F.adaptive_max_pool1d, output_size=n_frames)\n",
    "\n",
    "        frame_feat = []\n",
    "        ms_feat = []\n",
    "        for i in range(3):\n",
    "            y = self.down_samples[i](x)\n",
    "            # print(\"scale %d : \"%i, y.shape)\n",
    "            ms_feat.append(y)\n",
    "            attn = avg_pool(y) + max_pool(y)  # (B, n_dim, n_frames)\n",
    "            frame_feat.append(attn)\n",
    "            # frame_feat.append(attn.transpose(1, 2))  # (B, n_frames, n_dim)\n",
    "\n",
    "        frame_feat = torch.concat(frame_feat, dim=1)  # (B, 3*n_dim, n_frames)\n",
    "        frame_feat = self.conv_fusion(frame_feat)\n",
    "        frame_feat = torch.split(frame_feat, self.n_dim, dim=1)\n",
    "        frame_feat = [x.transpose(1, 2) for x in frame_feat]\n",
    "\n",
    "        v, k, q = frame_feat\n",
    "        attn = self.mha(q, k, v)\n",
    "        attn = attn.transpose(1, 2)  # (B, n_dim, n_frames)\n",
    "        # print(\"attn shape: \", attn.shape)\n",
    "\n",
    "        rec_feat = []\n",
    "        for i in range(3):\n",
    "            _attn = self.attn_upsamples[i](attn)\n",
    "            # y = ms_feat[i] + ms_feat[i] * _attn\n",
    "            y = (\n",
    "                ms_feat[i]\n",
    "                + self.beta[:, i * self.n_dim : (i + 1) * self.n_dim, :] * _attn\n",
    "            )\n",
    "            y = self.up_samples[i](y)\n",
    "            rec_feat.append(y)\n",
    "\n",
    "        rec_feat = rec_feat[0] + rec_feat[1] + rec_feat[2]\n",
    "        x = x + self.alpha * rec_feat\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2abdeab3-e785-4f72-90dc-6194edae8069",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-02T02:25:41.939757Z",
     "iopub.status.busy": "2023-08-02T02:25:41.939095Z",
     "iopub.status.idle": "2023-08-02T02:25:42.001222Z",
     "shell.execute_reply": "2023-08-02T02:25:42.000750Z",
     "shell.execute_reply.started": "2023-08-02T02:25:41.939711Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-activity"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 0 :  torch.Size([2, 32, 4000])\n",
      "scale 1 :  torch.Size([2, 32, 800])\n",
      "scale 2 :  torch.Size([2, 32, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3644, -0.0895,  1.7713,  ..., -0.3791, -0.2016,  0.1933],\n",
       "         [-1.2864,  0.3678,  0.9201,  ...,  1.1390, -0.1187, -0.5945],\n",
       "         [ 0.6886, -0.9900, -0.1911,  ..., -0.0610,  0.6797, -0.5514],\n",
       "         ...,\n",
       "         [ 0.2083,  1.3119, -0.2490,  ...,  1.2521,  0.6200, -0.0304],\n",
       "         [-0.4866, -0.1023, -0.0281,  ...,  0.3806,  0.0854,  1.3124],\n",
       "         [ 0.2304,  1.1231, -0.0236,  ..., -0.8259, -2.3159, -0.6258]],\n",
       "\n",
       "        [[ 0.5039,  1.0818, -0.5126,  ..., -0.2566, -0.0946,  1.2096],\n",
       "         [-0.0493, -0.7719, -0.1083,  ..., -0.7863, -2.3044,  0.0159],\n",
       "         [ 0.6359, -0.2596,  0.9761,  ..., -0.2098, -1.4546, -0.3291],\n",
       "         ...,\n",
       "         [ 0.4035, -1.0652, -0.8620,  ...,  0.1276,  0.8100,  0.2547],\n",
       "         [-0.6034,  1.2224,  0.0097,  ..., -1.6988,  1.0201, -0.5239],\n",
       "         [-0.0815,  0.3229,  0.1261,  ...,  1.1820,  0.1092, -0.3421]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiScaleFusion(n_dim=32)\n",
    "x = torch.randn(2, 32, 4000)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dface8f4-ed14-4fd0-81dd-675ce460a669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-02T02:25:49.072939Z",
     "iopub.status.busy": "2023-08-02T02:25:49.071891Z",
     "iopub.status.idle": "2023-08-02T02:25:49.103936Z",
     "shell.execute_reply": "2023-08-02T02:25:49.103183Z",
     "shell.execute_reply.started": "2023-08-02T02:25:49.072891Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_stage(\n",
    "    n_dim_in, n_dim_out, n_blocks, samples_per_frame, n_head=1, downsample_factor=1\n",
    "):\n",
    "    # print(n_dim_in, n_dim_out)\n",
    "    conv1 = nn.Conv1d(n_dim_in, n_dim_out, 3, stride=1, padding=1)\n",
    "    conv_blocks = [\n",
    "        MultiScaleFusion(\n",
    "            n_dim=n_dim_out,\n",
    "            n_head=n_head,\n",
    "            samples_per_frame=samples_per_frame,\n",
    "        )\n",
    "        for i in range(n_blocks)\n",
    "    ]\n",
    "    module = nn.Sequential(conv1, *conv_blocks)\n",
    "    if downsample_factor > 1:\n",
    "        module.add_module(\n",
    "            \"down-sample\", nn.Conv1d(n_dim_out, n_dim_out, 5, stride=2, padding=2)\n",
    "        )\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a239108e-1238-4e9d-b1b8-9f1f6c3a4ccd",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-02T02:25:49.985665Z",
     "iopub.status.busy": "2023-08-02T02:25:49.984641Z",
     "iopub.status.idle": "2023-08-02T02:25:53.388550Z",
     "shell.execute_reply": "2023-08-02T02:25:53.387469Z",
     "shell.execute_reply.started": "2023-08-02T02:25:49.985621Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-activity"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-08-02 10:25:50 43379:43379 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 0 :  torch.Size([16, 128, 16000])\n",
      "scale 1 :  torch.Size([16, 128, 3200])\n",
      "scale 2 :  torch.Size([16, 128, 1600])\n",
      "scale 0 :  torch.Size([16, 128, 16000])\n",
      "scale 1 :  torch.Size([16, 128, 3200])\n",
      "scale 2 :  torch.Size([16, 128, 1600])\n",
      "scale 0 :  torch.Size([16, 128, 16000])\n",
      "scale 1 :  torch.Size([16, 128, 3200])\n",
      "scale 2 :  torch.Size([16, 128, 1600])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-08-02 10:25:52 43379:43379 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-08-02 10:25:52 43379:43379 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "        aten::mkldnn_convolution        79.37%        2.125s        79.43%        2.127s      34.300ms            62  \n",
      "                       aten::add         4.59%     122.847ms         4.59%     122.847ms       4.095ms            30  \n",
      "                aten::avg_pool2d         3.30%      88.420ms         3.30%      88.420ms      14.737ms             6  \n",
      "        aten::upsample_nearest1d         2.91%      77.796ms         2.92%      78.112ms       5.207ms            15  \n",
      "                aten::clamp_min_         2.41%      64.624ms         2.41%      64.624ms       2.154ms            30  \n",
      "                       aten::mul         2.08%      55.660ms         2.08%      55.720ms       2.322ms            24  \n",
      "         aten::native_batch_norm         1.90%      50.841ms         1.90%      50.899ms      16.966ms             3  \n",
      "                   aten::normal_         1.47%      39.372ms         1.47%      39.372ms      39.372ms             1  \n",
      "       aten::adaptive_max_pool2d         0.87%      23.182ms         0.87%      23.182ms       2.576ms             9  \n",
      "      aten::_adaptive_avg_pool2d         0.63%      16.814ms         0.63%      16.902ms       1.878ms             9  \n",
      "                       aten::bmm         0.05%       1.437ms         0.09%       2.436ms     270.667us             9  \n",
      "              aten::_convolution         0.05%       1.354ms        79.50%        2.129s      34.333ms            62  \n",
      "                     aten::empty         0.05%       1.280ms         0.05%       1.280ms       3.754us           341  \n",
      "               aten::convolution         0.05%       1.257ms        79.55%        2.130s      34.354ms            62  \n",
      "                     aten::relu_         0.04%       1.003ms         2.45%      65.627ms       2.188ms            30  \n",
      "                     aten::copy_         0.02%     582.000us         0.02%     582.000us       4.311us           135  \n",
      "                   aten::squeeze         0.02%     504.000us         0.02%     668.000us       7.032us            95  \n",
      "                 aten::unsqueeze         0.02%     447.000us         0.02%     567.000us       3.416us           166  \n",
      "               aten::as_strided_         0.02%     421.000us         0.02%     421.000us       6.790us            62  \n",
      "                aten::as_strided         0.01%     349.000us         0.01%     349.000us       0.460us           759  \n",
      "                    aten::conv1d         0.01%     301.000us        79.56%        2.130s      34.358ms            62  \n",
      "                    aten::select         0.01%     273.000us         0.01%     275.000us       0.751us           366  \n",
      "                       aten::cat         0.01%     271.000us         0.01%     298.000us      99.333us             3  \n",
      "         aten::native_layer_norm         0.01%     227.000us         0.02%     483.000us     161.000us             3  \n",
      "                     aten::slice         0.01%     220.000us         0.01%     227.000us       5.044us            45  \n",
      "                     aten::clone         0.01%     219.000us         0.03%     892.000us       7.252us           123  \n",
      "                    aten::einsum         0.01%     166.000us         0.02%     649.000us     108.167us             6  \n",
      "                        aten::mm         0.01%     160.000us         0.01%     160.000us      53.333us             3  \n",
      "                      aten::div_         0.01%     154.000us         0.01%     194.000us      32.333us             6  \n",
      "                aten::empty_like         0.01%     147.000us         0.01%     197.000us       1.492us           132  \n",
      "                aten::bernoulli_         0.01%     138.000us         0.01%     141.000us      23.500us             6  \n",
      "                     aten::fill_         0.01%     136.000us         0.01%     136.000us       9.067us            15  \n",
      "                      aten::add_         0.00%     125.000us         0.00%     125.000us      20.833us             6  \n",
      "                   aten::permute         0.00%     116.000us         0.00%     118.000us       2.810us            42  \n",
      "       aten::adaptive_avg_pool1d         0.00%     105.000us         0.64%      17.252ms       1.917ms             9  \n",
      "                   aten::reshape         0.00%     102.000us         0.01%     182.000us       3.792us            48  \n",
      "       aten::adaptive_max_pool1d         0.00%      96.000us         0.88%      23.429ms       2.603ms             9  \n",
      "                      aten::ones         0.00%      96.000us         0.01%     224.000us      14.933us            15  \n",
      "                aten::avg_pool1d         0.00%      92.000us         3.31%      88.630ms      14.772ms             6  \n",
      "            aten::_reshape_alias         0.00%      88.000us         0.00%      88.000us       1.833us            48  \n",
      "              aten::index_select         0.00%      83.000us         0.00%      99.000us      33.000us             3  \n",
      "                 aten::transpose         0.00%      70.000us         0.00%      74.000us       3.083us            24  \n",
      "                  aten::_softmax         0.00%      70.000us         0.00%      70.000us      23.333us             3  \n",
      "                   aten::resize_         0.00%      66.000us         0.00%      66.000us       4.400us            15  \n",
      "                aten::contiguous         0.00%      64.000us         0.01%     186.000us      62.000us             3  \n",
      "                    aten::arange         0.00%      59.000us         0.00%      91.000us       7.583us            12  \n",
      "                      aten::view         0.00%      54.000us         0.00%      54.000us       2.571us            21  \n",
      "                    aten::matmul         0.00%      50.000us         0.09%       2.457ms     409.500us             6  \n",
      "                  aten::_to_copy         0.00%      46.000us         0.00%      82.000us       6.833us            12  \n",
      "                   aten::dropout         0.00%      44.000us         0.02%     516.000us      86.000us             6  \n",
      "       aten::adaptive_avg_pool2d         0.00%      36.000us         0.63%      16.938ms       1.882ms             9  \n",
      "                     aten::split         0.00%      34.000us         0.00%      61.000us      20.333us             3  \n",
      "                 aten::embedding         0.00%      33.000us         0.01%     143.000us      47.667us             3  \n",
      "                     aten::clamp         0.00%      32.000us         0.00%      32.000us      10.667us             3  \n",
      "                    aten::narrow         0.00%      29.000us         0.00%      54.000us       4.500us            12  \n",
      "    aten::_batch_norm_impl_index         0.00%      28.000us         1.90%      50.930ms      16.977ms             3  \n",
      "                       aten::sub         0.00%      25.000us         0.00%      25.000us       8.333us             3  \n",
      "                    aten::expand         0.00%      24.000us         0.00%      24.000us       4.000us             6  \n",
      "              aten::_unsafe_view         0.00%      19.000us         0.00%      19.000us       3.167us             6  \n",
      "                      aten::clip         0.00%      18.000us         0.00%      39.000us      13.000us             3  \n",
      "                        aten::to         0.00%      17.000us         0.00%     102.000us       5.667us            18  \n",
      "             aten::empty_strided         0.00%      17.000us         0.00%      17.000us       1.417us            12  \n",
      "                         aten::t         0.00%      16.000us         0.00%      19.000us       6.333us             3  \n",
      "                aten::batch_norm         0.00%      13.000us         1.90%      50.943ms      16.981ms             3  \n",
      "                aten::layer_norm         0.00%      11.000us         0.02%     494.000us     164.667us             3  \n",
      "                    aten::linear         0.00%       9.000us         0.01%     222.000us      74.000us             3  \n",
      "                     aten::randn         0.00%       8.000us         1.47%      39.384ms      39.384ms             1  \n",
      "                   aten::softmax         0.00%       8.000us         0.00%      78.000us      26.000us             3  \n",
      "                    aten::concat         0.00%       7.000us         0.01%     305.000us     101.667us             3  \n",
      "              aten::resolve_conj         0.00%       0.000us         0.00%       0.000us       0.000us           144  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.677s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modle = build_stage(\n",
    "    n_dim_in=32,\n",
    "    n_dim_out=128,\n",
    "    n_blocks=3,\n",
    "    samples_per_frame=400,\n",
    "    downsample_factor=2,\n",
    ")\n",
    "with torch.autograd.profiler.profile(enabled=True) as prof:\n",
    "    x = torch.randn(16, 32, 16000)\n",
    "    _ = modle(x).shape\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89996a-5f84-48a2-af06-dd24a7b8d12b",
   "metadata": {},
   "source": [
    "## Feature Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a46482-d8a5-4648-800c-b56b0182173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims=[32, 32, 64, 64, 128],\n",
    "        n_blocks=[2, 2, 2, 4, 2],\n",
    "        n_heads=[1, 1, 2, 2, 4],\n",
    "        samples_per_frame=400,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.samples_per_frame = samples_per_frame\n",
    "        self.conv_head = nn.Sequential(\n",
    "            nn.Conv1d(1, dims[0], 4, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(dims[0], dims[0], 3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                build_stage(\n",
    "                    n_dim_in=dims[max(i - 1, 0)],\n",
    "                    n_dim_out=dims[i],\n",
    "                    n_blocks=n_blocks[i],\n",
    "                    n_head=n_heads[i],\n",
    "                    samples_per_frame=samples_per_frame // (4 * (2**i)),\n",
    "                    downsample_factor=2 if i < 2 else 1,\n",
    "                )\n",
    "                for i in range(len(dims))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.conv_head = nn.Sequential(\n",
    "            SincConv_fast(out_channels=32, kernel_size=1024, padding=512),\n",
    "            LambdaFunctionModule(lambda x: torch.abs(x)),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(negative_slope=0.3),\n",
    "        )\n",
    "        self.gru = nn.GRU(input_size = 128,\n",
    "            hidden_size = 128,\n",
    "            num_layers = 3,\n",
    "            batch_first = True)\n",
    "        \n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def get_feature(self, x):\n",
    "        audio_length = x.shape[-1]\n",
    "        audio_frames = audio_length // self.samples_per_frame\n",
    "\n",
    "        x = self.conv_head(x)\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            # print(\"Input of the %d-th stage\"%(i+1), x.shape)\n",
    "            x = stage(x)  # (B, C, frames)\n",
    "            # print(\"Output of the %d-th stage\"%(i+1), x.shape)\n",
    "\n",
    "        # classfication\n",
    "        # feature = torch.mean(x, dim=-1)\n",
    "        x = x.permute(0, 2, 1)  #(batch, filt, time) >> (batch, time, filt)\n",
    "        self.gru.flatten_parameters()\n",
    "        x, _ = self.gru(x)\n",
    "        x = x[:,-1,:]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = self.get_feature(x)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464c3db-b29f-43f4-8802-cd5a05af83ee",
   "metadata": {},
   "source": [
    "# Audio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b888f305-780a-44a6-8766-7d3c621a94d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-01T14:15:23.635110Z",
     "iopub.status.busy": "2023-08-01T14:15:23.634555Z",
     "iopub.status.idle": "2023-08-01T14:15:23.640794Z",
     "shell.execute_reply": "2023-08-01T14:15:23.639635Z",
     "shell.execute_reply.started": "2023-08-01T14:15:23.635064Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for p in range(10000):\n",
    "#     a = 2/ (1 + torch.exp(-10 * torch.tensor(p/10000))) - 1\n",
    "#     print(p, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b831f-95fd-4320-b30c-7aa01061e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims=[32, 32, 64, 64, 128],\n",
    "        n_blocks=[2, 2, 2, 4, 2],\n",
    "        n_heads=[1, 1, 2, 2, 4],\n",
    "        samples_per_frame=640,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = LayerNorm(48000)\n",
    "        self.feature_model = FeatureModel(\n",
    "            dims=dims,\n",
    "            n_blocks=n_blocks,\n",
    "            n_heads=n_heads,\n",
    "            samples_per_frame=samples_per_frame,\n",
    "        )\n",
    "\n",
    "        self.mlp_v, self.mlp_c = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(dims[-1], dims[-1]),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(dims[-1], dims[-1]),\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.content_head = nn.Linear(dims[-1], 1, bias=False)\n",
    "        self.vocoder_head = nn.Linear(dims[-1], 8, bias=False)\n",
    "\n",
    "        self.grl = GradientReversal(alpha=0.01)\n",
    "\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x, stage=\"test\"):\n",
    "        x = self.norm(x)\n",
    "        feature = self.feature_model.get_feature(x)\n",
    "        # print(feature.shape)\n",
    "\n",
    "        vocoder_feature = self.mlp_v(feature)\n",
    "        content_feature = self.mlp_c(feature)\n",
    "\n",
    "        # feature = self.dropout(feature)\n",
    "        content_logit = self.content_head(self.dropout(content_feature)).squeeze()\n",
    "        vocoder_logit = self.vocoder_head(self.grl(self.dropout(vocoder_feature)))\n",
    "\n",
    "        return {\n",
    "            \"feature\": feature,\n",
    "            \"content_feature\": content_feature,\n",
    "            \"vocoder_feature\": vocoder_feature,\n",
    "            \"logit\": content_logit,\n",
    "            \"vocoder_logit\": vocoder_logit,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dd9d2-e854-40df-a59a-eeef961f1002",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "model = AudioModel()\n",
    "x = torch.randn(32, 1, 48000)\n",
    "model(x)\n",
    "with torch.autograd.profiler.profile(enabled=True) as prof:\n",
    "    x = torch.randn(16, 1, 48000)\n",
    "    _ = model(x).shape\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c0530-132e-4db4-b9d4-845df3669241",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "model.to(\"cuda:1\")\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = torch.randn(16, 1, 48000)\n",
    "y = Variable(x, requires_grad=True).to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a6189-bf2a-49e9-a685-5c32d9f53af0",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    z = model(y)\n",
    "    print(y.shape)\n",
    "    z = torch.sum(z)\n",
    "    z.backward()\n",
    "# NOTE: some columns were removed for brevityM\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
