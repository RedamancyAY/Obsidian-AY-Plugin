{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f183a79-d07e-4e9f-9606-520ada0bedf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T03:00:58.065517Z",
     "iopub.status.busy": "2023-07-23T03:00:58.055824Z",
     "iopub.status.idle": "2023-07-23T03:00:58.236974Z",
     "shell.execute_reply": "2023-07-23T03:00:58.235534Z",
     "shell.execute_reply.started": "2023-07-23T03:00:58.065228Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1caf9846-a1a5-49b5-b121-fb92bac2868c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T03:00:58.764822Z",
     "iopub.status.busy": "2023-07-23T03:00:58.764352Z",
     "iopub.status.idle": "2023-07-23T03:01:00.196967Z",
     "shell.execute_reply": "2023-07-23T03:01:00.195707Z",
     "shell.execute_reply.started": "2023-07-23T03:00:58.764778Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4a77b-985e-4aac-a4c7-67dad9261ac5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from .utils import AdaptiveConv1d, DepthwiseSeparableConv1d, Multi_Head_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8486292c-054f-4225-854e-12604b309f8a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-07-23T03:01:00.816368Z",
     "iopub.status.busy": "2023-07-23T03:01:00.815134Z",
     "iopub.status.idle": "2023-07-23T03:01:01.250427Z",
     "shell.execute_reply": "2023-07-23T03:01:01.249222Z",
     "shell.execute_reply.started": "2023-07-23T03:01:00.816315Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [],
   "source": [
    "from utils import AdaptiveConv1d, DepthwiseSeparableConv1d, Multi_Head_Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cb802c-4caa-4e7f-9358-4ec0758a03cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T02:11:49.604140Z",
     "iopub.status.busy": "2023-07-30T02:11:49.603724Z",
     "iopub.status.idle": "2023-07-30T02:11:49.614448Z",
     "shell.execute_reply": "2023-07-30T02:11:49.613448Z",
     "shell.execute_reply.started": "2023-07-30T02:11:49.604113Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        fan_out //= m.groups\n",
    "        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, (nn.Conv3d, nn.Conv1d)):\n",
    "        nn.init.xavier_uniform_(m.weight, gain=math.sqrt(2.0))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d, nn.BatchNorm3d, nn.LayerNorm)):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b10b1-44df-4b9f-a8f2-5be02d82cc4f",
   "metadata": {},
   "source": [
    "# Feature Model\n",
    "\n",
    "## Multi-Scale Fusion Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0dd7314-a750-470a-aade-d8e2ca6b6101",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-07-23T03:08:58.006111Z",
     "iopub.status.busy": "2023-07-23T03:08:58.005212Z",
     "iopub.status.idle": "2023-07-23T03:08:58.043937Z",
     "shell.execute_reply": "2023-07-23T03:08:58.043443Z",
     "shell.execute_reply.started": "2023-07-23T03:08:58.006064Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiScaleFusion(nn.Module):\n",
    "    def __init__(self, n_dim, n_head=1, samples_per_frame=400):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_dim = n_dim\n",
    "        self.samples_per_frame = samples_per_frame\n",
    "        self.norm = nn.BatchNorm1d(n_dim)\n",
    "\n",
    "        scales = [1, 5, 25]\n",
    "        assert samples_per_frame % scales[-1] == 0, samples_per_frame\n",
    "\n",
    "        self.down_samples = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.AvgPool1d(scales[i] * 3, stride=scales[i], padding=scales[i])\n",
    "                    if i > 0\n",
    "                    else nn.Identity(),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    # nn.GELU(),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(3)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.up_samples = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Upsample(scale_factor=scales[i]) if i > 0 else nn.Identity(),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                    # nn.GELU(),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(3)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.conv_fusion = nn.Sequential(\n",
    "            nn.Conv1d(n_dim * 3, n_dim, 3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(n_dim, n_dim * 3, 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.mha = Multi_Head_Attention(\n",
    "            max_k=80, embed_dim=n_dim, num_heads=n_head, dropout=0.1\n",
    "        )\n",
    "        self.attn_upsamples = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Upsample(scale_factor=samples_per_frame // scales[i]),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv1d(n_dim, n_dim, 3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(3)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.register_parameter(\"alpha\", nn.Parameter(torch.ones(1, n_dim, 1)))\n",
    "        self.register_parameter(\"beta\", nn.Parameter(torch.ones(1, n_dim * 3, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        short_cut = x\n",
    "        x = self.norm(x)\n",
    "        n_frames = x.shape[-1] // self.samples_per_frame\n",
    "        avg_pool = partial(F.adaptive_avg_pool1d, output_size=n_frames)\n",
    "        max_pool = partial(F.adaptive_max_pool1d, output_size=n_frames)\n",
    "\n",
    "        frame_feat = []\n",
    "        ms_feat = []\n",
    "        for i in range(3):\n",
    "            y = self.down_samples[i](x)\n",
    "            # print(\"scale %d : \"%i, y.shape)\n",
    "            ms_feat.append(y)\n",
    "            attn = avg_pool(y) + max_pool(y) # (B, n_dim, n_frames)\n",
    "            frame_feat.append(attn)  \n",
    "            # frame_feat.append(attn.transpose(1, 2))  # (B, n_frames, n_dim)\n",
    "\n",
    "        frame_feat = torch.concat(frame_feat, dim=1) # (B, 3*n_dim, n_frames)\n",
    "        frame_feat = self.conv_fusion(frame_feat)\n",
    "        frame_feat = torch.split(frame_feat, self.n_dim, dim=1)\n",
    "        frame_feat = [x.transpose(1, 2) for x in frame_feat]\n",
    "        \n",
    "        v, k, q = frame_feat\n",
    "        attn = self.mha(q, k, v)\n",
    "        attn = attn.transpose(1, 2)  # (B, n_dim, n_frames)\n",
    "        # print(\"attn shape: \", attn.shape)\n",
    "\n",
    "        rec_feat = []\n",
    "        for i in range(3):\n",
    "            _attn = self.attn_upsamples[i](attn)\n",
    "            # y = ms_feat[i] + ms_feat[i] * _attn\n",
    "            y = (\n",
    "                ms_feat[i]\n",
    "                + self.beta[:, i * self.n_dim : (i + 1) * self.n_dim, :] * _attn\n",
    "            )\n",
    "            y = self.up_samples[i](y)\n",
    "            rec_feat.append(y)\n",
    "\n",
    "        rec_feat = rec_feat[0] + rec_feat[1] + rec_feat[2]\n",
    "        x = x + self.alpha * rec_feat\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2abdeab3-e785-4f72-90dc-6194edae8069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T03:08:58.926908Z",
     "iopub.status.busy": "2023-07-23T03:08:58.926022Z",
     "iopub.status.idle": "2023-07-23T03:08:58.989698Z",
     "shell.execute_reply": "2023-07-23T03:08:58.989197Z",
     "shell.execute_reply.started": "2023-07-23T03:08:58.926865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 0 :  torch.Size([2, 32, 4000])\n",
      "scale 1 :  torch.Size([2, 32, 800])\n",
      "scale 2 :  torch.Size([2, 32, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3087,  0.4932, -1.8317,  ..., -1.2576, -0.0389,  0.3358],\n",
       "         [-0.0251,  0.3422,  1.6115,  ...,  1.9160,  0.3780,  1.5502],\n",
       "         [-1.0699, -1.0264,  0.5670,  ...,  1.7274,  0.2315, -1.4134],\n",
       "         ...,\n",
       "         [-0.4271,  1.2949,  0.8774,  ..., -1.1829, -0.8792,  0.0545],\n",
       "         [-0.2164, -0.3699, -0.0693,  ...,  0.6403, -1.7195,  2.0564],\n",
       "         [-0.2575,  0.7985,  0.7604,  ..., -0.2661, -0.8480,  0.9541]],\n",
       "\n",
       "        [[-0.2964,  0.5440,  1.2209,  ..., -0.1092, -0.1830,  0.0920],\n",
       "         [ 1.4341, -0.1512,  0.4483,  ..., -1.2406, -0.4744,  0.0289],\n",
       "         [ 0.7354,  0.8546,  1.3860,  ...,  0.2154,  0.9761, -0.9663],\n",
       "         ...,\n",
       "         [ 0.1285, -0.8272, -0.5731,  ..., -0.4903,  0.4128,  0.9564],\n",
       "         [ 0.0073, -0.2415,  0.0325,  ..., -1.5466,  1.5740,  0.8696],\n",
       "         [-1.4969,  1.0019,  0.1870,  ...,  1.4906, -1.0505, -1.5426]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiScaleFusion(n_dim=32)\n",
    "x = torch.randn(2, 32, 4000)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dface8f4-ed14-4fd0-81dd-675ce460a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stage(\n",
    "    n_dim_in, n_dim_out, n_blocks, samples_per_frame, n_head=1, downsample_factor=1\n",
    "):\n",
    "    # print(n_dim_in, n_dim_out)\n",
    "    conv1 = nn.Conv1d(n_dim_in, n_dim_out, 3, stride=1, padding=1)\n",
    "    conv_blocks = [\n",
    "        MultiScaleFusion(\n",
    "            n_dim=n_dim_out,\n",
    "            n_head=n_head,\n",
    "            samples_per_frame=samples_per_frame,\n",
    "        )\n",
    "        for i in range(n_blocks)\n",
    "    ]\n",
    "    module = nn.Sequential(conv1, *conv_blocks)\n",
    "    if downsample_factor > 1:\n",
    "        module.add_module(\n",
    "            \"down-sample\", nn.Conv1d(n_dim_out, n_dim_out, 5, stride=2, padding=2)\n",
    "        )\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a239108e-1238-4e9d-b1b8-9f1f6c3a4ccd",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-activity"
    ]
   },
   "outputs": [],
   "source": [
    "modle = build_stage(\n",
    "    n_dim_in=32,\n",
    "    n_dim_out=128,\n",
    "    n_blocks=3,\n",
    "    samples_per_frame=400,\n",
    "    downsample_factor=2,\n",
    ")\n",
    "with torch.autograd.profiler.profile(enabled=True) as prof:\n",
    "    x = torch.randn(16, 32, 24000)\n",
    "    _ = modle(x).shape\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89996a-5f84-48a2-af06-dd24a7b8d12b",
   "metadata": {},
   "source": [
    "## Feature Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a46482-d8a5-4648-800c-b56b0182173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims=[32, 64, 128],\n",
    "        n_blocks=[2, 6, 2],\n",
    "        n_heads=[1, 2, 4],\n",
    "        samples_per_frame=400,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.samples_per_frame = samples_per_frame\n",
    "        self.conv_head = nn.Sequential(\n",
    "            nn.Conv1d(1, dims[0], 4, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(dims[0], dims[0], 3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        self.stages = nn.ModuleList(\n",
    "            [\n",
    "                build_stage(\n",
    "                    n_dim_in=dims[max(i - 1, 0)],\n",
    "                    n_dim_out=dims[i],\n",
    "                    n_blocks=n_blocks[i],\n",
    "                    n_head=n_heads[i],\n",
    "                    samples_per_frame=400 // (4 * (2**i)),\n",
    "                    downsample_factor=2 if i < 2 else 1,\n",
    "                )\n",
    "                for i in range(3)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        from asteroid_filterbanks import Encoder, ParamSincFB\n",
    "        from ay.torchaudio.preprocess import PreEmphasis\n",
    "\n",
    "        self.audio_preprocess = nn.Sequential(\n",
    "            PreEmphasis(), nn.InstanceNorm1d(1, eps=1e-4, affine=True)\n",
    "        )\n",
    "        self.time_freq_repr = Encoder(\n",
    "            ParamSincFB(n_filters=256, kernel_size=251, stride=1), padding=125\n",
    "        )\n",
    "\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def preprocess_audio(self, x):\n",
    "        x = self.audio_preprocess(x)\n",
    "        x = torch.abs(self.time_freq_repr(x))\n",
    "        x = torch.log(x + 1e-6)\n",
    "        x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "        return x\n",
    "\n",
    "    def get_feature(self, x):\n",
    "        audio_length = x.shape[-1]\n",
    "        audio_frames = audio_length // self.samples_per_frame\n",
    "\n",
    "        x = self.conv_head(x)\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            # print(\"Input of the %d-th stage\"%(i+1), x.shape)\n",
    "            x = stage(x)  # (B, C, frames)\n",
    "            # print(\"Output of the %d-th stage\"%(i+1), x.shape)\n",
    "\n",
    "        # classfication\n",
    "        feature = torch.mean(x, dim=-1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = self.get_feature(x)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464c3db-b29f-43f4-8802-cd5a05af83ee",
   "metadata": {},
   "source": [
    "# Audio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b831f-95fd-4320-b30c-7aa01061e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims=[32, 64, 128],\n",
    "        n_blocks=[2, 6, 2],\n",
    "        n_heads=[1, 2, 4],\n",
    "        samples_per_frame=400,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_model = FeatureModel(\n",
    "            dims=dims,\n",
    "            n_blocks=n_blocks,\n",
    "            n_heads=n_heads,\n",
    "            samples_per_frame=samples_per_frame,\n",
    "        )\n",
    "\n",
    "        self.mlp_v, self.mlp_e = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(dims[-1], dims[-1]),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(dims[-1], dims[-1]),\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.content_head = nn.Linear(dims[-1], 1, bias=False)\n",
    "        self.vocoder_head = nn.Linear(dims[-1], 8, bias=False)\n",
    "        self.final_head = nn.Linear(dims[-1] * 2, 1, bias=False)\n",
    "\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x, stage=\"test\"):\n",
    "\n",
    "        \n",
    "        feature = self.feature_model.get_feature(x)\n",
    "\n",
    "        vocoder_feature = self.mlp_v(feature)\n",
    "        content_feature = self.mlp_e(feature)\n",
    "\n",
    "        # feature = self.dropout(feature)\n",
    "        emotion_logit = self.emotion_head(self.dropout(emotion_feature)).squeeze()\n",
    "        vocoder_logit = self.vocoder_head(self.dropout(vocoder_feature))\n",
    "\n",
    "        if stage == \"train\":\n",
    "            noise = torch.randn(vocoder_feature.shape).to(vocoder_feature.device)\n",
    "            vocoder_feature = vocoder_feature + noise * 0.1\n",
    "\n",
    "        logit = self.final_head(\n",
    "            self.dropout(\n",
    "                torch.concat(\n",
    "                    [emotion_feature, vocoder_feature],\n",
    "                    dim=-1,\n",
    "                )\n",
    "            )\n",
    "        ).squeeze()\n",
    "\n",
    "        adv_logit = self.final_head(\n",
    "            self.dropout(\n",
    "                torch.concat(\n",
    "                    [emotion_feature, vocoder_feature[torch.randperm(x.shape[0])]],\n",
    "                    dim=-1,\n",
    "                )\n",
    "            )\n",
    "        ).squeeze()\n",
    "\n",
    "        return {\n",
    "            \"feature\": feature,\n",
    "            \"emotion_feature\": emotion_feature,\n",
    "            \"vocoder_feature\": vocoder_feature,\n",
    "            \"logit\": logit,\n",
    "            \"vocoder_logit\": vocoder_logit,\n",
    "            \"emotion_logit\": emotion_logit,\n",
    "            \"adv_logit\":adv_logit\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dd9d2-e854-40df-a59a-eeef961f1002",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "model = AudioModel()\n",
    "x = torch.randn(32, 1, 48000)\n",
    "model(x)\n",
    "with torch.autograd.profiler.profile(enabled=True) as prof:\n",
    "    x = torch.randn(16, 1, 48000)\n",
    "    _ = model(x).shape\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c0530-132e-4db4-b9d4-845df3669241",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "model.to(\"cuda:1\")\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = torch.randn(16, 1, 48000)\n",
    "y = Variable(x, requires_grad=True).to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a6189-bf2a-49e9-a685-5c32d9f53af0",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    z = model(y)\n",
    "    print(y.shape)\n",
    "    z = torch.sum(z)\n",
    "    z.backward()\n",
    "# NOTE: some columns were removed for brevityM\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
