{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f183a79-d07e-4e9f-9606-520ada0bedf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T13:59:30.798027Z",
     "iopub.status.busy": "2023-08-09T13:59:30.797438Z",
     "iopub.status.idle": "2023-08-09T13:59:30.941570Z",
     "shell.execute_reply": "2023-08-09T13:59:30.940863Z",
     "shell.execute_reply.started": "2023-08-09T13:59:30.797978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf9846-a1a5-49b5-b121-fb92bac2868c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from ay2.torch.nn import LambdaFunctionModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311849e6-480b-41a6-8a5d-9dc525a294fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T06:43:46.308844Z",
     "iopub.status.busy": "2024-01-11T06:43:46.308203Z",
     "iopub.status.idle": "2024-01-11T06:43:46.315643Z",
     "shell.execute_reply": "2024-01-11T06:43:46.314387Z",
     "shell.execute_reply.started": "2024-01-11T06:43:46.308773Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4a77b-985e-4aac-a4c7-67dad9261ac5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "from .feature_extractor import LCNN, MSFM, RawNet2, ResNet\n",
    "# from .gradient_reversal import GradientReversal\n",
    "# from .modules.classifier import Classifier\n",
    "# from .modules.feature_extractor import FeatureExtractor, FeatureExtractor2D\n",
    "# from .modules.model_RawNet2 import LayerNorm, RawNet_FeatureExtractor, SincConv_fast\n",
    "from .utils import weight_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8486292c-054f-4225-854e-12604b309f8a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-09T13:59:33.978716Z",
     "iopub.status.busy": "2023-08-09T13:59:33.978231Z",
     "iopub.status.idle": "2023-08-09T13:59:34.014060Z",
     "shell.execute_reply": "2023-08-09T13:59:34.013014Z",
     "shell.execute_reply.started": "2023-08-09T13:59:33.978671Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [],
   "source": [
    "from gradient_reversal import GradientReversal\n",
    "from modules.classifier import Classifier\n",
    "from modules.feature_extractor import FeatureExtractor, FeatureExtractor2D\n",
    "from modules.model_RawNet2 import LayerNorm, RawNet_FeatureExtractor, SincConv_fast\n",
    "from utils import weight_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "416b831f-95fd-4320-b30c-7aa01061e40a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-09T14:58:15.556580Z",
     "iopub.status.busy": "2023-08-09T14:58:15.555664Z",
     "iopub.status.idle": "2023-08-09T14:58:16.363719Z",
     "shell.execute_reply": "2023-08-09T14:58:16.361220Z",
     "shell.execute_reply.started": "2023-08-09T14:58:15.556463Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor: str,\n",
    "        dims=[32, 64, 64, 64, 128],\n",
    "        n_blocks=[1, 1, 1, 2, 1],\n",
    "        n_heads=[1, 2, 2, 4, 1, 1],\n",
    "        samples_per_frame=640,\n",
    "        gru_node=128,\n",
    "        gru_layers=3,\n",
    "        fc_node=128,\n",
    "        num_classes=1,\n",
    "        vocoder_classes=8,\n",
    "        adv_vocoder=False,\n",
    "        cfg=None,\n",
    "        args=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # self.norm = LayerNorm(48000)\n",
    "        self.dims = dims\n",
    "        # self.feature_model = FeatureExtractor2D(\n",
    "        #     dims=dims,\n",
    "        #     n_blocks=n_blocks,\n",
    "        #     n_heads=n_heads,\n",
    "        #     samples_per_frame=samples_per_frame,\n",
    "        #     use_gru_head=False,\n",
    "        #     gru_node=gru_node,\n",
    "        #     gru_layers=gru_layers,\n",
    "        # )\n",
    "\n",
    "        if feature_extractor == \"LCNN\":\n",
    "            self.feature_model = LCNN()\n",
    "            final_dim = 64\n",
    "        elif feature_extractor == \"RawNet\":\n",
    "            self.feature_model = RawNet2()\n",
    "            final_dim = 1024\n",
    "        elif feature_extractor == \"ResNet\":\n",
    "            self.feature_model = ResNet()\n",
    "            final_dim = 512\n",
    "        elif feature_extractor == \"MSFM\":\n",
    "            self.feature_model = MSFM(\n",
    "                dims=dims,\n",
    "                n_blocks=n_blocks,\n",
    "                n_heads=n_heads,\n",
    "                args=args\n",
    "            )\n",
    "            final_dim = dims[-1]\n",
    "\n",
    "        self.feature_model.copy_final_stage()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.cls_content = nn.Linear(final_dim, 1, bias=False)\n",
    "        self.cls_voc = nn.Linear(final_dim, vocoder_classes + 1, bias=False)\n",
    "        self.cls_final = nn.Sequential(\n",
    "            nn.Linear(final_dim * 2, final_dim * 2, bias=False),\n",
    "            nn.BatchNorm1d(final_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(final_dim * 2, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.debug = 0\n",
    "\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        #     elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        #         nn.init.normal_(m.weight, mean=1, std=0.02)\n",
    "        #         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # self.apply(weight_init)\n",
    "\n",
    "\n",
    "        self.transform = v2.RandomErasing()\n",
    "\n",
    "\n",
    "    def module_similaryity(self):\n",
    "        loss = []\n",
    "        for p1, p2 in zip(\n",
    "                self.feature_model.get_final_block_parameters(),\n",
    "                self.feature_model.get_copied_final_block_parameters(),\n",
    "            ):\n",
    "            _loss = 1 - F.cosine_similarity(p1.view(1, -1), p2.view(1, -1))[0]\n",
    "            loss.append(_loss)\n",
    "        loss = sum(loss) / len(loss)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x, stage=\"test\", batch=None):\n",
    "        batch_size = x.shape[0]\n",
    "        res = {}\n",
    "\n",
    "        x = self.feature_model.preprocess(x, stage=stage)\n",
    "\n",
    "        # if stage == 'train':\n",
    "        #     x = self.transform(x)\n",
    "        \n",
    "        res[\"hidden_states\"] = self.feature_model.get_hidden_state(x)\n",
    "        res[\"content_feature\"] = self.feature_model.get_final_feature(\n",
    "            res[\"hidden_states\"]\n",
    "        )\n",
    "        res[\"content_logit\"] = self.cls_content(\n",
    "            self.dropout(res[\"content_feature\"])\n",
    "        ).squeeze()\n",
    "        \n",
    "\n",
    "        # learn a vocoder feature extractor and classifier\n",
    "        res[\"vocoder_feature\"] = self.feature_model.get_final_feature_copyed(\n",
    "            res[\"hidden_states\"].detach()\n",
    "        )\n",
    "        res[\"vocoder_logit\"] = self.cls_voc(self.dropout(res[\"vocoder_feature\"]))\n",
    "\n",
    "\n",
    "        res[\"content_voc_logit\"] = self.cls_voc(self.dropout(res[\"content_feature\"]))\n",
    "\n",
    "\n",
    "        # res[\"logit\"] = self.cls_final(\n",
    "        #     self.dropout(\n",
    "        #         torch.concat([res[\"content_feature\"], res[\"vocoder_feature\"]], dim=-1)\n",
    "        #     )\n",
    "        # ).squeeze()\n",
    "        # res[\"aug_logit\"] = self.cls_final(\n",
    "        #     self.dropout(\n",
    "        #         torch.concat(\n",
    "        #             [\n",
    "        #                 res[\"content_feature\"],\n",
    "        #                 res[\"vocoder_feature\"][torch.randperm(batch_size)],\n",
    "        #             ],\n",
    "        #             dim=-1,\n",
    "        #         )\n",
    "        #     )\n",
    "        # )\n",
    "        if batch is not None:\n",
    "            vocoder_label = batch[\"vocoder_label\"]\n",
    "            w_tensor = self.cls_voc.weight\n",
    "            logits_vocoder = res[\"vocoder_logit\"]\n",
    "            perturbation = 1\n",
    "            epsilon = 1e-5\n",
    "            grad_aug = -1 * w_tensor[vocoder_label] + torch.matmul(\n",
    "                logits_vocoder.detach(), w_tensor\n",
    "            )\n",
    "            FGSM_attack = perturbation * (\n",
    "                grad_aug.detach()\n",
    "                / (grad_aug.detach().norm(2, dim=1, keepdim=True) + epsilon)\n",
    "            )\n",
    "            # print(grad_aug.shape, FGSM_attack.shape, res[\"vocoder_feature\"].shape)\n",
    "            ratio = random.random()\n",
    "            # ratio = 1.0\n",
    "            feature_aug = ratio * FGSM_attack\n",
    "        else:\n",
    "            ratio = 0\n",
    "            feature_aug = 0.0\n",
    "\n",
    "        # print(feature_aug, res[\"vocoder_feature\"])\n",
    "        res[\"logit\"] = self.cls_final(\n",
    "            self.dropout(\n",
    "                torch.concat(\n",
    "                    [\n",
    "                        res[\"content_feature\"],\n",
    "                        res[\"vocoder_feature\"] + feature_aug,\n",
    "                    ],\n",
    "                    dim=-1,\n",
    "                )\n",
    "            )\n",
    "        ).squeeze()\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48146a05-6034-4d67-b788-45acb6700ffb",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-09T14:59:47.738745Z",
     "iopub.status.busy": "2023-08-09T14:59:47.738145Z",
     "iopub.status.idle": "2023-08-09T14:59:50.482245Z",
     "shell.execute_reply": "2023-08-09T14:59:50.481094Z",
     "shell.execute_reply.started": "2023-08-09T14:59:47.738693Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "model = AudioModel(vocoder_classes=7)\n",
    "x = torch.randn(32, 1, 48000)\n",
    "_ = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78813c98-620f-4189-adef-6ab216a760bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-09T14:00:42.856950Z",
     "iopub.status.busy": "2023-08-09T14:00:42.856267Z",
     "iopub.status.idle": "2023-08-09T14:00:42.962920Z",
     "shell.execute_reply": "2023-08-09T14:00:42.961630Z",
     "shell.execute_reply.started": "2023-08-09T14:00:42.856875Z"
    },
    "scrolled": true,
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ckpt = torch.load(\n",
    "#     \"/home/ay/data/DATA/1-model_save/0-Audio/Ours/LibriSeVoc_cross_dataset/version_7/checkpoints/best-epoch=3-val-auc=0.99.ckpt\"\n",
    "# )\n",
    "\n",
    "# state_dict = ckpt[\"state_dict\"]\n",
    "\n",
    "# state_dict2 = {key.replace(\"model.\", \"\", 1): state_dict[key] for key in state_dict}\n",
    "\n",
    "# model.load_state_dict(state_dict2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
