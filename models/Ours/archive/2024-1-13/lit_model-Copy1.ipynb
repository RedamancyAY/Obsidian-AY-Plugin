{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c616fb-6a15-4d51-94f4-b8d9d797d0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-05T08:21:43.755427Z",
     "iopub.status.busy": "2024-01-05T08:21:43.755045Z",
     "iopub.status.idle": "2024-01-05T08:21:43.782281Z",
     "shell.execute_reply": "2024-01-05T08:21:43.781329Z",
     "shell.execute_reply.started": "2024-01-05T08:21:43.755398Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a306fb43-dcfc-4954-87c6-c9a59870ff94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:52:54.816102Z",
     "iopub.status.busy": "2024-01-10T11:52:54.815535Z",
     "iopub.status.idle": "2024-01-10T11:52:57.761817Z",
     "shell.execute_reply": "2024-01-10T11:52:57.760833Z",
     "shell.execute_reply.started": "2024-01-10T11:52:54.816051Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ay2.torch.deepfake_detection import DeepfakeAudioClassification\n",
    "from ay2.torch.losses import (\n",
    "    BinaryTokenContrastLoss,\n",
    "    LabelSmoothingBCE,\n",
    "    MultiClass_ContrastLoss,\n",
    ")\n",
    "from ay2.torch.optim import Adam_GC\n",
    "from ay2.torch.optim.selective_weight_decay import (\n",
    "    Optimizers_with_selective_weight_decay,\n",
    ")\n",
    "from ay2.torchaudio.transforms import AddGaussianSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85192f-1c7d-4cd2-9787-9e7deaa367b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20497159-4ffd-4d5f-b806-cd2bdc2853a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from .model import AudioModel\n",
    "except ImportError:\n",
    "    from model import AudioModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f13f66-bdf7-4dc9-b460-b3fede209228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:50:48.513492Z",
     "iopub.status.busy": "2024-01-10T11:50:48.511782Z",
     "iopub.status.idle": "2024-01-10T11:50:48.524978Z",
     "shell.execute_reply": "2024-01-10T11:50:48.523722Z",
     "shell.execute_reply.started": "2024-01-10T11:50:48.513409Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_grad(var):\n",
    "    def hook(grad):\n",
    "        var.grad = grad\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "def reduce_similarity(feature1, feature2):\n",
    "    # 计算余弦相似度\n",
    "    cosine_similarity = F.cosine_similarity(feature1, feature2)\n",
    "\n",
    "    # 对余弦相似度进行处理以减小相似性\n",
    "    reduced_similarity = torch.mean(1 - cosine_similarity)\n",
    "\n",
    "    return reduced_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26aabe9-138a-4421-92f5-225b3edba33e",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-01-13T04:43:31.392294Z",
     "iopub.status.busy": "2024-01-13T04:43:31.391482Z",
     "iopub.status.idle": "2024-01-13T04:43:31.715078Z",
     "shell.execute_reply": "2024-01-13T04:43:31.713108Z",
     "shell.execute_reply.started": "2024-01-13T04:43:31.392218Z"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeepfakeAudioClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAudioModel_lit\u001b[39;00m(\u001b[43mDeepfakeAudioClassification\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DeepfakeAudioClassification' is not defined"
     ]
    }
   ],
   "source": [
    "class AudioModel_lit(DeepfakeAudioClassification):\n",
    "    def __init__(self, cfg=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = AudioModel(\n",
    "            feature_extractor=cfg.feature_extractor,\n",
    "            dims=cfg.dims,\n",
    "            n_blocks=cfg.n_blocks,\n",
    "            vocoder_classes=cfg.method_classes,\n",
    "            cfg=cfg,\n",
    "        )\n",
    "\n",
    "        self.beta1, self.beta2, self.beta3 = cfg.beta\n",
    "\n",
    "        self.one_stem = cfg.one_stem\n",
    "\n",
    "        self.bce_loss = LabelSmoothingBCE(label_smoothing=0.1)\n",
    "        self.contrast_loss2 = BinaryTokenContrastLoss(alpha=0.1)\n",
    "        self.contrast_lossN = MultiClass_ContrastLoss(alpha=2.5, distance=\"l2\")\n",
    "        # self.contrast_lossN = MultiClass_ContrastLoss(alpha=0.1)\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "        # self.transform = AddGaussianSNR(snr_max_db=20)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def calcuate_loss_one_stem(self, batch_res, batch, stage=\"train\"):\n",
    "        B = batch_res[\"logit\"].shape[0]\n",
    "\n",
    "        batch_res[\"logit\"] = batch_res[\"content_logit\"]\n",
    "        label = batch[\"label\"]\n",
    "        label_32 = label.type(torch.float32)\n",
    "        losses = {}\n",
    "\n",
    "        losses[\"cls_loss\"] = self.bce_loss(batch_res[\"logit\"].squeeze(), label_32)\n",
    "        losses[\"content_contrast_loss\"] = self.contrast_loss2(\n",
    "            batch_res[\"content_feature\"], label_32\n",
    "        )\n",
    "\n",
    "        # loss = sum(losses.values()) / (len(losses))\n",
    "        losses[\"loss\"] = (losses[\"cls_loss\"] + losses[\"content_contrast_loss\"]) / 2\n",
    "        return losses\n",
    "\n",
    "    def set_grad(self, prop):\n",
    "        def hook(grad):\n",
    "            prop = grad\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def get_vocoder_stem_loss(self, losses, batch_res, batch, stage=\"train\"):\n",
    "        losses[\"voc_cls_loss\"] = self.cross_entropy_loss(\n",
    "            batch_res[\"vocoder_logit\"], batch[\"vocoder_label\"]\n",
    "        )\n",
    "\n",
    "        # if stage == 'val':\n",
    "        # print(torch.argmax(batch_res[\"vocoder_logit\"], dim=1), batch[\"vocoder_label\"])\n",
    "        # if stage == 'train':\n",
    "        #     losses[\"voc_cls_loss\"].register_hook(self.set_grad())\n",
    "\n",
    "        # + self.cross_entropy_loss(batch_res[\"vocoder_logit\"], batch[\"emotion_label\"])\n",
    "        losses[\"voc_contrast_loss\"] = self.contrast_lossN(\n",
    "            batch_res[\"vocoder_feature\"], batch[\"vocoder_label\"].type(torch.float32)\n",
    "        )\n",
    "        vocoder_stem_loss = losses[\"voc_cls_loss\"] + losses[\"voc_contrast_loss\"]\n",
    "\n",
    "        # if stage == 'train':\n",
    "        #     self.optimizers().zero_grad()\n",
    "        #     self.vocoder_grad = None\n",
    "        #     batch_res[\"hidden_states\"].register_hook(self.set_grad(self.vocoder_grad))\n",
    "        #     losses[\"voc_cls_loss\"].backward(retain_graph=True)\n",
    "\n",
    "        return vocoder_stem_loss\n",
    "\n",
    "    def get_content_stem_loss(self, losses, batch_res, batch, stage=\"train\"):\n",
    "        label_32 = batch[\"label\"].type(torch.float32)\n",
    "        batch_size = len(label_32)\n",
    "        losses[\"content_cls_loss\"] = self.bce_loss(\n",
    "            batch_res[\"content_logit\"].squeeze(), label_32\n",
    "        )\n",
    "        losses[\"content_contrast_loss\"] = self.contrast_loss2(\n",
    "            batch_res[\"content_feature\"], label_32\n",
    "        )\n",
    "\n",
    "        vocoder_label = torch.ones_like(batch_res[\"content_voc_logit\"]) * (\n",
    "            1 / batch_res[\"content_voc_logit\"].shape[-1]\n",
    "        )\n",
    "        # _label = torch.zeros_like(batch_res[\"content_voc_logit\"])\n",
    "        # _label[:, 0] = 1.0\n",
    "        # for i in range(batch_size):\n",
    "        #     if batch['vocoder_label'][i] == 0:\n",
    "        #         vocoder_label[i] = _label[i]\n",
    "        losses[\"content_adv_loss\"] = self.cross_entropy_loss(\n",
    "            batch_res[\"content_voc_logit\"],\n",
    "            vocoder_label,\n",
    "        )\n",
    "        content_stem_loss = (\n",
    "            losses[\"content_cls_loss\"]\n",
    "            + losses[\"content_contrast_loss\"]\n",
    "            + losses[\"content_adv_loss\"]\n",
    "        )\n",
    "        return content_stem_loss\n",
    "\n",
    "    def calcuate_loss(self, batch_res, batch, stage=\"train\"):\n",
    "        B = batch_res[\"logit\"].shape[0]\n",
    "        label = batch[\"label\"]\n",
    "        label_32 = label.type(torch.float32)\n",
    "        losses = {}\n",
    "\n",
    "        losses[\"cls_loss\"] = self.bce_loss(batch_res[\"logit\"].squeeze(), label_32)\n",
    "        # losses[\"cls_loss_aug\"] = self.bce_loss(\n",
    "        #     batch_res[\"aug_logit\"].squeeze(), label_32\n",
    "        # )\n",
    "\n",
    "        vocoder_stem_loss = self.get_vocoder_stem_loss(losses, batch_res, batch, stage)\n",
    "        content_stem_loss = self.get_content_stem_loss(losses, batch_res, batch, stage)\n",
    "\n",
    "        losses[\"feature_similar_loss\"] = reduce_similarity(\n",
    "            batch_res[\"content_feature\"], batch_res[\"vocoder_feature\"]\n",
    "        )\n",
    "        # losses[\"module_similar_loss\"] = self.model.module_similaryity()\n",
    "        # similar_loss = losses[\"feature_similar_loss\"] + losses[\"module_similar_loss\"]\n",
    "        similar_loss = losses[\"feature_similar_loss\"]\n",
    "\n",
    "        # losses[\"loss\"] = (\n",
    "        #     losses[\"cls_loss\"]\n",
    "        #     # + self.loss_adjustment(losses[\"cls_loss_aug\"], losses[\"cls_loss\"], sigma=1.0)\n",
    "        #     + self.loss_adjustment(vocoder_stem_loss, losses[\"cls_loss\"], sigma=1.0)\n",
    "        #     + self.loss_adjustment(content_stem_loss, losses[\"cls_loss\"], sigma=0.5)\n",
    "        #     + self.loss_adjustment(similar_loss, losses[\"cls_loss\"], sigma=0.5)\n",
    "        # )\n",
    "\n",
    "        losses[\"loss\"] = content_stem_loss + vocoder_stem_loss + similar_loss\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def loss_adjustment(self, auxiliary_loss, main_loss, sigma=0.5):\n",
    "        while auxiliary_loss > main_loss * sigma:\n",
    "            auxiliary_loss = auxiliary_loss * 0.5\n",
    "        return auxiliary_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optim = Adam_GC  # or torch.optim.Adam\n",
    "        # optim = torch.optim.Adam\n",
    "        # optimizer = optim(self.model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "\n",
    "        optimizer = Optimizers_with_selective_weight_decay(\n",
    "            self.model, optimizer=\"Adam\", lr=0.0001, weight_decay=0.01\n",
    "        )\n",
    "        # optimizer = Optimizers_with_selective_weight_decay(\n",
    "        #     self.model, optimizer=\"SGD\", lr=0.01, weight_decay=0.01\n",
    "        # )\n",
    "        return [optimizer]\n",
    "\n",
    "    def _shared_pred(self, batch, batch_idx, stage=\"train\"):\n",
    "        audio, sample_rate = batch[\"audio\"], batch[\"sample_rate\"]\n",
    "\n",
    "        # if stage == 'train':\n",
    "        # audio = self.transform(audio)\n",
    "\n",
    "        batch_res = self.model(\n",
    "            audio, stage=stage, batch=batch if stage == \"train\" else None\n",
    "        )\n",
    "\n",
    "        # batch_res = self.model(audio, stage=stage, batch=batch)\n",
    "        batch_res[\"pred\"] = (torch.sigmoid(batch_res[\"logit\"]) + 0.5).int()\n",
    "\n",
    "        return batch_res\n",
    "\n",
    "    def _shared_eval_step(\n",
    "        self, batch, batch_idx, stage=\"train\", dataloader_idx=0, *args, **kwargs\n",
    "    ):\n",
    "        batch_res = self._shared_pred(batch, batch_idx, stage=stage)\n",
    "\n",
    "        label = batch[\"label\"]\n",
    "\n",
    "        if not self.one_stem:\n",
    "            loss = self.calcuate_loss(batch_res, batch, stage=stage)\n",
    "        else:\n",
    "            loss = self.calcuate_loss_one_stem(batch_res, batch, stage=stage)\n",
    "\n",
    "        suffix = \"\" if dataloader_idx == 0 else f\"-dl{dataloader_idx}\"\n",
    "        self.log_dict(\n",
    "            {f\"{stage}-{key}{suffix}\": loss[key] for key in loss},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "            prog_bar=True,\n",
    "            add_dataloader_idx=False,\n",
    "            batch_size=batch[\"label\"].shape[0],\n",
    "        )\n",
    "        batch_res.update(loss)\n",
    "        return batch_res"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
