{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c616fb-6a15-4d51-94f4-b8d9d797d0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-05T07:34:12.053965Z",
     "iopub.status.busy": "2024-02-05T07:34:12.053143Z",
     "iopub.status.idle": "2024-02-05T07:34:12.108165Z",
     "shell.execute_reply": "2024-02-05T07:34:12.106158Z",
     "shell.execute_reply.started": "2024-02-05T07:34:12.053883Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a306fb43-dcfc-4954-87c6-c9a59870ff94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T13:20:47.468064Z",
     "iopub.status.busy": "2024-02-21T13:20:47.467404Z",
     "iopub.status.idle": "2024-02-21T13:20:47.764213Z",
     "shell.execute_reply": "2024-02-21T13:20:47.762875Z",
     "shell.execute_reply.started": "2024-02-21T13:20:47.468003Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import statistics\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ay2.torch.deepfake_detection import DeepfakeAudioClassification\n",
    "from ay2.torch.losses import (\n",
    "    BinaryTokenContrastLoss,\n",
    "    Focal_loss,\n",
    "    LabelSmoothingBCE,\n",
    "    MultiClass_ContrastLoss,\n",
    ")\n",
    "from ay2.torch.optim import Adam_GC\n",
    "from ay2.torch.optim.selective_weight_decay import (\n",
    "    Optimizers_with_selective_weight_decay,\n",
    "    Optimizers_with_selective_weight_decay_for_modulelist,\n",
    ")\n",
    "from ay2.torchaudio.transforms import AddGaussianSNR\n",
    "from tqdm.auto import tqdm\n",
    "from ay2.torchaudio.transforms.self_operation import (\n",
    "    AudioToTensor,\n",
    "    CentralAudioClip,\n",
    "    RandomAudioClip,\n",
    "    RandomPitchShift,\n",
    "    RandomSpeed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed85192f-1c7d-4cd2-9787-9e7deaa367b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T12:52:10.898459Z",
     "iopub.status.busy": "2024-02-16T12:52:10.897718Z",
     "iopub.status.idle": "2024-02-16T12:52:10.969593Z",
     "shell.execute_reply": "2024-02-16T12:52:10.968724Z",
     "shell.execute_reply.started": "2024-02-16T12:52:10.898396Z"
    }
   },
   "outputs": [],
   "source": [
    "from ay2.tools import (\n",
    "    freeze_modules,\n",
    "    rich_bar,\n",
    "    unfreeze_modules,\n",
    "    find_unsame_name_for_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5ecf97a-6cda-44e2-9d98-bef91307b122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-14T03:22:40.792906Z",
     "iopub.status.busy": "2024-02-14T03:22:40.792476Z",
     "iopub.status.idle": "2024-02-14T03:22:40.831991Z",
     "shell.execute_reply": "2024-02-14T03:22:40.831037Z",
     "shell.execute_reply.started": "2024-02-14T03:22:40.792863Z"
    }
   },
   "outputs": [],
   "source": [
    "from ay2.torchaudio.transforms import SpecAugmentBatchTransform\n",
    "from ay2.torchaudio.transforms.self_operation import RandomSpeed\n",
    "\n",
    "random_speed = RandomSpeed(min_speed=0.5, max_speed=2.0, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20497159-4ffd-4d5f-b806-cd2bdc2853a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from .model import AudioModel\n",
    "except ImportError:\n",
    "    from model import AudioModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f13f66-bdf7-4dc9-b460-b3fede209228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T11:50:48.513492Z",
     "iopub.status.busy": "2024-01-10T11:50:48.511782Z",
     "iopub.status.idle": "2024-01-10T11:50:48.524978Z",
     "shell.execute_reply": "2024-01-10T11:50:48.523722Z",
     "shell.execute_reply.started": "2024-01-10T11:50:48.513409Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_grad(var):\n",
    "    def hook(grad):\n",
    "        var.grad = grad\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "def reduce_similarity(feature1, feature2):\n",
    "    # 计算余弦相似度\n",
    "    cosine_similarity = F.cosine_similarity(feature1, feature2)\n",
    "\n",
    "    # 对余弦相似度进行处理以减小相似性\n",
    "    reduced_similarity = torch.mean(1 - cosine_similarity)\n",
    "\n",
    "    return reduced_similarity\n",
    "\n",
    "\n",
    "def mixup(data, targets, alpha=0.8):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    new_data = data * lam + shuffled_data * (1 - lam)\n",
    "    new_targets = [targets, shuffled_targets, lam]\n",
    "    return new_data, new_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26aabe9-138a-4421-92f5-225b3edba33e",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-02-12T14:58:37.487308Z",
     "iopub.status.busy": "2024-02-12T14:58:37.486610Z",
     "iopub.status.idle": "2024-02-12T14:58:37.666377Z",
     "shell.execute_reply": "2024-02-12T14:58:37.664493Z",
     "shell.execute_reply.started": "2024-02-12T14:58:37.487246Z"
    },
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeepfakeAudioClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAudioModel_lit\u001b[39;00m(\u001b[43mDeepfakeAudioClassification\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DeepfakeAudioClassification' is not defined"
     ]
    }
   ],
   "source": [
    "class AudioModel_lit(DeepfakeAudioClassification):\n",
    "    def __init__(self, cfg=None, args=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = AudioModel(\n",
    "            feature_extractor=cfg.feature_extractor,\n",
    "            dims=cfg.dims,\n",
    "            n_blocks=cfg.n_blocks,\n",
    "            vocoder_classes=cfg.method_classes,\n",
    "            cfg=cfg,\n",
    "            args=args,\n",
    "        )\n",
    "        self.cfg = cfg\n",
    "        self.beta1, self.beta2, self.beta3 = cfg.beta\n",
    "\n",
    "        self.one_stem = cfg.one_stem\n",
    "\n",
    "        # self.transform = AddGaussianSNR(snr_max_db=20)\n",
    "        self.audio_transform = SpecAugmentBatchTransform.from_policy(\"ss\")\n",
    "        self.ttt_transform = [\n",
    "            RandomSpeed(min_speed=0.5, max_speed=2.0, p=1),\n",
    "            CentralAudioClip(length=48000),\n",
    "        ]\n",
    "        self.ttt_transform = AddGaussianSNR(snr_max_db=5)\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.mixup = False\n",
    "        # freeze_modules(self.model.feature_model.get_main_stem())\n",
    "\n",
    "        self.configure_loss_fn()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_loss_fn(\n",
    "        self,\n",
    "    ):\n",
    "        self.bce_loss = LabelSmoothingBCE(label_smoothing=0.1)\n",
    "        self.contrast_loss2 = BinaryTokenContrastLoss(alpha=0.1)\n",
    "        self.contrast_lossN = MultiClass_ContrastLoss(alpha=2.5, distance=\"l2\")\n",
    "        # self.contrast_lossN = MultiClass_ContrastLoss(alpha=0.1)\n",
    "        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "        from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "        self.focal_loss = partial(sigmoid_focal_loss, reduction=\"mean\")\n",
    "\n",
    "    def calcuate_loss_one_stem(self, batch_res, batch, stage=\"train\"):\n",
    "        B = batch_res[\"logit\"].shape[0]\n",
    "\n",
    "        batch_res[\"logit\"] = batch_res[\"content_logit\"]\n",
    "        label = batch[\"label\"]\n",
    "        label_32 = label.type(torch.float32)\n",
    "        losses = {}\n",
    "\n",
    "        losses[\"cls_loss\"] = self.bce_loss(batch_res[\"logit\"].squeeze(), label_32)\n",
    "        losses[\"content_contrast_loss\"] = self.contrast_loss2(\n",
    "            batch_res[\"content_feature\"], label_32\n",
    "        )\n",
    "\n",
    "        # loss = sum(losses.values()) / (len(losses))\n",
    "        losses[\"loss\"] = (losses[\"cls_loss\"] + losses[\"content_contrast_loss\"]) / 2\n",
    "        return losses\n",
    "\n",
    "    # Copied and edited from https://www.kaggle.com/code/riadalmadani/fastai-effb0-base-model-birdclef2023\n",
    "    def binary_classification_loss(self, logit, label, mixup=False):\n",
    "        logit = logit.squeeze()\n",
    "        if not mixup:\n",
    "            return self.bce_loss(logit, label.type(torch.float32))\n",
    "        else:\n",
    "            targets1, targets2, lam = label[0:3]\n",
    "            return lam * self.bce_loss(logit, targets1.type(torch.float32)) + (\n",
    "                1 - lam\n",
    "            ) * self.bce_loss(logit, targets2.type(torch.float32))\n",
    "\n",
    "    def get_vocoder_stem_loss(self, losses, batch_res, batch, stage=\"train\"):\n",
    "        if not \"vocoder_label\" in batch.keys():\n",
    "            return 0.0\n",
    "\n",
    "        losses[\"voc_cls_loss\"] = self.ce_loss(\n",
    "            batch_res[\"vocoder_logit\"], batch[\"vocoder_label\"]\n",
    "        )\n",
    "\n",
    "        losses[\"voc_contrast_loss\"] = self.contrast_lossN(\n",
    "            batch_res[\"vocoder_feature\"], batch[\"vocoder_label\"].type(torch.float32)\n",
    "        )\n",
    "        vocoder_stem_loss = losses[\"voc_cls_loss\"] + losses[\"voc_contrast_loss\"]\n",
    "\n",
    "        return vocoder_stem_loss\n",
    "\n",
    "    def get_content_stem_loss(self, losses, batch_res, batch, stage=\"train\"):\n",
    "        label_32 = batch[\"label\"].type(torch.float32)\n",
    "        batch_size = len(label_32)\n",
    "\n",
    "        losses[\"content_cls_loss\"] = self.binary_classification_loss(\n",
    "            batch_res[\"content_logit\"], label_32\n",
    "        )\n",
    "        losses[\"content_contrast_loss\"] = self.contrast_loss2(\n",
    "            batch_res[\"content_feature\"], label_32\n",
    "        )\n",
    "        content_stem_loss = losses[\"content_cls_loss\"] + losses[\"content_contrast_loss\"]\n",
    "        return content_stem_loss\n",
    "\n",
    "    def get_content_adv_loss(self, batch_res, batch, stage=\"train\"):\n",
    "        vocoder_label = torch.ones_like(batch_res[\"content_voc_logit\"]) * (\n",
    "            1 / batch_res[\"content_voc_logit\"].shape[-1]\n",
    "        )\n",
    "        loss = self.ce_loss(\n",
    "            batch_res[\"content_voc_logit\"],\n",
    "            vocoder_label,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def calcuate_loss(self, batch_res, batch, stage=\"train\"):\n",
    "        B = batch_res[\"logit\"].shape[0]\n",
    "        label = batch[\"label\"]\n",
    "        losses = {}\n",
    "\n",
    "        losses[\"cls_loss\"] = self.binary_classification_loss(batch_res[\"logit\"], label)\n",
    "        if stage == \"train\":\n",
    "            losses[\"cls_loss\"] += self.focal_loss(\n",
    "                batch_res[\"shuffle_logit\"], batch[\"shuffle_label\"].type(torch.float32)\n",
    "            )\n",
    "\n",
    "        losses[\"vocoder_stem_loss\"] = self.get_vocoder_stem_loss(\n",
    "            losses, batch_res, batch, stage\n",
    "        )\n",
    "        content_stem_loss = self.get_content_stem_loss(losses, batch_res, batch, stage)\n",
    "\n",
    "        losses[\"speed_loss\"] = self.ce_loss(\n",
    "            batch_res[\"speed_logit\"],\n",
    "            batch[\"speed\"].long(),\n",
    "        )\n",
    "\n",
    "        losses[\"loss\"] = (\n",
    "            # losses[\"cls_loss\"]\n",
    "            # + 0.5 * content_stem_loss\n",
    "            content_stem_loss\n",
    "            + 0.5 * losses[\"cls_loss\"]\n",
    "            + +0.5 * losses[\"vocoder_stem_loss\"]\n",
    "            + 1.0 * losses[\"speed_loss\"]\n",
    "        )\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def loss_adjustment(self, auxiliary_loss, main_loss, sigma=0.5):\n",
    "        while auxiliary_loss > main_loss * sigma:\n",
    "            auxiliary_loss = auxiliary_loss * 0.9\n",
    "        return auxiliary_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = Adam_GC  # or torch.optim.Adam\n",
    "        # optim = torch.optim.Adam\n",
    "        # optimizer = optim(self.model.parameters(), lr=0.00001, weight_decay=0.0001)\n",
    "\n",
    "        optimizer = Optimizers_with_selective_weight_decay(\n",
    "            self.model, optimizer=\"Adam\", lr=0.0001, weight_decay=0.01\n",
    "        )\n",
    "        # optimizer = Optimizers_with_selective_weight_decay(\n",
    "        #     self.model, optimizer=\"SGD\", lr=0.001, weight_decay=0.01\n",
    "        # )\n",
    "        return [optimizer]\n",
    "\n",
    "    def _shared_pred(self, batch, batch_idx, stage=\"train\"):\n",
    "        \"\"\"common predict step for train/val/test\n",
    "\n",
    "        Note that the data augmenation is done in the self.model.feature_extractor.\n",
    "\n",
    "        \"\"\"\n",
    "        audio, sample_rate = batch[\"audio\"], batch[\"sample_rate\"]\n",
    "        audio = self.model.feature_model.preprocess(audio, stage=stage)\n",
    "        # if stage == \"train\":\n",
    "        # audio = self.audio_transform(audio)\n",
    "        batch_res = self.model(\n",
    "            audio, stage=stage, batch=batch if stage == \"train\" else None\n",
    "        )\n",
    "\n",
    "        # batch_res[\"pred\"] = (torch.sigmoid(batch_res[\"logit\"]) + 0.5).int()\n",
    "        batch_res[\"pred\"] = batch_res[\"logit\"]\n",
    "\n",
    "        return batch_res\n",
    "\n",
    "    def _shared_eval_step(\n",
    "        self,\n",
    "        batch,\n",
    "        batch_idx,\n",
    "        stage=\"train\",\n",
    "        loss=True,\n",
    "        dataloader_idx=0,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"common evaluation step for train/val/test\n",
    "\n",
    "        In contrast to the predict step, this evaluation step calculates the losses and logs\n",
    "        them to logger.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_res = self._shared_pred(batch, batch_idx, stage=stage)\n",
    "        label = batch[\"label\"]\n",
    "\n",
    "        if not loss:\n",
    "            return batch_res\n",
    "\n",
    "        if not self.one_stem:\n",
    "            loss = self.calcuate_loss(batch_res, batch, stage=stage)\n",
    "        else:\n",
    "            loss = self.calcuate_loss_one_stem(batch_res, batch, stage=stage)\n",
    "\n",
    "        suffix = \"\" if dataloader_idx == 0 else f\"-dl{dataloader_idx}\"\n",
    "        self.log_dict(\n",
    "            {f\"{stage}-{key}{suffix}\": loss[key] for key in loss},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "            prog_bar=True,\n",
    "            add_dataloader_idx=False,\n",
    "            batch_size=batch[\"label\"].shape[0],\n",
    "        )\n",
    "        batch_res.update(loss)\n",
    "        return batch_res\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"custom training step for twp-step parameter updating.\"\"\"\n",
    "\n",
    "        opt1 = self.optimizers()\n",
    "\n",
    "        batch_res = self._shared_eval_step(batch, batch_idx, stage=\"train\")\n",
    "\n",
    "        opt1.zero_grad()\n",
    "        self.manual_backward(batch_res[\"loss\"], retain_graph=True)\n",
    "        # opt1.step()\n",
    "\n",
    "        freeze_modules([self.model.cls_voc, self.model.feature_model.get_main_stem()])\n",
    "        batch_res[\"content_adv_loss\"] = 0.5 * self.get_content_adv_loss(\n",
    "            batch_res, batch\n",
    "        )\n",
    "        # batch_res[\"content_adv_loss\"] = self.loss_adjustment(\n",
    "        #     batch_res[\"content_adv_loss\"], batch_res[\"cls_loss\"]\n",
    "        # )\n",
    "        self.manual_backward(batch_res[\"content_adv_loss\"])\n",
    "        unfreeze_modules([self.model.cls_voc, self.model.feature_model.get_main_stem()])\n",
    "\n",
    "        opt1.step()\n",
    "\n",
    "        return batch_res\n",
    "\n",
    "    def get_ttt_optimizer(self, *args, **kwargs):\n",
    "        \"\"\"get optimizer for test-time training\"\"\"\n",
    "        if hasattr(self, \"ttt_optimizer\"):\n",
    "            return self.ttt_optimizer\n",
    "        else:\n",
    "            from ay2.tools import color_print\n",
    "\n",
    "            print(\"build optimizer for TTT\")\n",
    "            parameters = nn.ModuleList(\n",
    "                self.model.feature_model.get_main_stem()\n",
    "                + self.model.feature_model.get_content_stem()\n",
    "                + self.model.feature_model.get_vocoder_stem()\n",
    "            ).parameters()\n",
    "            parameters = self.model.parameters()\n",
    "            self.ttt_optimizer = torch.optim.SGD(parameters, lr=0.001, momentum=0.9)\n",
    "            # self.ttt_optimizer = torch.optim.SGD(parameters, lr=0.001)\n",
    "            # self.ttt_optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
    "            return self.ttt_optimizer\n",
    "\n",
    "    def backup_state_dict(self):\n",
    "        if not hasattr(self, \"org_state_dict\"):\n",
    "            self.org_state_dict = deepcopy(self.model.state_dict())\n",
    "\n",
    "    def restore_state_dict(self):\n",
    "        self.model.load_state_dict(self.org_state_dict)\n",
    "\n",
    "    def hash_module(self, m):\n",
    "        _hashes = []\n",
    "        for k, v in dict(m.state_dict()).items():\n",
    "            _h = hash(tuple(v.reshape(-1).tolist()))\n",
    "            _hashes.append(_h)\n",
    "        return sum(_hashes)\n",
    "\n",
    "    # def on_test_start(self):\n",
    "    #     if hasattr(self, \"ttt_optimizer\"):\n",
    "    #         del self.ttt_optimizer\n",
    "\n",
    "    #     if hasattr(self, \"cov_on_train\"):\n",
    "    #         return\n",
    "\n",
    "    #     log_dir = self.trainer.log_dir\n",
    "    #     ckpt_path = os.path.join(log_dir, \"hello.ckpt\")\n",
    "    #     if os.path.exists(ckpt_path):\n",
    "    #         (\n",
    "    #             self.cov_on_train,\n",
    "    #             self.mu_on_train,\n",
    "    #             self.coral_on_train,\n",
    "    #             self.std_on_train,\n",
    "    #             self.vocoder_feats,\n",
    "    #         ) = torch.load(ckpt_path, map_location=self.device)\n",
    "    #         return\n",
    "\n",
    "    #     feat_all = {\"feat\": [], \"org_feat\": [], \"voc_feat\": [], \"content_feat\": []}\n",
    "    #     self.cov_on_train, self.mu_on_train, self.coral_on_train = {}, {}, {}\n",
    "    #     self.std_on_train = {}\n",
    "    #     self.vocoder_feats = {\"real\": [], \"fake\": []}\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "    #         for i, batch in tqdm(enumerate(self.trainer.trainset_wo_transform)):\n",
    "    #             x = batch[\"audio\"].cuda()\n",
    "    #             x = self.model.feature_model.preprocess(x)\n",
    "    #             feat = self.model.feature_model.get_hidden_state(x)\n",
    "    #             content_feat = self.model.feature_model.get_final_feature(feat)\n",
    "    #             vocoder_feat = self.model.feature_model.get_final_feature_copyed(feat)\n",
    "\n",
    "    #             # fmt: off\n",
    "    #             self.vocoder_feats[\"fake\"].append(vocoder_feat[batch[\"label\"] == 0].cpu())\n",
    "    #             self.vocoder_feats[\"real\"].append(vocoder_feat[batch[\"label\"] == 1].cpu())\n",
    "    #             feat_all[\"feat\"].append(F.adaptive_avg_pool2d(feat, 1).squeeze([-1, -2]))\n",
    "    #             # feat_all[\"feat\"].append(F.adaptive_avg_pool1d(feat, 1).squeeze([-1]))\n",
    "    #             # feat_all[\"org_feat\"].append(feat)\n",
    "    #             feat_all[\"voc_feat\"].append(vocoder_feat)\n",
    "    #             # feat_all[\"content_feat\"].append(content_feat)\n",
    "    #             # fmt: on\n",
    "\n",
    "    #     # fmt: off\n",
    "    #     # print(torch.concat(self.vocoder_feats['real']).shape, torch.concat(self.vocoder_feats['fake']).shape)\n",
    "    #     n_samples = torch.concat(self.vocoder_feats['real']).shape[0]\n",
    "    #     ids = torch.randperm(n_samples)\n",
    "    #     self.vocoder_feats['real'] = torch.concat(self.vocoder_feats['real'])[ids]\n",
    "    #     self.vocoder_feats['fake'] = torch.concat(self.vocoder_feats['fake'])[ids]\n",
    "    #     self.vocoder_feats['real'] = self.vocoder_feats['real'].mean(dim=0, keepdims=True).cuda()\n",
    "    #     self.vocoder_feats['fake'] = self.vocoder_feats['fake'].mean(dim=0, keepdims=True).cuda()\n",
    "    #     # fmt: on\n",
    "\n",
    "    #     for key in [\n",
    "    #         \"feat\",\n",
    "    #         # \"org_feat\",\n",
    "    #         \"voc_feat\",\n",
    "    #         # \"content_feat\",\n",
    "    #     ]:\n",
    "    #         losses = []\n",
    "    #         _feat_all = feat_all[key]\n",
    "    #         _cov, _mu = self.get_cov(_feat_all[0]), _feat_all[0].mean(dim=0)\n",
    "    #         for i in range(1, len(_feat_all)):\n",
    "    #             loss = self.coral(self.get_cov(_feat_all[i]), _cov)\n",
    "    #             losses.append(loss.item())\n",
    "\n",
    "    #         _feat_all = torch.cat(_feat_all)\n",
    "    #         self.cov_on_train[key] = self.get_cov(_feat_all)\n",
    "    #         self.mu_on_train[key] = _feat_all.mean(dim=0)\n",
    "    #         self.std_on_train[key] = _feat_all.std(dim=0)\n",
    "    #         self.coral_on_train[key] = statistics.mean(losses)\n",
    "\n",
    "    #     torch.save(\n",
    "    #         [\n",
    "    #             self.cov_on_train,\n",
    "    #             self.mu_on_train,\n",
    "    #             self.coral_on_train,\n",
    "    #             self.std_on_train,\n",
    "    #             self.vocoder_feats,\n",
    "    #         ],\n",
    "    #         ckpt_path,\n",
    "    #     )\n",
    "\n",
    "    def get_cov(self, feat):\n",
    "        if feat.ndim == 2:\n",
    "            return torch.cov(feat.T)\n",
    "        elif feat.ndim == 3:\n",
    "            feat = F.adaptive_avg_pool1d(feat, 1).squeeze([-1])\n",
    "            return torch.cov(feat.T)\n",
    "        else:\n",
    "            feat = F.adaptive_avg_pool2d(feat, 1).squeeze([-1, -2])\n",
    "            return torch.cov(feat.T)\n",
    "\n",
    "    def Entropy_loss(self, logit):\n",
    "        def entropy(predict_prob):\n",
    "            l = -predict_prob * torch.log(predict_prob)\n",
    "            return l.sum(-1)\n",
    "\n",
    "        if logit.ndim == 1:\n",
    "            prob = torch.sigmoid(logit)\n",
    "            prob = torch.stack([prob, 1 - prob], dim=0)\n",
    "        else:\n",
    "            prob = torch.softmax(logit, dim=-1)\n",
    "        return torch.mean(entropy(prob))\n",
    "\n",
    "    def skewness(self, tensor):\n",
    "        mean = torch.mean(tensor)\n",
    "        diff = tensor - mean\n",
    "        numerator = torch.mean(diff**3)\n",
    "        denominator = torch.mean(diff**2).pow(1.5)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def coral(self, cs, ct):\n",
    "        d = cs.shape[0]\n",
    "        loss = (cs - ct).pow(2).sum() / (4.0 * d**2)\n",
    "        return loss\n",
    "\n",
    "    def Euclidean_distance(self, feat1, feat2):\n",
    "        return torch.mean((feat1 - feat2) ** 2)\n",
    "\n",
    "    def feat_loss(self, key, feat, cls_loss=None, **kwargs):\n",
    "        if key == \"feat\":\n",
    "            if feat.ndim == 4:\n",
    "                feat = F.adaptive_avg_pool2d(feat, 1).squeeze([-1, -2])\n",
    "            elif feat.ndim == 3:\n",
    "                feat = F.adaptive_avg_pool1d(feat, 1).squeeze([-1])\n",
    "\n",
    "        mu_feat = feat.mean(dim=0)\n",
    "\n",
    "        cosine_dis = torch.nn.functional.cosine_similarity(\n",
    "            feat, self.mu_on_train[key][None]\n",
    "        )\n",
    "        dis = self.Euclidean_distance(mu_feat, self.mu_on_train[key]).item()\n",
    "        if dis > 0.03:\n",
    "            mu_weight = 10\n",
    "        else:\n",
    "            mu_weight = 10\n",
    "\n",
    "        cov_feat = self.get_cov(feat)\n",
    "        cov_loss = self.coral(self.cov_on_train[key], cov_feat) * (\n",
    "            0.05 / self.coral_on_train[key]\n",
    "        )\n",
    "        mu_loss = (mu_feat - self.mu_on_train[key]).pow(2).mean() * 10\n",
    "        std_loss = (feat.std(dim=0) - self.std_on_train[key]).pow(2).mean() * 10\n",
    "        skewness_loss = torch.abs(\n",
    "            self.skewness(mu_feat) - self.skewness(self.mu_on_train[key])\n",
    "        )\n",
    "\n",
    "        debug = 1\n",
    "        if debug:\n",
    "            _dict = {\n",
    "                \"cls_loss\": \"{:.4f}\".format(cls_loss.item()),\n",
    "                \"l2-dis\": \"{:.4f}\".format(\n",
    "                    self.Euclidean_distance(mu_feat, self.mu_on_train[key]).item()\n",
    "                ),\n",
    "                \"cos-dis\": \"{:.4f}\".format(torch.mean(cosine_dis).item()),\n",
    "                \"mu_loss\": \"{:.4f}\".format(mu_loss.item()),\n",
    "                \"cov_loss\": \"{:.4f}\".format(cov_loss.item()),\n",
    "                \"skewness_loss\": \"{:.4f}\".format(skewness_loss.item()),\n",
    "            }\n",
    "            if not hasattr(self, \"xxxx\"):\n",
    "                self.xxxx = 0\n",
    "            if not hasattr(self, \"data\"):\n",
    "                self.data = pd.DataFrame(columns=list(_dict.keys()))\n",
    "            self.data.loc[len(self.data)] = _dict\n",
    "            self.xxxx += 1\n",
    "            if self.xxxx % 1000 == 0:\n",
    "                print(key, \" \".join([f\"{k}:{v}\" for k, v in _dict.items()]))\n",
    "\n",
    "        return mu_loss\n",
    "        # return skewness_loss + mu_loss\n",
    "        # return cov_loss + mu_loss\n",
    "\n",
    "    def on_test_end(\n",
    "        self,\n",
    "    ):\n",
    "        if hasattr(self, \"data\"):\n",
    "            save_path = find_unsame_name_for_file(self.trainer.log_dir + \"/data.csv\")\n",
    "            # self.data.to_csv(save_path, index=False)\n",
    "            del self.data\n",
    "\n",
    "    def ttt_step(self, batch, batch_idx):\n",
    "        batch[\"audio\"] = batch[\"audio\"].clone().requires_grad_()\n",
    "        batch_size = batch[\"audio\"].shape[0]\n",
    "        # audio_aug = self.ttt_transform(batch[\"audio\"].clone()).requires_grad_()\n",
    "\n",
    "        base_label = torch.ones(batch_size, dtype=batch[\"vocoder_label\"].dtype).to(\n",
    "            batch[\"audio\"].device\n",
    "        )\n",
    "        # batch['audio'], speed_labels = random_speed.batch_apply(batch['audio'])\n",
    "\n",
    "        _input = batch[\"audio\"]\n",
    "        # _input = torch.concat([batch[\"audio\"], audio_aug], dim=0)\n",
    "        x = self.model.feature_model.preprocess(_input)\n",
    "        _h = self.model.feature_model.get_hidden_state(x)\n",
    "        _c = self.model.feature_model.get_final_feature(_h)\n",
    "        # _content_logit = self.model.cls_content(_c).squeeze(-1)\n",
    "        _v = self.model.feature_model.get_final_feature_copyed(_h)\n",
    "        _logit = self.model.cls_final(\n",
    "            torch.concat(\n",
    "                [\n",
    "                    _c,\n",
    "                    self.vocoder_feats[\"fake\"].repeat(batch_size, 1),\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "        ).squeeze(-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(_logit, base_label * 0.0)\n",
    "\n",
    "        # loss += self.Euclidean_distance(_c[:batch_size, ...], _c[batch_size:,...]) * 10\n",
    "        # loss += self.loss_adjustment(\n",
    "        #     main_loss=loss, auxiliary_loss=self.Entropy_loss(_logit), sigma=0.5\n",
    "        # )\n",
    "\n",
    "        _h = _h[:batch_size, ...]\n",
    "        ### feature loss\n",
    "        _feat_loss = self.feat_loss(key=\"feat\", feat=_h, cls_loss=loss)\n",
    "        _feat_loss = self.loss_adjustment(\n",
    "            main_loss=loss, auxiliary_loss=_feat_loss, sigma=0.5\n",
    "        )\n",
    "        loss += _feat_loss\n",
    "\n",
    "        # _content_feat_loss = self.feat_loss(key='content_feat', feat=_v)\n",
    "        # _content_feat_loss = self.loss_adjustment(main_loss=loss, auxiliary_loss=_content_feat_loss)\n",
    "        # loss += _content_feat_loss\n",
    "\n",
    "        # _voc_feat_loss = self.feat_loss(key='voc_feat', feat=_v, cls_loss=loss)\n",
    "        # _voc_feat_loss = self.loss_adjustment(main_loss=loss, auxiliary_loss=_voc_feat_loss)\n",
    "        # loss += _voc_feat_loss\n",
    "\n",
    "        # speed loss\n",
    "        # base = torch.ones(\n",
    "        #     batch[\"audio\"].shape[0], dtype=batch[\"vocoder_label\"].dtype\n",
    "        # ).to(batch[\"audio\"].device) * 5\n",
    "        # loss = self.ce_loss(batch_res[\"speed_logit\"], base)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # @torch.enable_grad()\n",
    "    # @torch.inference_mode(False)\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     # h1 = self.hash_module(self.model)\n",
    "    #     torch.backends.cudnn.enabled = False\n",
    "\n",
    "    #     loss = self.ttt_step(batch, batch_idx)\n",
    "    #     self.backup_state_dict()\n",
    "\n",
    "    #     opt = self.get_ttt_optimizer()\n",
    "    #     opt.zero_grad()\n",
    "    #     self.manual_backward(loss, retain_graph=False)\n",
    "    #     opt.step()\n",
    "\n",
    "    #     # h2 = self.hash_module(self.model)\n",
    "    #     # print(h1, h2)\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "    #         batch_res = self._shared_eval_step(\n",
    "    #             batch, batch_idx, stage=\"test\", loss=False\n",
    "    #         )\n",
    "    #     # print(batch_res['logit'])\n",
    "    #     self.restore_state_dict()\n",
    "    #     return batch_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd409e22-0fcb-4bb6-a096-ad8a0f105a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T02:04:51.309847Z",
     "iopub.status.busy": "2024-02-21T02:04:51.309081Z",
     "iopub.status.idle": "2024-02-21T02:04:51.387367Z",
     "shell.execute_reply": "2024-02-21T02:04:51.385524Z",
     "shell.execute_reply.started": "2024-02-21T02:04:51.309733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2758, 1.0821, 1.1451])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.randn(3, 10)\n",
    "# torch.std(x, dim=1)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
