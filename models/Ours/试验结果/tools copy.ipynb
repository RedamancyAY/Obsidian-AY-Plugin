{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3611367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "from ay2.tools.pandas import format_numeric_of_df_columns\n",
    "from typing import List, Dict, Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c30c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/home/ay/data/DATA/1-model_save/00-Deepfake/1-df-audio-new\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211b1a52",
   "metadata": {},
   "source": [
    "# Help Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6497c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_result_from_csv_file(model, task, version=0, metric_prefix=\"test\", file_name=\"test\"):\n",
    "    \"\"\"\n",
    "    Read test results from a CSV file: `f\"{ROOT_PATH}/{model}/{task}/version_{version}/{file_name}.csv\"`\n",
    "\n",
    "    Args:\n",
    "        model (str): The name of the model.\n",
    "        task (str): The task for which the results are being read.\n",
    "        version (int, optional): The version number of the model. Defaults to 0.\n",
    "        metric_prefix (str, optional): The prefix used for metric columns in the CSV file. Defaults to \"test\".\n",
    "        file_name (str, optional): The name of the CSV file containing the test results. Defaults to \"test\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: A DataFrame containing the requested metrics, or None if the file does not exist.\n",
    "    \"\"\"\n",
    "    save_path = f\"{ROOT_PATH}/{model}/{task}/version_{version}\"\n",
    "    csv_path = os.path.join(save_path, f\"{file_name}.csv\")\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"Warning!!!! cannot find: \", csv_path)\n",
    "        return None\n",
    "\n",
    "    data = pd.read_csv(csv_path)\n",
    "    data = data[[f\"{metric_prefix}-acc\", f\"{metric_prefix}-auc\", f\"{metric_prefix}-eer\"]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d802fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_res_column_for_df(data:pd.DataFrame, acc=1, auc=1, eer=1, metric_prefix=\"test\")->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a new column in the DataFrame containing formatted model metrics.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame with columns for 'test-acc', 'test-auc', and 'test-eer'.\n",
    "        acc (bool, optional): Include the accuracy metric in the output. Defaults to True.\n",
    "        auc (bool, optional): Include the AUC metric in the output. Defaults to True.\n",
    "        eer (bool, optional): Include the Equal Error Rate (EER) metric in the output. Defaults to True.\n",
    "        metric_prefix (str, optional): The prefix used for metric columns. Defaults to \"test\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an additional column 'res' containing formatted metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def help_format(x):\n",
    "        _acc = \"{:.2f}\".format(x[f\"{metric_prefix}-acc\"] * 100)\n",
    "        _auc = \"{:.2f}\".format(x[f\"{metric_prefix}-auc\"] * 100)\n",
    "        _eer = \"{:.2f}\".format(x[f\"{metric_prefix}-eer\"] * 100)\n",
    "        \n",
    "        res = []\n",
    "        if acc: res.append(_acc)\n",
    "        if auc: res.append(_auc)\n",
    "        if eer: res.append(_eer)\n",
    "        res = \"/\".join(res) or \"\"\n",
    "    \n",
    "    data = data.apply(\n",
    "        lambda x: help_format(x),\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe5edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_versions = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10]\n",
    "MODELs_VERSIONS = {\n",
    "    \"LCNN\": _versions,\n",
    "    \"RawNet2\": _versions,\n",
    "    \"RawGAT\": _versions,\n",
    "    \"Wave2Vec2\": _versions,\n",
    "    \"WaveLM\": _versions,\n",
    "    \"LibriSeVoc\": _versions,\n",
    "    \"AudioClip\": _versions,\n",
    "    \"Wav2Clip\": _versions,\n",
    "    \"AASIST\": _versions,\n",
    "    \"SFATNet\": _versions,\n",
    "    \"ASDG\": _versions,\n",
    "    \"Ours/ResNet\":_versions,\n",
    "}\n",
    "models = MODELs_VERSIONS.keys()\n",
    "versions = MODELs_VERSIONS.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "524c7790",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_test_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m datas \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m version:\n\u001b[0;32m----> 7\u001b[0m     _data \u001b[38;5;241m=\u001b[39m read_test_result(model, task, v)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         datas\u001b[38;5;241m.\u001b[39mappend(_data[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_test_result' is not defined"
     ]
    }
   ],
   "source": [
    "tasks=[\"LibriSeVoc_inner\", \"wavefake_inner\", \"DECRO_english\", \"DECRO_chinese\"]\n",
    "\n",
    "for model, version in zip(models, versions):\n",
    "    for task in tasks:\n",
    "        datas = []\n",
    "        for v in version:\n",
    "            _data = read_test_result(model, task, v)\n",
    "            if _data is not None:\n",
    "                datas.append(_data[0:1])\n",
    "        datas = pd.concat(datas, ignore_index=True)\n",
    "        datas = datas[:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e411ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseReadResult(object):\n",
    "    def __init__(self, model, versions: List[int]=[], task=None, calc_avg=True, acc=0,auc=1, eer=1, metric_prefix=\"test\", file_name=\"test\", late_read=False):\n",
    "        assert len(models) == len(versions)\n",
    "\n",
    "        self.acc=acc\n",
    "        self.auc=auc\n",
    "        self.eer=eer\n",
    "        self.metric_prefix = metric_prefix\n",
    "        self.task = task\n",
    "        self.calc_avg = calc_avg\n",
    "        self.file_name=file_name\n",
    "        self.model_names = []\n",
    "        for model, version in zip(models, versions):\n",
    "            if isinstance(version, int):\n",
    "                self.model_names.append([model, version, model])\n",
    "            else:\n",
    "                for _v in version:\n",
    "                    self.model_names.append([model, _v, model + f\"-{_v}\"])\n",
    "\n",
    "        self.configure_Columns()\n",
    "\n",
    "        if not late_read:\n",
    "            self.data = self.read_all_datas(calc_avg=calc_avg)\n",
    "\n",
    "    def configure_Columns(self):\n",
    "        raise NotImplementedError\n",
    "        # self.METHODs = []\n",
    "\n",
    "    def set_Columns(self, columns):\n",
    "        self.METHODs = columns\n",
    "        self.data = self.read_all_datas(calc_avg=self.calc_avg)\n",
    "        \n",
    "    \n",
    "    def post_read_operation(self, data):\n",
    "        return data\n",
    "    \n",
    "    def read_data_for_model(self, model, version, model_name, calc_avg=True):\n",
    "        _data = read_test_result(model, self.task, version, metric_prefix=self.metric_prefix, file_name=self.file_name)\n",
    "        if _data is None:\n",
    "            return None\n",
    "\n",
    "        _data = self.post_read_operation(_data)\n",
    "        \n",
    "        res = _data\n",
    "        if calc_avg:\n",
    "            res.loc[\"Avg\", :] = res.mean()\n",
    "        res[\"res\"] = res.apply(\n",
    "            lambda x: format_res(x, acc=self.acc, auc=self.auc, eer=self.eer, metric_prefix=self.metric_prefix),\n",
    "            axis=1,\n",
    "        )\n",
    "        res[\"model\"] = model_name\n",
    "        res[\"dataset\"] = self.task.split(\"_\")[0]\n",
    "        try:\n",
    "            res[\"method\"] = self.METHODs\n",
    "        except ValueError as e:\n",
    "            print(model, version, e)\n",
    "            return None\n",
    "        return res\n",
    "\n",
    "    def read_all_datas(self, calc_avg=True):\n",
    "        DATA = []\n",
    "        model_names = []\n",
    "        for model, version, model_name in self.model_names:\n",
    "            _data = self.read_data_for_model(model, version, model_name, calc_avg=calc_avg)\n",
    "            if _data is not None:\n",
    "                DATA.append(_data)\n",
    "            model_names.append(model_name)\n",
    "\n",
    "        data = pd.concat(DATA)\n",
    "        data = data.pivot(index=\"model\", columns=[\"method\"], values=\"res\").reset_index(drop=False).set_index(\"model\")\n",
    "        models2 = [x for x in model_names if x in data.index]\n",
    "        data = data.loc[models2]\n",
    "        data = data[self.METHODs]\n",
    "        return data\n",
    "\n",
    "    def display(self, show_latex=0, drop=None):\n",
    "        data = self.data\n",
    "        if drop is not None:\n",
    "            data = self.data.drop(drop, axis=1)\n",
    "        display(HTML(data.to_html()))\n",
    "        if show_latex:\n",
    "            res = data.style.to_latex(column_format=\"lrr\")\n",
    "            res = res.replace('_', '-').replace('$', '')\n",
    "            print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "403c8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_inner_evaluation(\n",
    "    models,\n",
    "    tasks=[\"LibriSeVoc_inner\", \"wavefake_inner\", \"DECRO_english\", \"DECRO_chinese\"],\n",
    "    versions=[],\n",
    "    show_latex=0,\n",
    "    show_html=1,\n",
    "    metric_prefix=\"test\",\n",
    "    avg_res=True,\n",
    "    file_name=\"test\",\n",
    "):\n",
    "    metric_auc = f\"{metric_prefix}-auc\"\n",
    "    metric_eer = f\"{metric_prefix}-eer\"\n",
    "\n",
    "    DATA = []\n",
    "    for model, version in zip(models, versions):\n",
    "        for task in tasks:\n",
    "            datas = []\n",
    "            for v in version:\n",
    "                _data = read_test_result(model, task, v)\n",
    "                if _data is not None:\n",
    "                    datas.append(_data[0:1])\n",
    "            datas = pd.concat(datas, ignore_index=True)\n",
    "            datas = datas[:].mean()\n",
    "\n",
    "            _data = datas\n",
    "            if _data is None:\n",
    "                continue\n",
    "            res = dict(_data)\n",
    "            res[\"model\"] = model\n",
    "            res[\"task\"] = task\n",
    "            DATA.append(res)\n",
    "\n",
    "    data = pd.DataFrame(DATA)\n",
    "\n",
    "    data2 = data.groupby(\"model\").mean(numeric_only=True).reset_index()\n",
    "    data2[\"task\"] = \"mean\"\n",
    "    data = pd.concat([data, data2], ignore_index=True)\n",
    "\n",
    "    data[\"res\"] = data.apply(\n",
    "        lambda x: format_res(x, acc=0, auc=1, eer=1, metric_prefix=metric_prefix),\n",
    "        axis=1,\n",
    "    )\n",
    "    data = data.pivot(index=\"model\", columns=\"task\", values=\"res\").rename_axis(None, axis=1).reset_index()\n",
    "\n",
    "    data = data.set_index(\"model\")\n",
    "    columns = tasks\n",
    "    if avg_res:\n",
    "        columns += [\"mean\"]\n",
    "    data = data[columns]\n",
    "    data = data.loc[models]\n",
    "\n",
    "    data = format_numeric_of_df_columns(data)\n",
    "    \n",
    "    if show_html:\n",
    "        display(HTML(data.to_html()))\n",
    "    if show_latex:\n",
    "        print(data.style.to_latex(column_format=\"lrr\"))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a403d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = display_inner_evaluation(models, versions=versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d5915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
