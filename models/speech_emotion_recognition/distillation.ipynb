{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70dafd5b-a881-4fd4-9f4b-79a060b226ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T06:16:19.787811Z",
     "iopub.status.busy": "2023-07-28T06:16:19.787271Z",
     "iopub.status.idle": "2023-07-28T06:16:19.833495Z",
     "shell.execute_reply": "2023-07-28T06:16:19.832750Z",
     "shell.execute_reply.started": "2023-07-28T06:16:19.787759Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63f9a65-97e3-442c-82a3-41f446343c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T06:16:28.966051Z",
     "iopub.status.busy": "2023-07-28T06:16:28.965474Z",
     "iopub.status.idle": "2023-07-28T06:16:29.009531Z",
     "shell.execute_reply": "2023-07-28T06:16:29.008772Z",
     "shell.execute_reply.started": "2023-07-28T06:16:28.966005Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a531fa67-7c25-480f-926f-cc176113abd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-30T02:13:26.578517Z",
     "iopub.status.busy": "2023-07-30T02:13:26.577522Z",
     "iopub.status.idle": "2023-07-30T02:13:28.600829Z",
     "shell.execute_reply": "2023-07-30T02:13:28.599991Z",
     "shell.execute_reply.started": "2023-07-30T02:13:26.578470Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from ay2.torch.deepfake_detection import DeepfakeAudioClassification\n",
    "from ay2.torch.lightning.callbacks import (\n",
    "    ACC_Callback,\n",
    "    APCallback,\n",
    "    AUC_Callback,\n",
    "    Color_progress_bar,\n",
    "    EER_Callback,\n",
    "    EarlyStop,\n",
    ")\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaba3e6-880b-4074-ada5-93659512306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .train_model import WaveLM_lit\n",
    "\n",
    "from ._data import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938906be-0520-408a-b8d5-5322194c0539",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-activity"
    ]
   },
   "outputs": [],
   "source": [
    "from train_model import WaveLM_lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203c464-21c1-4065-9169-8b4d3ab5581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Ours.model import AudioModel, FeatureModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b98cb-36b2-43d6-bf29-8e34882fb487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T05:30:07.093598Z",
     "iopub.status.busy": "2023-07-26T05:30:07.092992Z",
     "iopub.status.idle": "2023-07-26T05:30:07.148562Z",
     "shell.execute_reply": "2023-07-26T05:30:07.147301Z",
     "shell.execute_reply.started": "2023-07-26T05:30:07.093541Z"
    }
   },
   "source": [
    "## Our's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747acaeb-b4f2-4659-8f0f-54fb4c1d9789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-28T06:23:34.885054Z",
     "iopub.status.busy": "2023-07-28T06:23:34.884519Z",
     "iopub.status.idle": "2023-07-28T06:23:34.928359Z",
     "shell.execute_reply": "2023-07-28T06:23:34.927605Z",
     "shell.execute_reply.started": "2023-07-28T06:23:34.885006Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pretrained_model():\n",
    "    ckpt_path = \"/home/ay/data/DATA/1-model_save/0-Audio/speech_emotion_recognition/version_0/checkpoints/best-epoch=110-val-loss=0.16.ckpt\"\n",
    "    model1 = WaveLM_lit(num_classes=13)\n",
    "    model1 = model1.load_from_checkpoint(ckpt_path).to(\"cpu\")\n",
    "    return model1.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e059901-72ee-4453-935b-13200afd484a",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9509f528-5269-436f-be3f-911bd40b0e28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T06:37:06.015632Z",
     "iopub.status.busy": "2023-07-26T06:37:06.014948Z",
     "iopub.status.idle": "2023-07-26T06:37:06.063869Z",
     "shell.execute_reply": "2023-07-26T06:37:06.063201Z",
     "shell.execute_reply.started": "2023-07-26T06:37:06.015584Z"
    }
   },
   "outputs": [],
   "source": [
    "class CLIPLoss1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPLoss1D, self).__init__()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.loss_image = nn.CrossEntropyLoss()\n",
    "        self.loss_text = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logit_scale * text_features @ image_features.t()\n",
    "\n",
    "        batch_size = image_features.shape[0]\n",
    "        ground_truth = torch.arange(\n",
    "            batch_size, dtype=torch.long, device=image_features.device\n",
    "        )\n",
    "        return (\n",
    "            self.loss_image(logits_per_image, ground_truth)\n",
    "            + self.loss_text(logits_per_text, ground_truth)\n",
    "        ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e2f5f711-5072-48be-9fb7-1c6e91dddde0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T07:23:41.166327Z",
     "iopub.status.busy": "2023-07-26T07:23:41.165800Z",
     "iopub.status.idle": "2023-07-26T07:23:41.229455Z",
     "shell.execute_reply": "2023-07-26T07:23:41.228709Z",
     "shell.execute_reply.started": "2023-07-26T07:23:41.166274Z"
    }
   },
   "outputs": [],
   "source": [
    "class Our_lit(DeepfakeAudioClassification):\n",
    "    def __init__(self, num_classes=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model1 = get_pretrained_model()\n",
    "        self.model2 = FeatureModel()\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 128),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.loss_fn = CLIPLoss1D()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def calcuate_loss(self, batch_res, batch):\n",
    "        loss1 = self.loss_fn(self.mlp1(batch_res[\"feat_org\"]), batch_res[\"feat_tar\"])\n",
    "        loss2 = self.loss_fn(batch_res[\"feat_org\"], self.mlp2(batch_res[\"feat_tar\"]))\n",
    "        return (loss1 + loss2) / 2\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model2.parameters(), lr=0.0001, weight_decay=0.0001\n",
    "        )\n",
    "        return [optimizer]\n",
    "\n",
    "    def _shared_pred(self, batch, batch_idx):\n",
    "        audio, sample_rate = batch[\"audio\"], batch[\"sample_rate\"]\n",
    "        # if len(audio.shape) == 3:\n",
    "        # audio = audio[:, 0, :]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature_org = self.model1.get_feature(audio[:, 0, :])\n",
    "        feature_tar = self.model2.get_feature(audio)\n",
    "        return {\"feat_org\": feature_org, \"feat_tar\": feature_tar}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec118b7-c278-450c-b948-2855e41838d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T01:06:40.852132Z",
     "iopub.status.busy": "2023-07-27T01:06:40.851539Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_callbacks():\n",
    "    callbacks = [\n",
    "        Color_progress_bar(),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=None,\n",
    "            save_top_k=1,\n",
    "            monitor=\"val-loss\",\n",
    "            mode=\"min\",\n",
    "            save_last=True,\n",
    "            filename=\"best-{epoch}-{val-loss:.2f}\",\n",
    "        ),\n",
    "        EarlyStop(\n",
    "            min_epochs=50,\n",
    "            monitor=\"val-loss\",\n",
    "            min_delta=0.001,\n",
    "            patience=5,\n",
    "            mode=\"min\",\n",
    "            stopping_threshold=0.1,\n",
    "            verbose=False,\n",
    "        ),\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a1441-973a-423a-80ad-e96eca7d1194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T02:41:03.975062Z",
     "iopub.status.busy": "2023-07-27T02:41:03.974469Z",
     "iopub.status.idle": "2023-07-27T02:41:04.122873Z",
     "shell.execute_reply": "2023-07-27T02:41:04.121515Z",
     "shell.execute_reply.started": "2023-07-27T02:41:03.975014Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/ay/data/DATA/1-model_save/0-Audio\"\n",
    "\n",
    "def start_distillation(gpu):\n",
    "\n",
    "    dl = get_data()\n",
    "\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=300,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[gpu],\n",
    "        logger=pl.loggers.CSVLogger(\n",
    "            ROOT_DIR,\n",
    "            name=\"speech_emotion_recognition\",\n",
    "            version=1,\n",
    "        ),\n",
    "        check_val_every_n_epoch=1,\n",
    "        callbacks=make_callbacks(),\n",
    "        default_root_dir=ROOT_DIR,\n",
    "    )\n",
    "    model = Our_lit()\n",
    "\n",
    "    trainer.fit(model, dl.train, val_dataloaders=dl.val)\n",
    "\n",
    "    torch.save(model.model2.state_dict(), trainer.logger.log_dir + \"/model2.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
