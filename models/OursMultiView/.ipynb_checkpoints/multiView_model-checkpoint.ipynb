{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f183a79-d07e-4e9f-9606-520ada0bedf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T03:19:57.295296Z",
     "iopub.status.busy": "2024-03-25T03:19:57.294686Z",
     "iopub.status.idle": "2024-03-25T03:19:57.346128Z",
     "shell.execute_reply": "2024-03-25T03:19:57.344831Z",
     "shell.execute_reply.started": "2024-03-25T03:19:57.295228Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1caf9846-a1a5-49b5-b121-fb92bac2868c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T03:19:57.349811Z",
     "iopub.status.busy": "2024-03-25T03:19:57.348923Z",
     "iopub.status.idle": "2024-03-25T03:19:58.863110Z",
     "shell.execute_reply": "2024-03-25T03:19:58.862186Z",
     "shell.execute_reply.started": "2024-03-25T03:19:57.349753Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from ay2.torch.nn import LambdaFunctionModule\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311849e6-480b-41a6-8a5d-9dc525a294fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-25T03:19:58.873911Z",
     "iopub.status.busy": "2024-03-25T03:19:58.873733Z",
     "iopub.status.idle": "2024-03-25T03:19:59.199845Z",
     "shell.execute_reply": "2024-03-25T03:19:59.198988Z",
     "shell.execute_reply.started": "2024-03-25T03:19:58.873891Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "51d4a77b-985e-4aac-a4c7-67dad9261ac5",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-25T07:50:14.769286Z",
     "iopub.status.busy": "2024-03-25T07:50:14.769010Z",
     "iopub.status.idle": "2024-03-25T07:50:14.795535Z",
     "shell.execute_reply": "2024-03-25T07:50:14.794368Z",
     "shell.execute_reply.started": "2024-03-25T07:50:14.769266Z"
    },
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from .rawnet.rawnet2 import RawNet2\n",
    "    from .resnet import ResNet\n",
    "    from .resnet1d import ResNet1D\n",
    "except ImportError:\n",
    "    from rawnet.rawnet2 import RawNet2\n",
    "    from resnet import ResNet\n",
    "    from resnet1d import ResNet1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "416b831f-95fd-4320-b30c-7aa01061e40a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-25T07:50:15.522797Z",
     "iopub.status.busy": "2024-03-25T07:50:15.521639Z",
     "iopub.status.idle": "2024-03-25T07:50:15.600343Z",
     "shell.execute_reply": "2024-03-25T07:50:15.599261Z",
     "shell.execute_reply.started": "2024-03-25T07:50:15.522746Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiViewModel(nn.Module):\n",
    "    def __init__(self, verbose=0, cfg=None, args=None, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.feature_model2D = ResNet()\n",
    "        self.feature_model1D = ResNet1D(\n",
    "            in_channels=1,\n",
    "            base_filters=64,\n",
    "            kernel_size=3,\n",
    "            downsample_stride=2,\n",
    "            groups=1,\n",
    "            n_block=8,\n",
    "            n_classes=0,\n",
    "            downsample_gap=2,\n",
    "            increasefilter_gap=2,\n",
    "            verbose=1,\n",
    "        )\n",
    "        final_dim = 512\n",
    "\n",
    "        self.proj1D = nn.Linear(1024, 512)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.cls_final = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Linear(final_dim * 2, 1, bias=False)),\n",
    "        )\n",
    "        self.cls1D, self.cls2D = [\n",
    "            nn.Sequential(\n",
    "                nn.utils.weight_norm(nn.Linear(final_dim, 1, bias=False)),\n",
    "            )\n",
    "            for _ in range(2)\n",
    "        ]\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.spectrogram_transforms = nn.ModuleList(\n",
    "            [\n",
    "                torchaudio.transforms.Spectrogram(\n",
    "                    n_fft=512 // (2 ** (i + 2)), hop_length=187\n",
    "                )\n",
    "                for i in range(4)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.alphas = nn.ParameterList(\n",
    "            [nn.Parameter(torch.ones(1, _c, 1) * 0.5) for _c in [64, 128, 256, 512]]\n",
    "        )\n",
    "        self.betas = nn.ParameterList(\n",
    "            [nn.Parameter(torch.ones(1, _c, 1, 1) * 0.5) for _c in [64, 128, 256, 512]]\n",
    "        )\n",
    "        self.set_verbose(verbose)\n",
    "\n",
    "    def feature_norm(self, code):\n",
    "        code_norm = code.norm(p=2, dim=1, keepdim=True) / 10.0\n",
    "        code = torch.div(code, code_norm)\n",
    "        return code\n",
    "\n",
    "    def print_shape(self, *args):\n",
    "        for x in args:\n",
    "            print(x.shape)\n",
    "\n",
    "    def set_verbose(self, verbose):\n",
    "        self.feature_model1D.verbose = verbose\n",
    "        self.feature_model2D.verbose = verbose\n",
    "\n",
    "    def transform_audio_into_spectorgram(self, x, transform):\n",
    "        x = transform(x)\n",
    "        x = torch.log(x + 1e-7)\n",
    "        x = (x - torch.mean(x, dim=(1, 2, 3), keepdim=True)) / (\n",
    "            torch.std(x, dim=(1, 2, 3), keepdim=True) + 1e-9\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    def fuse_2Dfeat_for_1D(self, feat1D, feat2D, idx):\n",
    "        h, w = feat2D.shape[-2:]\n",
    "        L = feat1D.shape[-1]\n",
    "        scale_factor = L / (h * w)\n",
    "\n",
    "        feat2D = rearrange(feat2D, \"b c h w -> b c (w h)\")\n",
    "        feat2D = F.upsample_nearest(feat2D, scale_factor=scale_factor + 0.0001)\n",
    "\n",
    "        feat1D = self.alphas[idx] * feat1D + (1 - self.alphas[idx]) * feat2D\n",
    "        return feat1D\n",
    "\n",
    "    def fuse_1Dfeat_for_2D(self, feat1D, feat2D, idx):\n",
    "        feat1D = self.transform_audio_into_spectorgram(\n",
    "            feat1D, self.spectrogram_transforms[idx]\n",
    "        )\n",
    "        feat2D = self.betas[idx] * feat2D + (1 - self.betas[idx]) * feat1D\n",
    "        return feat2D\n",
    "\n",
    "    def forward(self, x, stage=\"test\", batch=None):\n",
    "        batch_size = x.shape[0]\n",
    "        res = {}\n",
    "        # _input = x.clone()\n",
    "\n",
    "        feat1_1 = self.feature_model1D.compute_stage1(x)\n",
    "        feat2_1 = self.feature_model2D.compute_stage1(x)\n",
    "\n",
    "        feat1_2 = self.feature_model1D.compute_stage2(\n",
    "            self.fuse_2Dfeat_for_1D(feat1_1, feat2_1, 0)\n",
    "        )\n",
    "        feat2_2 = self.feature_model2D.compute_stage2(\n",
    "            self.fuse_1Dfeat_for_2D(feat1_1, feat2_1, 0)\n",
    "        )\n",
    "\n",
    "        feat1_3 = self.feature_model1D.compute_stage3(\n",
    "            self.fuse_2Dfeat_for_1D(feat1_2, feat2_2, 1)\n",
    "        )\n",
    "        feat2_3 = self.feature_model2D.compute_stage3(\n",
    "            self.fuse_1Dfeat_for_2D(feat1_2, feat2_2, 1)\n",
    "        )\n",
    "\n",
    "        feat1_4 = self.feature_model1D.compute_stage4(\n",
    "            self.fuse_2Dfeat_for_1D(feat1_3, feat2_3, 2)\n",
    "        )\n",
    "        feat2_4 = self.feature_model2D.compute_stage4(\n",
    "            self.fuse_1Dfeat_for_2D(feat1_3, feat2_3, 2)\n",
    "        )\n",
    "\n",
    "        feat1 = self.feature_model1D.compute_latent_feature(feat1_4)\n",
    "        feat2 = self.feature_model2D.compute_latent_feature(feat2_4)\n",
    "\n",
    "        res[\"feature\"] = torch.concat([feat1, feat2], dim=-1)\n",
    "        res[\"logit1D\"] = self.cls1D(self.dropout(feat1)).squeeze(-1)\n",
    "        res[\"logit2D\"] = self.cls2D(self.dropout(feat2)).squeeze(-1)\n",
    "        res[\"logit\"] = self.cls_final(self.dropout(res[\"feature\"])).squeeze(-1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "48146a05-6034-4d67-b788-45acb6700ffb",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-25T07:50:16.272798Z",
     "iopub.status.busy": "2024-03-25T07:50:16.271571Z",
     "iopub.status.idle": "2024-03-25T07:50:16.728863Z",
     "shell.execute_reply": "2024-03-25T07:50:16.728014Z",
     "shell.execute_reply.started": "2024-03-25T07:50:16.272752Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet1D input shape torch.Size([3, 1, 48000])\n",
      "ResNet1D Stage 1: output shape torch.Size([3, 64, 12000])\n",
      "ResNet Stage 1: output shape torch.Size([3, 64, 65, 65])\n",
      "ResNet1D Stage 2: output shape torch.Size([3, 128, 6000])\n",
      "ResNet Stage 2: output shape torch.Size([3, 128, 33, 33])\n",
      "ResNet1D Stage 3: output shape torch.Size([3, 256, 3000])\n",
      "ResNet Stage 3: output shape torch.Size([3, 256, 17, 17])\n",
      "ResNet1D Stage 4: output shape torch.Size([3, 512, 1500])\n",
      "ResNet Stage 4: output shape torch.Size([3, 512, 9, 9])\n",
      "ResNet1D Latent Feature: output shape torch.Size([3, 512])\n",
      "ResNet Latent Feature: output shape torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "model = MultiViewModel(verbose=1)\n",
    "x = torch.randn(3, 1, 48000)\n",
    "_ = model(x)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
