{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "490a38db-54f9-4997-97ad-26ba9df87df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:26:19.030719Z",
     "iopub.status.busy": "2024-08-03T07:26:19.029321Z",
     "iopub.status.idle": "2024-08-03T07:26:19.068735Z",
     "shell.execute_reply": "2024-08-03T07:26:19.067645Z",
     "shell.execute_reply.started": "2024-08-03T07:26:19.030654Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22232e64-280e-4d5d-949f-86df8af05afb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:26:19.617276Z",
     "iopub.status.busy": "2024-08-03T07:26:19.617038Z",
     "iopub.status.idle": "2024-08-03T07:26:21.504824Z",
     "shell.execute_reply": "2024-08-03T07:26:21.503790Z",
     "shell.execute_reply.started": "2024-08-03T07:26:19.617257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from numpy import prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf1f3f0-c3b3-4548-b06d-763b77ed24e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:26:21.506414Z",
     "iopub.status.busy": "2024-08-03T07:26:21.506076Z",
     "iopub.status.idle": "2024-08-03T07:26:21.522334Z",
     "shell.execute_reply": "2024-08-03T07:26:21.521514Z",
     "shell.execute_reply.started": "2024-08-03T07:26:21.506395Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5393c463-729e-4c2c-bd85-bacd05797d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T07:26:21.523090Z",
     "iopub.status.busy": "2024-08-03T07:26:21.522944Z",
     "iopub.status.idle": "2024-08-03T07:26:21.542500Z",
     "shell.execute_reply": "2024-08-03T07:26:21.541674Z",
     "shell.execute_reply.started": "2024-08-03T07:26:21.523075Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "66f33913-0252-415c-b11a-46052534a0a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T02:25:17.712690Z",
     "iopub.status.busy": "2024-08-04T02:25:17.712047Z",
     "iopub.status.idle": "2024-08-04T02:25:17.765175Z",
     "shell.execute_reply": "2024-08-04T02:25:17.763229Z",
     "shell.execute_reply.started": "2024-08-04T02:25:17.712631Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from capsules import RoutingCapsules\n",
    "except ImportError:\n",
    "    from .capsules import RoutingCapsules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8d3c7-0270-4c23-b16b-8e78ec0efcd6",
   "metadata": {},
   "source": [
    "# Mel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f19c8e-9fbf-457c-858e-23f1d85768cd",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "需要使用Mel Spectrogram将输入的音频转换为 $224\\times 224$的谱图，然后在通道上叠加三次，得到$3\\times 224 \\times 224$的输入。　　\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "31fdf42e-e681-4964-94d6-a5949590329c",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T11:00:52.521793Z",
     "iopub.status.busy": "2024-08-03T11:00:52.521267Z",
     "iopub.status.idle": "2024-08-03T11:00:52.575250Z",
     "shell.execute_reply": "2024-08-03T11:00:52.573762Z",
     "shell.execute_reply.started": "2024-08-03T11:00:52.521747Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Melspec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=16000,\n",
    "            hop_length=215,\n",
    "            n_fft=512,\n",
    "            n_mels=224,\n",
    "        )\n",
    "        # self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        #     sample_rate=16000,\n",
    "        #     hop_length=512,\n",
    "        #     n_fft=2048,\n",
    "        #     n_mels=224,\n",
    "        # ) # (48000 -> 224,94)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "          x:torch.Tensor: (B, 1, 48000)\n",
    "\n",
    "        Returns:\n",
    "            a tensor: (B, 3, 224, 224)\n",
    "        \"\"\"\n",
    "        mel = self.mel_spectrogram(x)\n",
    "        mel = torch.concat([mel, mel, mel], dim=1)\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eacabadc-3a3e-44de-a601-27bcda8b3ace",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T11:00:53.285776Z",
     "iopub.status.busy": "2024-08-03T11:00:53.285178Z",
     "iopub.status.idle": "2024-08-03T11:00:53.344702Z",
     "shell.execute_reply": "2024-08-03T11:00:53.343706Z",
     "shell.execute_reply.started": "2024-08-03T11:00:53.285717Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 测试\n",
    "x = torch.randn(2, 1, 16000 * 3)\n",
    "m = Melspec()\n",
    "m(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86a957-65d9-4fdf-98e6-43062db1dff6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f390c4-754b-4057-b1d3-6f446a167c85",
   "metadata": {},
   "source": [
    "# VGG18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10588be-088b-4c7b-a247-b3527604002c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "<center><img src=\"https://cdn.jsdelivr.net/gh/RedamancyAY/CloudImage@main/img/202408031530037.png\" width=\"700\" alt=\"model structrue\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48d404a-4949-49d8-92a9-8a54a0d35ff1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "模型代码见：[torchvision.models.vgg — Torchvision main documentation](https://pytorch.org/vision/main/_modules/torchvision/models/vgg.html#vgg19)\n",
    "\n",
    "原始模型的forward：\n",
    "```python\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804dafcd-571f-4020-9b69-961fae2052df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    ":::{warning}\n",
    "不知道是什么VGG18提取出来的是什么格式，\n",
    "- 如果是二维特征，那么形状为$512 \\times 7 \\times 7$\n",
    "- 如果是一维特征，就是分类前的特征，那么是$B \\times 512$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b9a79fec-ff02-4263-9747-035ad82f0c09",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T10:24:25.394284Z",
     "iopub.status.busy": "2024-08-03T10:24:25.392638Z",
     "iopub.status.idle": "2024-08-03T10:24:25.446155Z",
     "shell.execute_reply": "2024-08-03T10:24:25.444877Z",
     "shell.execute_reply.started": "2024-08-03T10:24:25.394202Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VGG18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = vgg19(weights=torchvision.models.VGG19_Weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "          x:torch.Tensor: (B, 3, 224, 224)\n",
    "\n",
    "        Returns:\n",
    "            a tensor: (B, 512, 7, 7)\n",
    "        \"\"\"\n",
    "        x = self.model.features(x)  # (B, 512, 7, 7)\n",
    "        # x = self.model.avgpool(x) # (B, 512, 7, 7)\n",
    "        # x = torch.flatten(x, 1) # (B, 512)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b419de86-c0ea-4888-8002-81966721dc41",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T10:24:28.089277Z",
     "iopub.status.busy": "2024-08-03T10:24:28.088042Z",
     "iopub.status.idle": "2024-08-03T10:24:29.442769Z",
     "shell.execute_reply": "2024-08-03T10:24:29.442116Z",
     "shell.execute_reply.started": "2024-08-03T10:24:28.089203Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 224, 94)\n",
    "model = VGG18()\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c74df-59d5-4d65-9df2-7bc03ea3404f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57240d2-ca97-4931-94a4-82e0db5cfe26",
   "metadata": {},
   "source": [
    ":::{warning}\n",
    "不知道是什么VGG18提取出来的是什么格式，这里的attention也知道该怎么实现\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209f78a-16a6-46a6-a502-d2158cbf25f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let $F$ denote the set of features extracted by VGG18, where $F=\\left\\{f_1, f_2, \\ldots, f_n\\right\\}$ and each $f_i$ is a feature vector. The attention mechanism assigns a weight $w_i$ to each feature vector $f_i$, with the weights being determined by a trainable attention layer. The output of the attention mechanism, $F^{\\prime}$, is a weighted sum of the feature vectors, given by:\n",
    "$$\n",
    "F^{\\prime}=\\sum_{i=1}^n w_i \\cdot f_i\n",
    "$$\n",
    "\n",
    "The weights $w_i$ are computed using a softmax function over the scores assigned to each feature vector by the attention layer, as follows:\n",
    "$$\n",
    "w_i=\\frac{e^{s\\left(f_i\\right)}}{\\sum_{j=1}^n e^{s\\left(f_j\\right)}}\n",
    "$$\n",
    "where $s\\left(f_i\\right)$ is the score assigned to feature vector $f_i$ by the attention layer, which is typically implemented as a fully connected layer with a single output unit. The softmax function ensures that the weights sum up to 1 , allowing them to be interpreted as probabilities that indicate the importance of each feature vector in the context of the detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd15b4-eae6-44e8-995c-b0c97764ea40",
   "metadata": {},
   "source": [
    "看起来像，VGG提取出来的特征是($B \\times T\\times C$), 在$T$维度上进行加权和相加，得到$B\\times C$。　\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f34c13b8-f11b-4ad4-856f-210f420238aa",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T07:46:39.377814Z",
     "iopub.status.busy": "2024-08-03T07:46:39.377529Z",
     "iopub.status.idle": "2024-08-03T07:46:39.416163Z",
     "shell.execute_reply": "2024-08-03T07:46:39.415126Z",
     "shell.execute_reply.started": "2024-08-03T07:46:39.377790Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f543d068-a582-47d5-ac2d-0479aba9c886",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T11:20:51.267466Z",
     "iopub.status.busy": "2024-08-03T11:20:51.266753Z",
     "iopub.status.idle": "2024-08-03T11:20:51.322285Z",
     "shell.execute_reply": "2024-08-03T11:20:51.320361Z",
     "shell.execute_reply.started": "2024-08-03T11:20:51.267403Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-activity"
    ]
   },
   "outputs": [],
   "source": [
    "class Attention_to_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "          x:torch.Tensor: (B, 512, 7, 7)\n",
    "\n",
    "        Returns:\n",
    "            a tensor: (B, 512, 7, 7)\n",
    "        \"\"\"\n",
    "        x = rearrange(x, \"b c h w -> b (h w ) c\")\n",
    "        attn_weight = self.attn(x)  # (b, hw, 1)\n",
    "\n",
    "        attn_weight = attn_weight.softmax(1)  # (b, hw, 1)\n",
    "        x = x * attn_weight  # (b, hw, c)\n",
    "        x = x.sum(1)  # (b, c)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a2b50717-2bb5-47a4-8263-31438c8b278a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T11:20:56.606389Z",
     "iopub.status.busy": "2024-08-03T11:20:56.605793Z",
     "iopub.status.idle": "2024-08-03T11:20:56.661614Z",
     "shell.execute_reply": "2024-08-03T11:20:56.660855Z",
     "shell.execute_reply.started": "2024-08-03T11:20:56.606331Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Attention_to_1D()\n",
    "x = torch.randn(2, 512, 7, 7)\n",
    "m(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5c699f39-807d-4609-9df8-fe2f1984e366",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T10:52:56.233932Z",
     "iopub.status.busy": "2024-08-03T10:52:56.233639Z",
     "iopub.status.idle": "2024-08-03T10:52:56.263599Z",
     "shell.execute_reply": "2024-08-03T10:52:56.262945Z",
     "shell.execute_reply.started": "2024-08-03T10:52:56.233905Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention_to_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.Linear(49, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "          x:torch.Tensor: (B, 512, 7, 7)\n",
    "\n",
    "        Returns:\n",
    "            a tensor: (B, 512, 7, 7)\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "        x = rearrange(x, \"b c h w -> b c (h w)\")\n",
    "        attn_weight = self.attn(x)  # (b, c, 1)\n",
    "        attn_weight = attn_weight.softmax(1)  # (b, hw, 1)\n",
    "\n",
    "        x = x * attn_weight  # (b, c, (hw))\n",
    "        x = x.sum((1), keepdims=True)  # (b, 1, hw)\n",
    "        x = rearrange(x, \"b 1 (h w) -> b 1 h w\", h=h)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7dcd1017-ade1-4955-bd78-4cee471cabc8",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T10:52:57.186247Z",
     "iopub.status.busy": "2024-08-03T10:52:57.185786Z",
     "iopub.status.idle": "2024-08-03T10:52:57.212886Z",
     "shell.execute_reply": "2024-08-03T10:52:57.212211Z",
     "shell.execute_reply.started": "2024-08-03T10:52:57.186222Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 7, 7])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Attention_to_2D()\n",
    "x = torch.randn(2, 512, 7, 7)\n",
    "m(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ab4b2-d5d5-45d5-b86e-bd6d9738e8d9",
   "metadata": {},
   "source": [
    "# Capsule Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc6a85-0c08-45f1-a7e5-58dcae734d30",
   "metadata": {},
   "source": [
    "## conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9cbc24b9-2c0e-4d7c-9132-c0721ea4bc22",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-07-31T13:45:50.811136Z",
     "iopub.status.busy": "2024-07-31T13:45:50.810804Z",
     "iopub.status.idle": "2024-07-31T13:45:50.889883Z",
     "shell.execute_reply": "2024-07-31T13:45:50.889159Z",
     "shell.execute_reply.started": "2024-07-31T13:45:50.811107Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 1, 1])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layer = nn.Sequential(\n",
    "    nn.Conv2d(1, 512, 3, stride=2, bias=True, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(512, 512, 3, stride=2, bias=True, padding=0),\n",
    ")\n",
    "x = torch.randn(2, 1, 7, 7)\n",
    "conv_layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2caaca-518a-493b-88de-ac1dbcc0bac7",
   "metadata": {},
   "source": [
    "## PrimaryCaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5cf7bd5-748b-483f-a85d-5fe3bbbbb106",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T07:51:08.796651Z",
     "iopub.status.busy": "2024-08-03T07:51:08.795929Z",
     "iopub.status.idle": "2024-08-03T07:51:08.851279Z",
     "shell.execute_reply": "2024-08-03T07:51:08.850120Z",
     "shell.execute_reply.started": "2024-08-03T07:51:08.796588Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32, kernel_size=9, stride=2):\n",
    "        \"\"\"Constructs a list of convolutional layers to be used in\n",
    "        creating capsule output vectors.\n",
    "        param num_capsules: number of capsules to create\n",
    "        param in_channels: input depth of features, default value = 256\n",
    "        param out_channels: output depth of the convolutional layers, default value = 32\n",
    "        \"\"\"\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        # creating a list of convolutional layers for each capsule I want to create\n",
    "        # all capsules have a conv layer with the same parameters\n",
    "        self.capsules = nn.ModuleList(\n",
    "            [nn.Linear(in_features=in_channels, out_features=out_channels) for _ in range(num_capsules)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the feedforward behavior.\n",
    "        param x: the input; features from a convolutional layer\n",
    "        return: a set of normalized, capsule output vectors\n",
    "        \"\"\"\n",
    "        # get batch size of inputs\n",
    "        batch_size = x.size(0)\n",
    "        # reshape convolutional layer outputs to be (batch_size, vector_dim=1152, 1)\n",
    "        # print(self.capsules[0](x).shape)\n",
    "\n",
    "        u = [capsule(x).view(batch_size, -1, 1) for capsule in self.capsules]\n",
    "\n",
    "        # stack up output vectors, u, one for each capsule\n",
    "        u = torch.cat(u, dim=-1)\n",
    "        # squashing the stack of vectors\n",
    "        u_squash = self.squash(u)\n",
    "        return u_squash\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        \"\"\"Squashes an input Tensor so it has a magnitude between 0-1.\n",
    "        param input_tensor: a stack of capsule inputs, s_j\n",
    "        return: a stack of normalized, capsule output vectors, v_j\n",
    "        \"\"\"\n",
    "        squared_norm = (input_tensor**2).sum(dim=-1, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)  # normalization coeff\n",
    "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5e1dcda-2153-44ad-b396-e6472cbba4ad",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T07:51:09.473039Z",
     "iopub.status.busy": "2024-08-03T07:51:09.471798Z",
     "iopub.status.idle": "2024-08-03T07:51:09.638082Z",
     "shell.execute_reply": "2024-08-03T07:51:09.636756Z",
     "shell.execute_reply.started": "2024-08-03T07:51:09.472976Z"
    },
    "lines_to_next_cell": 0,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 512)\n",
    "m = PrimaryCaps(kernel_size=3, in_channels=512, out_channels=512, stride=1, num_capsules=32)\n",
    "m(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd561db-9688-4a9d-9d8d-6611aee16eed",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ebe980-8426-4ed0-81d7-f41014dca024",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4d51129-ee7b-4447-a9b8-ece91d6518ae",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T07:51:25.978594Z",
     "iopub.status.busy": "2024-08-03T07:51:25.978077Z",
     "iopub.status.idle": "2024-08-03T07:51:26.042639Z",
     "shell.execute_reply": "2024-08-03T07:51:26.041517Z",
     "shell.execute_reply.started": "2024-08-03T07:51:25.978561Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(input_tensor, dim=1):\n",
    "    # transpose input\n",
    "    transposed_input = input_tensor.transpose(dim, len(input_tensor.size()) - 1)\n",
    "    # calculate softmax\n",
    "    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\n",
    "    # un-transpose result\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input_tensor.size()) - 1)\n",
    "\n",
    "\n",
    "# dynamic routing\n",
    "def dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\n",
    "    \"\"\"Performs dynamic routing between two capsule layers.\n",
    "    param b_ij: initial log probabilities that capsule i should be coupled to capsule j\n",
    "    param u_hat: input, weighted capsule vectors, W u\n",
    "    param squash: given, normalizing squash function\n",
    "    param routing_iterations: number of times to update coupling coefficients\n",
    "    return: v_j, output capsule vectors\n",
    "    \"\"\"\n",
    "    # update b_ij, c_ij for number of routing iterations\n",
    "    for iteration in range(routing_iterations):\n",
    "        # softmax calculation of coupling coefficients, c_ij\n",
    "        c_ij = softmax(b_ij, dim=2)\n",
    "\n",
    "        # calculating total capsule inputs, s_j = sum(c_ij*u_hat)\n",
    "        s_j = (c_ij * u_hat).sum(dim=2, keepdim=True)\n",
    "\n",
    "        # squashing to get a normalized vector output, v_j\n",
    "        v_j = squash(s_j)\n",
    "\n",
    "        # if not on the last iteration, calculate agreement and new b_ij\n",
    "        if iteration < routing_iterations - 1:\n",
    "            # agreement\n",
    "            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\n",
    "\n",
    "            # new b_ij\n",
    "            b_ij = b_ij + a_ij\n",
    "\n",
    "    return v_j  # return latest v_j\n",
    "\n",
    "\n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=10, previous_layer_nodes=32 * 6 * 6, in_channels=8, out_channels=16):\n",
    "        \"\"\"Constructs an initial weight matrix, W, and sets class variables.\n",
    "        param num_capsules: number of capsules to create\n",
    "        param previous_layer_nodes: dimension of input capsule vector, default value = 1152\n",
    "        param in_channels: number of capsules in previous layer, default value = 8\n",
    "        param out_channels: dimensions of output capsule vector, default value = 16\n",
    "        \"\"\"\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        # setting class variables\n",
    "        self.num_capsules = num_capsules\n",
    "        self.previous_layer_nodes = previous_layer_nodes  # vector input (dim=1152)\n",
    "        self.in_channels = in_channels  # previous layer's number of capsules\n",
    "\n",
    "        # starting out with a randomly initialized weight matrix, W\n",
    "        # these will be the weights connecting the PrimaryCaps and DigitCaps layers\n",
    "        self.W = nn.Parameter(torch.randn(num_capsules, previous_layer_nodes, in_channels, out_channels))\n",
    "\n",
    "    def forward(self, u):\n",
    "        \"\"\"Defines the feedforward behavior.\n",
    "        param u: the input; vectors from the previous PrimaryCaps layer\n",
    "        return: a set of normalized, capsule output vectors\n",
    "        \"\"\"\n",
    "\n",
    "        # adding batch_size dims and stacking all u vectors\n",
    "        u = u[None, :, :, None, :]\n",
    "        # 4D weight matrix\n",
    "        W = self.W[:, None, :, :, :]\n",
    "\n",
    "        # calculating u_hat = W*u\n",
    "        u_hat = torch.matmul(u, W)\n",
    "\n",
    "        # getting the correct size of b_ij\n",
    "        # setting them all to 0, initially\n",
    "        b_ij = torch.zeros(*u_hat.size(), device=u.device)\n",
    "\n",
    "        # update coupling coefficients and calculate v_j\n",
    "        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=1)\n",
    "\n",
    "        return v_j.transpose(0, 1).squeeze((2, 3))  # return final vector outputs\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        \"\"\"Squashes an input Tensor so it has a magnitude between 0-1.\n",
    "        param input_tensor: a stack of capsule inputs, s_j\n",
    "        return: a stack of normalized, capsule output vectors, v_j\n",
    "        \"\"\"\n",
    "        # same squash function as before\n",
    "        squared_norm = (input_tensor**2).sum(dim=-1, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm)  # normalization coeff\n",
    "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "372acce5-dbc5-448b-a196-478b0ba53964",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T07:51:50.890897Z",
     "iopub.status.busy": "2024-08-03T07:51:50.890592Z",
     "iopub.status.idle": "2024-08-03T07:51:50.989967Z",
     "shell.execute_reply": "2024-08-03T07:51:50.989044Z",
     "shell.execute_reply.started": "2024-08-03T07:51:50.890880Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = DigitCaps(in_channels=32, num_capsules=1, out_channels=512, previous_layer_nodes=512).cuda()\n",
    "x = torch.randn(2, 512, 32).cuda()\n",
    "d(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0cb8878c-fb21-434c-93b6-957d9732b76a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-04T02:25:59.659639Z",
     "iopub.status.busy": "2024-08-04T02:25:59.658953Z",
     "iopub.status.idle": "2024-08-04T02:25:59.867357Z",
     "shell.execute_reply": "2024-08-04T02:25:59.866311Z",
     "shell.execute_reply.started": "2024-08-04T02:25:59.659578Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-student",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 512])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = RoutingCapsules(in_caps=32, in_dim=512, num_caps=2, num_routing=3, dim_caps=512)\n",
    "\n",
    "x = torch.randn(2, 32, 512)\n",
    "d(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "375381ff-a9b6-4c70-8140-904a7c4564a5",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T12:38:52.626292Z",
     "iopub.status.busy": "2024-08-03T12:38:52.625640Z",
     "iopub.status.idle": "2024-08-03T12:38:52.696751Z",
     "shell.execute_reply": "2024-08-03T12:38:52.694903Z",
     "shell.execute_reply.started": "2024-08-03T12:38:52.626223Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, stride=2, bias=True, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, stride=2, bias=True, padding=0),\n",
    "        )\n",
    "\n",
    "        num_capsules = 10\n",
    "\n",
    "        self.primary1 = PrimaryCaps(\n",
    "            kernel_size=3, in_channels=512, out_channels=512, stride=1, num_capsules=num_capsules\n",
    "        )\n",
    "        # self.digits1 = DigitCaps(in_channels=num_capsules, num_capsules=2, out_channels=512, previous_layer_nodes=512)\n",
    "        self.digits1 = RoutingCapsules(in_caps=num_capsules, in_dim=512, num_caps=2, num_routing=3, dim_caps=512)\n",
    "\n",
    "        self.primary2 = PrimaryCaps(\n",
    "            kernel_size=3, in_channels=512, out_channels=512, stride=1, num_capsules=num_capsules\n",
    "        )\n",
    "        # self.digits2 = DigitCaps(in_channels=num_capsules, num_capsules=2, out_channels=512, previous_layer_nodes=512)\n",
    "        self.digits2 = RoutingCapsules(in_caps=num_capsules, in_dim=512, num_caps=2, num_routing=3, dim_caps=512)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x:torch.Tensor: (B, 512, 7, 7)\n",
    "\n",
    "        Returns:\n",
    "            a tensor: (B, 512, 7, 7)\n",
    "        \"\"\"\n",
    "        if x.ndim == 4:\n",
    "            x = self.conv_layer(x).squeeze(-1).squeeze(-1)  # (B, 512)\n",
    "        x = self.primary1(x)  # (B, 512, num_capsules)\n",
    "        # x = self.digits1(x)  # (B, num_capsules, 512)\n",
    "        x = self.digits1(x.transpose(1, 2))  # (B, num_capsules, 512)\n",
    "\n",
    "        # return x.transpose(1,2 )\n",
    "\n",
    "        x = x.mean(1)  # (B, num_capsules, 512)-> (B, 512)\n",
    "        x = self.primary2(x)  # (B, 512, num_capsules)\n",
    "        x = self.digits2(x.transpose(1, 2))  # (B, 2, 512)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f7eb26d7-0f35-4b06-859c-1cf33e58ef53",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T12:38:54.314976Z",
     "iopub.status.busy": "2024-08-03T12:38:54.314446Z",
     "iopub.status.idle": "2024-08-03T12:38:54.559337Z",
     "shell.execute_reply": "2024-08-03T12:38:54.558646Z",
     "shell.execute_reply.started": "2024-08-03T12:38:54.314924Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 512])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 512)\n",
    "m1 = CN()\n",
    "m1(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82969c-9a44-4b3d-9dc3-bc8ba6cbce0f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1387f5b9-1698-4ec4-9a30-fe97cf25d84e",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T11:23:25.169068Z",
     "iopub.status.busy": "2024-08-03T11:23:25.168757Z",
     "iopub.status.idle": "2024-08-03T11:23:25.203463Z",
     "shell.execute_reply": "2024-08-03T11:23:25.201713Z",
     "shell.execute_reply.started": "2024-08-03T11:23:25.169042Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ABCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mel = Melspec()\n",
    "        self.model = VGG18()\n",
    "        self.attention = Attention_to_1D()\n",
    "        self.capsule = CN()\n",
    "\n",
    "        self.cls_head = nn.Linear(512, 1)\n",
    "\n",
    "    def extract_2D_feature(self, x):\n",
    "        x = self.mel(x)  # (B, 3, 224, 224)\n",
    "        x = self.model(x)  # (B, 512, 7, 7)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        capsule_output = None\n",
    "\n",
    "        x = self.extract_2D_feature(x)\n",
    "        x = self.attention(x)  # (B, 512)\n",
    "\n",
    "        # logit = self.cls_head(x)\n",
    "\n",
    "        capsule_output = self.capsule(x)  # (B, 2, 512)\n",
    "        # logit = capsule_output.mean(-1)\n",
    "        # logit = (capsule_output**2).sum(dim=-1) ** 0.5\n",
    "\n",
    "        logit = self.cls_head(capsule_output).squeeze(-1)\n",
    "        \n",
    "        return logit, capsule_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "67e733cc-acf0-46d2-83eb-03b7ca84aa2c",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T11:23:25.976200Z",
     "iopub.status.busy": "2024-08-03T11:23:25.975782Z",
     "iopub.status.idle": "2024-08-03T11:23:29.094247Z",
     "shell.execute_reply": "2024-08-03T11:23:29.093719Z",
     "shell.execute_reply.started": "2024-08-03T11:23:25.976158Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (224) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.7951e-08,  1.2886e-07],\n",
       "         [-2.9169e-07,  2.0479e-07]], grad_fn=<MeanBackward1>),\n",
       " tensor([[[-9.6815e-06, -4.2319e-05,  1.2162e-05,  ..., -3.2966e-05,\n",
       "            5.9100e-07,  3.3521e-06],\n",
       "          [ 1.7074e-05,  8.9024e-07,  5.0483e-06,  ...,  9.4862e-06,\n",
       "           -3.5924e-05, -1.5700e-05]],\n",
       " \n",
       "         [[-1.4014e-05, -4.4551e-05,  6.8642e-06,  ..., -3.3662e-05,\n",
       "           -8.0840e-07,  1.4957e-06],\n",
       "          [ 1.8795e-05,  5.2550e-06,  2.6086e-06,  ...,  1.3798e-05,\n",
       "           -4.0248e-05, -9.9059e-06]]], grad_fn=<SqueezeBackward2>))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 1, 48000)\n",
    "model = ABCNet()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bf1a5de-efc3-401b-8cb8-5f9cfed1f8b0",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T07:53:16.178236Z",
     "iopub.status.busy": "2024-08-03T07:53:16.177677Z",
     "iopub.status.idle": "2024-08-03T07:53:17.643166Z",
     "shell.execute_reply": "2024-08-03T07:53:17.642300Z",
     "shell.execute_reply.started": "2024-08-03T07:53:16.178210Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000],\n",
       "        [0.5000, 0.5000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 1, 48000).cuda()\n",
    "model = ABCNet().cuda()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a715a81-fcbb-40aa-8a58-916ca5581f72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03812844-9563-4438-b0a5-7d554c50eda5",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T08:46:41.744731Z",
     "iopub.status.busy": "2024-08-03T08:46:41.743359Z",
     "iopub.status.idle": "2024-08-03T08:46:41.802936Z",
     "shell.execute_reply": "2024-08-03T08:46:41.801536Z",
     "shell.execute_reply.started": "2024-08-03T08:46:41.744662Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CapsuleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructs a CapsuleLoss module.\"\"\"\n",
    "        super(CapsuleLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"Defines how the loss compares inputs.\n",
    "        param x: digit capsule outputs\n",
    "        param labels:\n",
    "        param images: the original MNIST image input data\n",
    "        param reconstructions: reconstructed MNIST image data\n",
    "        return: weighted margin and reconstruction loss, averaged over a batch\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        ##  calculate the margin loss   ##\n",
    "\n",
    "        # get magnitude of digit capsule vectors, v_c\n",
    "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        # calculate \"correct\" and incorrect loss\n",
    "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
    "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
    "\n",
    "        # sum the losses, with a lambda = 0.5\n",
    "        # print(labels.shape, left.shape)\n",
    "        margin_loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        margin_loss = margin_loss.mean()\n",
    "        return margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98e72b03-eda1-4ea7-8b90-21ca9bfdc54c",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T08:47:55.097879Z",
     "iopub.status.busy": "2024-08-03T08:47:55.097382Z",
     "iopub.status.idle": "2024-08-03T08:47:55.138532Z",
     "shell.execute_reply": "2024-08-03T08:47:55.137712Z",
     "shell.execute_reply.started": "2024-08-03T08:47:55.097841Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1]) torch.Size([32, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11.2212)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = CapsuleLoss()\n",
    "x = torch.randn(32, 2, 512)\n",
    "labels = torch.randint(0, 1, (32, 1))\n",
    "loss(x, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459d48c-674e-41c6-bff9-f300bba15c55",
   "metadata": {},
   "source": [
    "# Lit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "76ed8492-60fd-4c23-b35f-d66786b97a36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T14:04:10.058960Z",
     "iopub.status.busy": "2024-07-31T14:04:10.058424Z",
     "iopub.status.idle": "2024-07-31T14:04:10.913301Z",
     "shell.execute_reply": "2024-07-31T14:04:10.911582Z",
     "shell.execute_reply.started": "2024-07-31T14:04:10.058912Z"
    }
   },
   "outputs": [],
   "source": [
    "from ay2.torch.deepfake_detection import DeepfakeAudioClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086832c-1cff-40e7-837f-1af2d0a3b5b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ABCNet_lit(DeepfakeAudioClassification):\n",
    "    def __init__(self, cfg=None, args=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = ABCNet()\n",
    "        self.configure_loss_fn()\n",
    "\n",
    "        if args is not None and hasattr(args, \"profiler\"):\n",
    "            self.profiler = args.profiler\n",
    "        else:\n",
    "            self.profiler = None\n",
    "\n",
    "        self.lr = 1e-4\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_loss_fn(self):\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.cap_loss = CapsuleLoss()\n",
    "\n",
    "    def calcuate_loss(self, batch_res, batch):\n",
    "        label = batch[\"label\"]\n",
    "        batch_size = len(label)\n",
    "        ce_loss = self.ce_loss(batch_res[\"ce_logit\"], label.long())\n",
    "        cls_loss = self.bce_loss(batch_res[\"logit\"], label.type(torch.float32))\n",
    "        cap_loss = (\n",
    "            self.cap_loss(batch_res[\"capsule_output\"], label[:, None]) if batch_res[\"capsule_output\"] is not None else 0\n",
    "        )\n",
    "        loss = 1.0 * ce_loss + 0 * cls_loss + 1.0 * cap_loss\n",
    "        # loss = cap_loss\n",
    "        return {\"loss\": loss, \"ce_loss\": ce_loss, \"cls_loss\": cls_loss, \"cap_loss\": cap_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.num_training_batches = self.trainer.num_training_batches\n",
    "        return [optimizer]\n",
    "\n",
    "    def _shared_pred(self, batch, batch_idx, stage=\"train\"):\n",
    "        audio, sample_rate = batch[\"audio\"], batch[\"sample_rate\"]\n",
    "\n",
    "        B = len(audio)\n",
    "        logit, capsule_output = self.model(audio)\n",
    "        binary_logit = F.softmax(logit, dim=-1)[:, 1]\n",
    "\n",
    "        return {\"ce_logit\": logit, \"logit\": binary_logit, \"capsule_output\": capsule_output}\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx, stage=\"train\", dataloader_idx=0, *args, **kwargs):\n",
    "        try:\n",
    "            batch_res = self._shared_pred(batch, batch_idx, stage=stage)\n",
    "        except TypeError:\n",
    "            batch_res = self._shared_pred(batch, batch_idx)\n",
    "\n",
    "        label = batch[\"label\"]\n",
    "        loss = self.calcuate_loss(batch_res, batch)\n",
    "\n",
    "        if not isinstance(loss, dict):\n",
    "            loss = {\"loss\": loss}\n",
    "\n",
    "        suffix = \"\" if dataloader_idx == 0 else f\"-dl{dataloader_idx}\"\n",
    "        self.log_dict(\n",
    "            {f\"{stage}-{key}{suffix}\": loss[key] for key in loss},\n",
    "            on_step=True if stage == \"train\" else False,\n",
    "            # on_step=False,\n",
    "            on_epoch=True,\n",
    "            logger=True,\n",
    "            prog_bar=True,\n",
    "            add_dataloader_idx=False,\n",
    "            batch_size=batch[\"label\"].shape[0],\n",
    "        )\n",
    "        batch_res.update(loss)\n",
    "        return batch_res"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
