{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18ad77-3c01-45f4-ab20-96459b6aec6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6675238-daf2-4b82-bf7f-34f7e2bba751",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import torchyin\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3366f-3b1b-44c9-9803-f0641167b489",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Model Componenets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0671adf-39c4-4c75-b070-621ff53c3382",
   "metadata": {},
   "source": [
    "## Preprocess　"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb26f4-53f4-46a1-bafb-2df016b1b577",
   "metadata": {},
   "source": [
    "> Furthermore, each input audio file was processed by a standard pre-emphasis filter with coefficient 0.97 to emphasize the mid-high frequencies, and we imposed a window length of 32 ms and a hop size of 16 ms to extract the log spectrograms. The final input spectrograms were fixed to have size of $L=128$ frames by $M=256$ frequency bins, corresponding to 2.064 seconds of content. The patches were created by applying a $P_L=16 \\times P_M=16$ grid, corresponding to a sequence with length $N=128$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399478c-e951-4c1d-b6ab-fcdd3e86330b",
   "metadata": {},
   "source": [
    "There are three operations in preprocess:\n",
    "1. use PreEmphasis to filter the input audio\n",
    "2. use spectrogram to transform the audio into spectrogram\n",
    "3. crop the spectrogram into patches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb6e0a8-43fc-47e8-ab18-5635439d8666",
   "metadata": {},
   "source": [
    "### Pre-Emphasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2548be-ad24-4625-a87e-4e9562ac7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreEmphasis(torch.nn.Module):\n",
    "    def __init__(self, coef: float = 0.97) -> None:\n",
    "        super().__init__()\n",
    "        self.coef = coef\n",
    "        # make kernel\n",
    "        # In pytorch, the convolution operation uses cross-correlation. So, filter is flipped.\n",
    "        self.register_buffer(\n",
    "            \"flipped_filter\",\n",
    "            torch.FloatTensor([-self.coef, 1.0]).unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        assert x.ndim in [2, 3]\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        # reflect padding to match lengths of in/out\n",
    "        x = F.pad(x, (1, 0), \"reflect\")\n",
    "        return F.conv1d(x, self.flipped_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ae353-7aa7-4134-ab20-cb529c6c952f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1, 5)\n",
    "model = PreEmphasis()\n",
    "model(x), x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a0302-fdc6-4ae7-b64f-8b14c4946d59",
   "metadata": {},
   "source": [
    "### Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ad389-2610-41ff-a2a0-57bd817a35fb",
   "metadata": {},
   "source": [
    "> We imposed a window length of 32 ms and a hop size of 16 ms to extract the log spectrograms. The final input spectrograms were fixed to have size of $L=128$ frames by $M=256$ frequency bins, corresponding to 2.064 seconds of content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e7e5d-0dca-4564-ab5e-f9b1e6e3d497",
   "metadata": {},
   "source": [
    "For a audio with 16000 HZ:\n",
    "- 32ms windows length: win_length = 16 * 32 = 512\n",
    "- 16ms hop size: hop_length = 16 * 16 = 256\n",
    "- 256 frequency bins: n_fft = 511, but 0 < win_length <= n_fft, so, I set win_length to 511\n",
    "- For the frames, the author use 2.064s audio to generate 128 frames. About 64 frames per second. Therefore, I further adjust hop_length to 250, to generate 64 frames per second.\n",
    "\n",
    "The final setting is:\n",
    "```python\n",
    "stft = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=511,\n",
    "    win_length=511,\n",
    "    hop_length=250,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "026f1c1c-1374-4770-9413-23fb825b8cb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-30T02:56:34.880083Z",
     "iopub.status.busy": "2024-09-30T02:56:34.879027Z",
     "iopub.status.idle": "2024-09-30T02:56:34.920266Z",
     "shell.execute_reply": "2024-09-30T02:56:34.919500Z",
     "shell.execute_reply.started": "2024-09-30T02:56:34.880039Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSpectrogram(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.stft = torchaudio.transforms.Spectrogram(\n",
    "            n_fft=511,\n",
    "            win_length=511,\n",
    "            hop_length=250,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stft(x)\n",
    "        x = torch.log(x + 1e-7)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33f44962-3711-42db-821c-70ee07660709",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-30T02:56:36.025751Z",
     "iopub.status.busy": "2024-09-30T02:56:36.025198Z",
     "iopub.status.idle": "2024-09-30T02:56:36.059023Z",
     "shell.execute_reply": "2024-09-30T02:56:36.057880Z",
     "shell.execute_reply.started": "2024-09-30T02:56:36.025713Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 256, 192])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 1, 16000 * 3)\n",
    "t = CustomSpectrogram()\n",
    "t(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837237c5-eeab-4e5d-b20a-6c6633a7412b",
   "metadata": {},
   "source": [
    "### Crop patches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7959406-4ce6-4dd0-a679-028cb1f42821",
   "metadata": {},
   "source": [
    "> Let us denote with $X \\in \\mathbb{R}^{L \\times M}$ the log-spectrogram of an input recording $x(t)$, with $L$ denoting the number of frames and $M$ the number of frequency bins of $X$.\n",
    "\n",
    "\n",
    "\n",
    "> In a first step, the input log-spectrogram $X$ is split into a sequence of non-overlapping 2D patches $x_p \\in \\mathbb{R}^{P_L \\times P_M}$. Each patch has a fixed amount of rows $\\left(P_L\\right)$ and of columns $\\left(P_M\\right)$, and the total amount of patches $N$ is fixed and equal to $(L \\cdot M) /\\left(P_L \\cdot P_M\\right)$\n",
    "> $$\n",
    "> x_p=\\operatorname{reshape}\\left(X, P_L, P_M\\right)\n",
    "> $$\n",
    "> with $p \\in 1 \\ldots N$.\n",
    "\n",
    "> The patches were created by applying a $P_L=16 \\times P_M=16$ grid, corresponding to a sequence with length $N=128$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ce6a9-2da8-475d-886c-525e5e43a9e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T06:44:02.176643Z",
     "iopub.status.busy": "2024-09-02T06:44:02.174784Z",
     "iopub.status.idle": "2024-09-02T06:44:02.187847Z",
     "shell.execute_reply": "2024-09-02T06:44:02.185576Z",
     "shell.execute_reply.started": "2024-09-02T06:44:02.176561Z"
    }
   },
   "source": [
    "The patch size is $16 \\times 16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373b9cd-010b-4045-9481-35003de57ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropPatch(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, \"B 1 (m pm) (l pl)-> B (m l) (pm pl)\", pm=16, pl=16)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e7e7d-aa9b-46bf-8efd-154eb13b0eda",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.randn([2, 1, 256, 192])\n",
    "t = CropPatch()\n",
    "print(t(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83b7c3-6f0a-4e2e-9207-528d28bdc68b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Combine above three steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7506a-cd86-4311-8cc4-68e2f47f8ef4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Preprocess(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.pre_emphasis = PreEmphasis()\n",
    "        self.spec_transform = CustomSpectrogram()\n",
    "        self.crop_patch = CropPatch()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, 1, L), L = 48000\n",
    "        \"\"\"\n",
    "        x = self.pre_emphasis(x)  # (B, 1, L)\n",
    "        spec = self.spec_transform(x)  # (B, 1, H, W)\n",
    "        patch = self.crop_patch(spec)  # (B, HW/256, 16*16)\n",
    "        return x, spec, patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c7cb5-7810-453a-9009-6d50788dc32f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "module = Preprocess()\n",
    "x = torch.randn(3, 1, 48000)\n",
    "x, spec, patch = module(x)\n",
    "print(patch.shape, spec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7848eb-4184-4e5d-9d67-8102693953d6",
   "metadata": {},
   "source": [
    "## Encoder　& Decoders　"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff897ad-b525-4666-a851-df79d7693077",
   "metadata": {},
   "source": [
    "| Component | Global Params |  | MLP Blocks |  | Self-Attention Blocks |  |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "|  | Depth | Embedding Size | Dimensions | Dropout | Number of Heads | Head Dimension |\n",
    "| $E$ - Spectrogram Encoder | 8 | 1024 | 2048 | 0 | 8 | 64 |\n",
    "| $D_{\\text {dec }}^X$ - Spectrogram Decoder | 6 | 512 | 1024 | 0 | 8 | 64 |\n",
    "| $D_{\\text {dec }}^{f_0}$ - F0 Decoder | 4 | 512 | 1024 | 0 | 8 | 64 |\n",
    "| $P-$ Synthesis Predictor | 4 | 512 | 1024 | 0.1 | 6 | 64 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9375d2f3-0048-4f86-97ad-3f35d89afd17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from vit_pytorch import ViT\n",
    "except ImportError:\n",
    "    from .vit_pytorch import ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a51730-1f69-43a6-ad5c-f091c9f020b0",
   "metadata": {},
   "source": [
    "### help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ee71b-db44-4fe6-8a0b-3065aad6da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer(dim=1024, depth=8, heads=8, mlp_dim=2048, dropout=0.1):\n",
    "    v = ViT(\n",
    "        image_size=256,\n",
    "        patch_size=16,\n",
    "        num_classes=1000,\n",
    "        dim=dim,\n",
    "        depth=depth,\n",
    "        heads=heads,\n",
    "        mlp_dim=mlp_dim,\n",
    "        dropout=dropout,\n",
    "        emb_dropout=0.1,\n",
    "    )\n",
    "    return v.transformer\n",
    "\n",
    "\n",
    "def get_pos_embedding(num_patches=192, dim=1024):\n",
    "    pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "    return pos_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86deda-0adf-48e3-bffd-eaf7c185e98a",
   "metadata": {},
   "source": [
    "### Encoder　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f1d3a-cd2a-4759-90be-f38008d9ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        dim = 1024\n",
    "\n",
    "        ### the original patch is 16x16, thus we need project its dim from 256 into dim\n",
    "        self.mlp = nn.Linear(256, dim)\n",
    "\n",
    "        self.pos_embedding = get_pos_embedding(dim=dim, num_patches=192)\n",
    "        self.transformer = get_transformer(dim=1024, depth=8, heads=8, mlp_dim=2048, dropout=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        b, n, _ = x.shape\n",
    "        x += self.pos_embedding[:, :n, :]  # (b, n, dim) + (1, n, dim)\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c7938-3577-4c38-b2d8-ea93456c0374",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "x = torch.randn(2, 192, 256)\n",
    "encoder(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98640e87-d33e-40d3-a31b-b6e9a05d003b",
   "metadata": {},
   "source": [
    "### SpectrogramDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a5840-1635-4203-a2e3-d18ccb76d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDecoder(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        dim = 512\n",
    "\n",
    "        ### the dim of encoder output is 1024, thus we need project its dim from 1024 into dim\n",
    "        self.mlp = nn.Linear(1024, dim)\n",
    "\n",
    "        self.pos_embedding = get_pos_embedding(dim=dim, num_patches=192)\n",
    "        self.transformer = get_transformer(dim=dim, depth=6, heads=8, mlp_dim=1024, dropout=0)\n",
    "\n",
    "        self.mlp2 = nn.Linear(dim, 16 * 16)\n",
    "\n",
    "    def forward(self, x, num_bins, num_frames, patch_size=16):\n",
    "        x = self.mlp(x)\n",
    "        b, n, _ = x.shape\n",
    "        x += self.pos_embedding[:, :n, :]  # (b, n, dim) + (1, n, dim)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        ## convert embedding into spectrogram\n",
    "        x = self.mlp2(x)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"B (m l) (pm pl) -> B 1 (m pm) (l pl)\",\n",
    "            pm=patch_size,\n",
    "            pl=patch_size,\n",
    "            m=num_bins // patch_size,\n",
    "            l=num_frames // patch_size,\n",
    "        )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432a950-1f7a-46ed-9681-9c0b945ffdab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-solution"
    ]
   },
   "outputs": [],
   "source": [
    "m = SpectrogramDecoder()\n",
    "x = torch.randn(2, 192, 1024)\n",
    "y = m(x, 256, 192)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b981287-2aa3-46c8-a171-d44796eee352",
   "metadata": {},
   "source": [
    "### FundamentalFrequencyDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba831e6-8f89-4973-95d4-c5363832bded",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Extract F0　"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e447f253-a379-4ebf-9b72-0a1c3c99d036",
   "metadata": {},
   "source": [
    "Let us denote with $F_0 \\in \\mathbb{R}^{L \\times M}$ the contour matrix of the $f_0$ trajectory, i.e. a matrix with the same dimension as the input spectrogram, in which\n",
    "\n",
    "$$\n",
    "F_0(l, m)= \\begin{cases}1 & \\text { if } f_0(m) \\approx \\frac{f_s}{2 M} \\cdot m \\\\ 0 & \\text { otherwise }\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75d29cbf-1d64-4610-9930-023d7752c683",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-30T02:31:05.322518Z",
     "iopub.status.busy": "2024-09-30T02:31:05.321437Z",
     "iopub.status.idle": "2024-09-30T02:31:05.358869Z",
     "shell.execute_reply": "2024-09-30T02:31:05.358138Z",
     "shell.execute_reply.started": "2024-09-30T02:31:05.322463Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_f0(x):\n",
    "    \"\"\"\n",
    "    Assume that the input audio x is with shape (B, 1, 48000). If its length is not equal to 48000,\n",
    "    you may have to change th frame stride (second).\n",
    "    \"\"\"\n",
    "    pitch = torchyin.estimate(\n",
    "        x[:, 0, :],\n",
    "        sample_rate=16000,\n",
    "        pitch_min=20,\n",
    "        pitch_max=9000,\n",
    "        frame_stride=0.01513,  # actually is 0.015625\n",
    "    )\n",
    "    return pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0746a80-8a31-4ef4-bbce-b811c40b756f",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-30T02:31:09.279663Z",
     "iopub.status.busy": "2024-09-30T02:31:09.278581Z",
     "iopub.status.idle": "2024-09-30T02:31:09.346382Z",
     "shell.execute_reply": "2024-09-30T02:31:09.345786Z",
     "shell.execute_reply.started": "2024-09-30T02:31:09.279609Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 192])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(8, 1, 48000)\n",
    "get_f0(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13296b-6b2f-4646-a690-b15ab50dc51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_f0_matrix(pitch, spec, eps=1):\n",
    "    res = torch.zeros_like(spec[:, 0, :, :])\n",
    "    B, _, num_bins, num_frames = spec.shape\n",
    "    for batch in range(B):\n",
    "        for i in range(num_bins):\n",
    "            for j in range(num_frames):\n",
    "                base = 16000 / 512 * (i + 1)\n",
    "                if pitch[batch, j] > base - eps and pitch[batch, j] < base + eps:\n",
    "                    res[batch, i, j] = 1\n",
    "                else:\n",
    "                    res[batch, i, j] = 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_f0_matrix2(pitch, spec, eps=1):\n",
    "    base_frequencies = (16000 / 512) * (torch.arange(256, device=pitch.device) + 1)\n",
    "    base_frequencies = base_frequencies.view(1, -1, 1)  # Shape: (1, 256, 1)\n",
    "    pitch_expanded = pitch.unsqueeze(1)  # Shape: (8, 1, 192)\n",
    "    condition = (pitch_expanded > base_frequencies - eps) & (pitch_expanded < base_frequencies + eps)\n",
    "    res2 = condition.float()\n",
    "    return res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14971185-273a-4349-85e2-a10dbbdfbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, sr = torchaudio.load(\"/home/ay/LibriSeVoc/melgan/103_1241_000004_000002_gen.wav\")\n",
    "# x = x[:, None, 48000 : 48000 * 2]\n",
    "# t = CustomSpectrogram()\n",
    "# spec = t(x)\n",
    "# pitch = get_f0(x)\n",
    "# res1 = get_f0_matrix(pitch, spec)\n",
    "\n",
    "# res2 = get_f0_matrix2(pitch, spec)\n",
    "# torch.abs(res1 - res2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70fc2d-19d4-49d6-97be-eda32c81621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianConv2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GaussianConv2d, self).__init__()\n",
    "\n",
    "        # Define the Gaussian Kernel\n",
    "        gaussian_kernel = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\n",
    "        gaussian_kernel /= gaussian_kernel.sum()  # Normalize the kernel\n",
    "\n",
    "        # Convert to 4D tensor required by nn.Conv2d (out_channels, in_channels, height, width)\n",
    "        gaussian_kernel = gaussian_kernel.view(1, 1, 3, 3)  # (1, 1, 3, 3)\n",
    "\n",
    "        # Initialize the Conv2d layer\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        # Set the kernel weight to the Gaussian kernel\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight = nn.Parameter(gaussian_kernel, requires_grad=False)\n",
    "\n",
    "        # Freeze the weights\n",
    "        self.conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50efbd15-5967-4c0b-877a-0f4bf2ba449a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class F0ReconstructionLoss(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = GaussianConv2d()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def extract_f0(self, x):\n",
    "        pitch = get_f0(x)\n",
    "        res = get_f0_matrix2(pitch, spec=None)  # (B, num_bins, num_frames)\n",
    "        res = res[:, None, :, :]\n",
    "        res = self.conv(res)\n",
    "        return res\n",
    "\n",
    "    def forward(self, x, f0):\n",
    "        return self.compute_loss(x, f0)\n",
    "\n",
    "    def compute_loss(self, x, f0):\n",
    "        f0_target = self.extract_f0(x)\n",
    "        loss = self.criterion(f0, f0_target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d629772-defe-439c-b24d-15e299aef82b",
   "metadata": {},
   "source": [
    "#### build Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e509842-780f-4b41-942a-9911af291776",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FundamentalFrequencyDecoder(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        dim = 512\n",
    "\n",
    "        ### the dim of encoder output is 1024, thus we need project its dim from 1024 into dim\n",
    "        self.mlp = nn.Linear(1024, dim)\n",
    "\n",
    "        self.pos_embedding = get_pos_embedding(dim=dim, num_patches=192)\n",
    "        self.transformer = get_transformer(dim=dim, depth=4, heads=8, mlp_dim=1024, dropout=0)\n",
    "\n",
    "        self.mlp2 = nn.Linear(dim, 16 * 16)\n",
    "\n",
    "    def forward(self, x, num_bins, num_frames, patch_size=16):\n",
    "        x = self.mlp(x)\n",
    "        b, n, _ = x.shape\n",
    "        x += self.pos_embedding[:, :n, :]  # (b, n, dim) + (1, n, dim)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        ## convert embedding into spectrogram\n",
    "        x = self.mlp2(x)\n",
    "        x = rearrange(\n",
    "            x,\n",
    "            \"B (m l) (pm pl) -> B 1 (m pm) (l pl)\",\n",
    "            pm=patch_size,\n",
    "            pl=patch_size,\n",
    "            m=num_bins // patch_size,\n",
    "            l=num_frames // patch_size,\n",
    "        )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e31639-b853-4732-abcf-0c7c7483a93d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "m = FundamentalFrequencyDecoder()\n",
    "x = torch.randn(2, 192, 1024)\n",
    "y = m(x, 256, 192)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315a72a-6427-4a44-b74a-403f5682feb8",
   "metadata": {},
   "source": [
    "### Cls Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88162890-a69a-46b8-8a9a-f0bf740eb11c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClsDecoder(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        dim = 512\n",
    "\n",
    "        ### the dim of encoder output is 1024, thus we need project its dim from 1024 into dim\n",
    "        self.mlp = nn.Linear(1024, dim)\n",
    "\n",
    "        self.pos_embedding = get_pos_embedding(dim=dim, num_patches=192)\n",
    "        self.transformer = get_transformer(dim=dim, depth=4, heads=6, mlp_dim=1024, dropout=0.1)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.cls_head = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        x = self.mlp(x)\n",
    "        cls_tokens = repeat(self.cls_token, \"1 1 d -> b 1 d\", b=b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        x += self.pos_embedding[:, : n + 1, :]  # (b, n, dim) + (1, n, dim)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        ## classification\n",
    "        feat = x[:, 0, :]\n",
    "        x = self.cls_head(feat)\n",
    "        return x, feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f84a61-c775-4ceb-b561-25121f6656cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "m = ClsDecoder()\n",
    "x = torch.randn(2, 192, 1024)\n",
    "y, feat = m(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a19c92-48f8-479a-b381-61d9212d6646",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe09e2-4c9c-4188-91ef-73b588059210",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SFATNet(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.preprocess = Preprocess()\n",
    "        self.encoder = Encoder()\n",
    "        self.spec_decoder = SpectrogramDecoder()\n",
    "        self.f0_decoder = FundamentalFrequencyDecoder()\n",
    "        self.cls_decoder = ClsDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_new, spec, patch = self.preprocess(x)\n",
    "\n",
    "        feat = self.encoder(patch)\n",
    "\n",
    "        num_bins, num_frames, patch_size = spec.shape[-2], spec.shape[-1], 16\n",
    "        \n",
    "        pred_spec = self.spec_decoder(feat, num_bins, num_frames, patch_size)\n",
    "        pred_f0 = self.f0_decoder(feat, num_bins, num_frames, patch_size)\n",
    "        pred_logit, feature = self.cls_decoder(feat)\n",
    "\n",
    "        return {\n",
    "            \"emphasis_x\": x,\n",
    "            \"spec\": spec,\n",
    "            \"pred_spec\": pred_spec,\n",
    "            \"pred_f0\": pred_f0,\n",
    "            \"logit\": pred_logit.squeeze(-1),\n",
    "            \"feature\" : feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956519b-ceac-49da-b3c9-3e0e860ffe5f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3, 1, 48000)\n",
    "m = SFATNet()\n",
    "m(x)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
