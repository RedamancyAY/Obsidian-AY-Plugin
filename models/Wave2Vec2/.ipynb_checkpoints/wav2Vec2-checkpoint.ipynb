{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caf012ad-3972-4f7c-912b-03394bda3388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T08:32:40.106354Z",
     "iopub.status.busy": "2024-09-18T08:32:40.105040Z",
     "iopub.status.idle": "2024-09-18T08:32:40.146214Z",
     "shell.execute_reply": "2024-09-18T08:32:40.144897Z",
     "shell.execute_reply.started": "2024-09-18T08:32:40.106290Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882cdde4-14f0-4ed7-8a56-ded709a8784c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T08:32:40.665642Z",
     "iopub.status.busy": "2024-09-18T08:32:40.664139Z",
     "iopub.status.idle": "2024-09-18T08:32:42.703259Z",
     "shell.execute_reply": "2024-09-18T08:32:42.702627Z",
     "shell.execute_reply.started": "2024-09-18T08:32:40.665577Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b459dc3-b5ef-4a27-a3ba-16063a60d954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T08:32:42.704465Z",
     "iopub.status.busy": "2024-09-18T08:32:42.704246Z",
     "iopub.status.idle": "2024-09-18T08:32:42.724954Z",
     "shell.execute_reply": "2024-09-18T08:32:42.724487Z",
     "shell.execute_reply.started": "2024-09-18T08:32:42.704447Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseLine(nn.Module):\n",
    "    def __init__(self, pretrain_feat=\"extract_features\", backend=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        assert pretrain_feat in [\"last_hidden_state\", \"extract_features\"]\n",
    "        self.pretrain_feat = pretrain_feat\n",
    "        # The channels of used features for the pretrained model is 512 when using\n",
    "        # the 'extract_features',  but 768 when [\"last_hidden_state\"] is used.\n",
    "        C_features = 512 if pretrain_feat == \"extract_features\" else 768\n",
    "\n",
    "        self.pretrain_model = Wav2Vec2Model.from_pretrained(\n",
    "            \"/usr/local/ay_data/0-model_weights/models--facebook--wav2vec2-base-960h\"\n",
    "        )\n",
    "\n",
    "        self.backend = backend\n",
    "        if backend == \"resnet\":\n",
    "            self.backend_model = ResNet50(in_channels=C_features, classes=1)\n",
    "        elif backend == \"linear\":\n",
    "            self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "            self.backend_model = nn.Linear(C_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = self.pretrain_model(x)[self.pretrain_feat]\n",
    "        feature = torch.transpose(feature, 1, 2)\n",
    "        if self.backend == \"linear\":\n",
    "            feature = torch.squeeze(self.pooler(feature), -1)\n",
    "        # print(feature.shape, self.pooler(feature).shape)\n",
    "        outputs = self.backend_model(feature)\n",
    "        return outputs\n",
    "\n",
    "    def extract_feature(self, x):\n",
    "        # print(x.shape, self.pretrain_feat)\n",
    "        feature = self.pretrain_model(x)[self.pretrain_feat]\n",
    "        feature = torch.transpose(feature, 1, 2)\n",
    "        if self.backend == \"linear\":\n",
    "            feature = torch.squeeze(self.pooler(feature), -1)\n",
    "        return feature\n",
    "\n",
    "    def make_prediction(self, feature):\n",
    "        # print(feature.shape, self.pooler(feature).shape)\n",
    "        outputs = self.backend_model(feature)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4552a42-6ed4-40dc-8d6d-3d9d1197df44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T08:32:42.725608Z",
     "iopub.status.busy": "2024-09-18T08:32:42.725467Z",
     "iopub.status.idle": "2024-09-18T08:32:43.700690Z",
     "shell.execute_reply": "2024-09-18T08:32:43.699547Z",
     "shell.execute_reply.started": "2024-09-18T08:32:42.725592Z"
    },
    "scrolled": true,
    "tags": [
     "active-ipynb",
     "style-student"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at /usr/local/ay_data/0-model_weights/models--facebook--wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3212],\n",
       "        [0.3307],\n",
       "        [0.3266],\n",
       "        [0.3083],\n",
       "        [0.3145],\n",
       "        [0.3171],\n",
       "        [0.3300],\n",
       "        [0.3246],\n",
       "        [0.3117],\n",
       "        [0.3167]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10, 69000)\n",
    "model = BaseLine(backend=\"linear\")\n",
    "\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "992cd584-474b-4616-bda6-dd8f3db096d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-18T10:37:37.954557Z",
     "iopub.status.busy": "2024-09-18T10:37:37.954266Z",
     "iopub.status.idle": "2024-09-18T10:37:37.989114Z",
     "shell.execute_reply": "2024-09-18T10:37:37.987364Z",
     "shell.execute_reply.started": "2024-09-18T10:37:37.954531Z"
    }
   },
   "outputs": [],
   "source": [
    "# from torchtnt.utils.flops import FlopTensorDispatchMode\n",
    "# with FlopTensorDispatchMode(model) as ftdm:\n",
    "#     res = model(x).mean()\n",
    "#     flops_forward = copy.deepcopy(ftdm.flop_counts)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
