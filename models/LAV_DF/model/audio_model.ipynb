{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff5c2c1-8607-4e7a-a954-2bfb6d01bdf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T08:30:35.962766Z",
     "iopub.status.busy": "2023-07-11T08:30:35.962283Z",
     "iopub.status.idle": "2023-07-11T08:30:37.465078Z",
     "shell.execute_reply": "2023-07-11T08:30:37.464237Z",
     "shell.execute_reply.started": "2023-07-11T08:30:35.962722Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import Tensor\n",
    "from torch.nn import BCEWithLogitsLoss, MSELoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3654e523-d6b9-448f-bb05-71c7033e54c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T08:30:37.467917Z",
     "iopub.status.busy": "2023-07-11T08:30:37.466904Z",
     "iopub.status.idle": "2023-07-11T08:30:37.556632Z",
     "shell.execute_reply": "2023-07-11T08:30:37.555664Z",
     "shell.execute_reply.started": "2023-07-11T08:30:37.467859Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from .audio_encoder import get_audio_encoder\n",
    "from .boundary_module import BoundaryModule\n",
    "from .frame_classifier import FrameLogisticRegression\n",
    "from .loss import MaskedFrameLoss, MaskedBMLoss, MaskedContrastLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "398f6660-cc26-4fa3-b152-b5eb268f599e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T15:07:39.232369Z",
     "iopub.status.busy": "2023-07-11T15:07:39.231830Z",
     "iopub.status.idle": "2023-07-11T15:07:39.246234Z",
     "shell.execute_reply": "2023-07-11T15:07:39.244961Z",
     "shell.execute_reply.started": "2023-07-11T15:07:39.232320Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Batfd(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # basis\n",
    "        temporal_dim=800,  # max frames of the audio\n",
    "        max_duration=40,  # max continuous deepfake frames\n",
    "        # encoder\n",
    "        a_encoder: str = \"cnn\",  # encoder type\n",
    "        n_features=(32, 64, 64),  # features dims in encoder\n",
    "        a_cla_feature_in=256,  # feature dim of the encoder output\n",
    "        # boundary\n",
    "        boundary_features=(512, 128),  # features dims in boundary module\n",
    "        boundary_samples=10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.temporal_dim = temporal_dim\n",
    "\n",
    "        self.audio_encoder = get_audio_encoder(\n",
    "            a_cla_feature_in, temporal_dim, a_encoder, n_features\n",
    "        )\n",
    "        self.audio_frame_classifier = FrameLogisticRegression(\n",
    "            n_features=a_cla_feature_in\n",
    "        )\n",
    "\n",
    "        a_bm_in = a_cla_feature_in + 1\n",
    "        self.audio_boundary_module = BoundaryModule(\n",
    "            a_bm_in, boundary_features, boundary_samples, temporal_dim, max_duration\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self, audio: Tensor\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        # encoders\n",
    "        a_features = self.audio_encoder(audio)\n",
    "        a_frame_cla = self.audio_frame_classifier(a_features)\n",
    "        a_bm_in = torch.column_stack([a_features, a_frame_cla])\n",
    "        a_bm_map = self.audio_boundary_module(a_bm_in)\n",
    "        return (\n",
    "            a_bm_map,\n",
    "            a_frame_cla,\n",
    "            a_features,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebe4b28e-2a7e-4ff9-ad51-f23fc71e0308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T15:07:40.084559Z",
     "iopub.status.busy": "2023-07-11T15:07:40.084086Z",
     "iopub.status.idle": "2023-07-11T15:07:40.112070Z",
     "shell.execute_reply": "2023-07-11T15:07:40.110921Z",
     "shell.execute_reply.started": "2023-07-11T15:07:40.084515Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Batfd_Audio_lit(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # basis\n",
    "        temporal_dim=800,  # max frames of the audio\n",
    "        max_duration=64,  # max continuous deepfake frames\n",
    "        # encoder\n",
    "        a_encoder: str = \"cnn\",  # encoder type\n",
    "        n_features=(32, 64, 64),  # features dims in encoder\n",
    "        a_cla_feature_in=256,  # feature dim of the encoder output\n",
    "        # boundary\n",
    "        boundary_features=(512, 128),  # features dims in boundary module\n",
    "        boundary_samples=10,\n",
    "        # training settings\n",
    "        weight_frame_loss=2.0,\n",
    "        weight_modal_bm_loss=1.0,\n",
    "        weight_decay=0.0001,\n",
    "        learning_rate=0.0002,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.temporal_dim = temporal_dim\n",
    "        self.max_duration = max_duration\n",
    "\n",
    "        self.audio_model = Batfd(\n",
    "            temporal_dim=temporal_dim,\n",
    "            max_duration=max_duration,\n",
    "            a_encoder=a_encoder,\n",
    "            n_features=n_features,\n",
    "            a_cla_feature_in=a_cla_feature_in,\n",
    "            boundary_features=boundary_features,\n",
    "            boundary_samples=boundary_samples,\n",
    "        )\n",
    "\n",
    "        self.weight_frame_loss = weight_frame_loss\n",
    "        self.weight_modal_bm_loss = weight_modal_bm_loss\n",
    "        self.frame_loss = MaskedFrameLoss(BCEWithLogitsLoss())\n",
    "        self.bm_loss = MaskedBMLoss(MSELoss())\n",
    "\n",
    "        \n",
    "        self.weight_decay = weight_decay\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(\n",
    "            self.audio_model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            betas=(0.5, 0.9),\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": ReduceLROnPlateau(\n",
    "                    optimizer, factor=0.5, patience=3, verbose=True, min_lr=1e-8\n",
    "                ),\n",
    "                \"monitor\": \"val_loss\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def loss_fn(\n",
    "        self,\n",
    "        a_bm_map: Tensor,\n",
    "        a_frame_cla: Tensor,\n",
    "        n_frames: Tensor,\n",
    "        a_bm_label,\n",
    "        a_frame_label,\n",
    "    ) -> Dict[str, Tensor]:\n",
    "\n",
    "        a_bm_loss = self.bm_loss(a_bm_map, a_bm_label, n_frames)\n",
    "        a_frame_loss = self.frame_loss(a_frame_cla.squeeze(1), a_frame_label, n_frames)\n",
    "\n",
    "        loss = (\n",
    "            self.weight_modal_bm_loss * a_bm_loss\n",
    "            + self.weight_frame_loss * a_frame_loss\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"a_bm_loss\": a_bm_loss,\n",
    "            \"a_frame_loss\": a_frame_loss,\n",
    "        }\n",
    "\n",
    "    def shared_evaluate_step(self, batch, batch_idx, prefix=\"\"):\n",
    "        a_bm_map, a_frame_cla, a_features = self.audio_model(batch[\"audio\"])\n",
    "        # print(batch['audio'].shape, batch['bm_label'].shape, a_bm_map.shape, a_frame_cla.shape)\n",
    "        loss_dict = self.loss_fn(\n",
    "            a_bm_map=a_bm_map,\n",
    "            a_frame_cla=a_frame_cla,\n",
    "            n_frames=batch['frames'],\n",
    "            a_bm_label=batch[\"bm_label\"],\n",
    "            a_frame_label=batch[\"frame_label\"],\n",
    "        )\n",
    "\n",
    "        self.log_dict(\n",
    "            {f\"{prefix}_{k}\": v for k, v in loss_dict.items()},\n",
    "            on_step=True if prefix == 'train' else False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        res = loss_dict\n",
    "        res[\"bm_map\"] = a_bm_map\n",
    "        res[\"frame_label\"] = a_frame_cla\n",
    "        res['frames'] = batch['frames']\n",
    "        return res\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        batch: Optional[Union[Tensor, Sequence[Tensor]]] = None,\n",
    "        batch_idx: Optional[int] = None,\n",
    "    ) -> Tensor:\n",
    "        res = self.shared_evaluate_step(batch, batch_idx, prefix=\"train\")\n",
    "        return res\n",
    "\n",
    "    def validation_step(\n",
    "        self,\n",
    "        batch: Optional[Union[Tensor, Sequence[Tensor]]] = None,\n",
    "        batch_idx: Optional[int] = None,\n",
    "        dataloader_idx: Optional[int] = None,\n",
    "    ) -> Tensor:\n",
    "        res = self.shared_evaluate_step(batch, batch_idx, prefix=\"val\")\n",
    "        return res\n",
    "\n",
    "    def test_step(\n",
    "        self,\n",
    "        batch: Optional[Union[Tensor, Sequence[Tensor]]] = None,\n",
    "        batch_idx: Optional[int] = None,\n",
    "        dataloader_idx: Optional[int] = None,\n",
    "    ) -> Tensor:\n",
    "        res = self.shared_evaluate_step(batch, batch_idx, prefix=\"test\")\n",
    "        return res\n",
    "\n",
    "    def predict_step(\n",
    "        self, batch: Tensor, batch_idx: int, dataloader_idx: Optional[int] = None\n",
    "    ) -> Tensor:\n",
    "        res = self.shared_evaluate_step(batch, batch_idx, prefix=\"predict\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9dee28-0e91-4abd-aa58-e395f1659fdb",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-07-11T08:45:12.685690Z",
     "iopub.status.busy": "2023-07-11T08:45:12.685136Z",
     "iopub.status.idle": "2023-07-11T08:45:12.693909Z",
     "shell.execute_reply": "2023-07-11T08:45:12.692613Z",
     "shell.execute_reply.started": "2023-07-11T08:45:12.685644Z"
    },
    "lines_to_next_cell": 0,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "audio = torch.randn(1, 64, 3200)\n",
    "\n",
    "model = Batfd()\n",
    "a_bm_map, a_frame_cla, a_features = model(audio)\n",
    "\n",
    "a_bm_map.shape, a_frame_cla.shape, a_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d26514-9da2-495b-b9f5-cdc9be62b48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffd0a2-1d9e-48a6-98f0-3c499ef7f8d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
