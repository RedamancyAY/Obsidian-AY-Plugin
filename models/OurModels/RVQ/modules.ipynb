{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "try:\n",
    "    from .quantization import ResidualVectorQuantizer, QuantizedResult\n",
    "except ImportError:\n",
    "    from quantization import ResidualVectorQuantizer, QuantizedResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 \n",
    "\n",
    "<center><img src=\"https://cdn.jsdelivr.net/gh/RedamancyAY/CloudImage@main/img20241209231215699.png\" width=\"400\" alt=\"$fileName\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioQuantization(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, sample_rate=16000, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        model_name = \"facebook/hubert-base-ls960\"\n",
    "        self.model = HubertModel.from_pretrained(model_name)\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "        self.dim = self.model.config.hidden_size\n",
    "        self.RVQ = ResidualVectorQuantizer(\n",
    "            dimension=self.dim, bins=50, n_q=8\n",
    "        )\n",
    "        self.frame_rate = sample_rate / np.prod(self.model.config.conv_stride)\n",
    "\n",
    "    def forward(self, x) -> QuantizedResult:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y = self.model(x, output_hidden_states=True)\n",
    "            hidden_states = y.hidden_states\n",
    "\n",
    "        # feat is with shape of (B, T, C)\n",
    "        feat = hidden_states[9]\n",
    "\n",
    "        ## rvq require the input be shape of (B, C, T)\n",
    "        vq_res = self.RVQ(feat.transpose(2, 1), frame_rate=self.frame_rate)\n",
    "\n",
    "\n",
    "        ## vq_res.codes is with shape of (n_q, B, T), we only use the last quantizer\n",
    "        ## thus, the output if with shape of (B, T)\n",
    "        vq_res.codes = vq_res.codes[-1]\n",
    "\n",
    "        ## the shape of quantized is (B, C, T)\n",
    "        # quantized = vq_res.quantized\n",
    "        \n",
    "    \n",
    "        return vq_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff3300; text-decoration-color: #ff3300; background-color: #00ff00\">Build ResidualVectorQuantizer with dim </span><span style=\"color: #ff3300; text-decoration-color: #ff3300; background-color: #00ff00; font-weight: bold\">768</span><span style=\"color: #ff3300; text-decoration-color: #ff3300; background-color: #00ff00\">, </span><span style=\"color: #ff3300; text-decoration-color: #ff3300; background-color: #00ff00; font-weight: bold\">50</span><span style=\"color: #ff3300; text-decoration-color: #ff3300; background-color: #00ff00\"> bins, </span><span style=\"color: #ff3300; text-decoration-color: #ff3300; background-color: #00ff00; font-weight: bold\">8</span><span style=\"color: #ff3300; text-decoration-color: #ff3300; background-color: #00ff00\"> cascaded quantizers</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;51;0;48;2;0;255;0mBuild ResidualVectorQuantizer with dim \u001b[0m\u001b[1;38;2;255;51;0;48;2;0;255;0m768\u001b[0m\u001b[38;2;255;51;0;48;2;0;255;0m, \u001b[0m\u001b[1;38;2;255;51;0;48;2;0;255;0m50\u001b[0m\u001b[38;2;255;51;0;48;2;0;255;0m bins, \u001b[0m\u001b[1;38;2;255;51;0;48;2;0;255;0m8\u001b[0m\u001b[38;2;255;51;0;48;2;0;255;0m cascaded quantizers\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/Coding2/0-Deepfake/2-Audio/models/OurModels/RVQ/quantization/core_vq.py:307: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "quantization_model = AudioQuantization()\n",
    "x = torch.randn(16, 48000)\n",
    "res = quantization_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[36, 36, 11,  ..., 24, 22, 20],\n",
       "        [ 5, 22,  7,  ..., 35, 11, 32],\n",
       "        [35, 35,  4,  ..., 23, 45, 10],\n",
       "        ...,\n",
       "        [42,  1, 20,  ..., 12, 28, 12],\n",
       "        [14, 12, 37,  ..., 32, 14, 20],\n",
       "        [23, 30, 26,  ..., 23, 12, 37]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_means(x: torch.Tensor, segment_sizes: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      tensor: torch.Tensor: a 2D tensor with shape of `(L, C)`\n",
    "      segment_sizes: torch.Tensor: a 1D tensor that its sum is equal to the length `L` of tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.Tenosr: the tensor with reduce length `(L', C)`, where $L'=len(segment_sizes)$\n",
    "    \"\"\"\n",
    "    assert x.size(0) == segment_sizes.sum(), \"Sum of segment sizes must equal the tensor's first dimension size.\"\n",
    "\n",
    "    # Create an indices tensor that maps each row in the tensor to its corresponding segment\n",
    "    indices = torch.repeat_interleave(torch.arange(len(segment_sizes), device=x.device), segment_sizes)\n",
    "\n",
    "    # Create a tensor to hold the sum of each segment\n",
    "    segment_sums = torch.zeros(len(segment_sizes), x.size(1), device=x.device)\n",
    "\n",
    "    # Scatter and sum the inputs into the segment_sums tensor\n",
    "    segment_sums.scatter_add_(0, indices.unsqueeze(1).expand(-1, x.size(1)), x)\n",
    "\n",
    "    # Calculate the mean of each segment\n",
    "    _segment_means = segment_sums / segment_sizes.unsqueeze(1)\n",
    "\n",
    "    return _segment_means\n",
    "\n",
    "\n",
    "def reduce_feat_by_phonemes(\n",
    "    hidden_states: Tensor, audio_lengths: Tensor, phoneme_ids: Tensor, debug: bool = False\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    For each audio, combine continuous phonemes to reduce the temporal dimension of audio features.\n",
    "    For example, the audio with 10 frames, which phoneme ids will change from\n",
    "    ```python\n",
    "    [0, 0, 0, 1, 1, 1, 2, 2, 0, 0] -> [0, 1, 2, 0]\n",
    "    ```\n",
    "    and the hidden_states will also changed by this way.\n",
    "\n",
    "\n",
    "    Args:\n",
    "      hidden_states:Tensor: a 3D tensor with shape of (B, T, C), where T is audio frames\n",
    "      audio_lengths:Tensor: a 1D tensor with shape of (B,) that represent the legnth of each audio\n",
    "      phoneme_ids:Tensor: a 2D tensor with shape of (B, T) that represents the phoneme ids in each audio frame\n",
    "      debug:bool: determine whether to debug tensor info.\n",
    "\n",
    "    Returns:\n",
    "        the reduced hidden_states with shape (B*L', C), reduced audio lengths, reduced phoneme ids.\n",
    "    \"\"\"\n",
    "\n",
    "    reduced_hidden_states = []\n",
    "    reduced_audio_lengths = []\n",
    "    reduced_phoneme_ids = []\n",
    "    phoneme_counts = []\n",
    "\n",
    "    if debug:\n",
    "        s = time.time()\n",
    "        print(\"reduce feat input:\", hidden_states.shape, audio_lengths.shape, phoneme_ids.shape)\n",
    "\n",
    "    for i in range(len(audio_lengths)):\n",
    "        _phoneme_ids = phoneme_ids[i, : audio_lengths[i]]\n",
    "        unique_ids, _phoneme_counts = _phoneme_ids.unique_consecutive(return_counts=True)\n",
    "        phoneme_counts += _phoneme_counts.tolist()\n",
    "\n",
    "        reduced_audio_lengths.append(len(unique_ids))\n",
    "        reduced_phoneme_ids.append(unique_ids)\n",
    "\n",
    "    reduced_audio_lengths = torch.tensor(reduced_audio_lengths)\n",
    "    reduced_phoneme_ids = torch.nn.utils.rnn.pad_sequence(reduced_phoneme_ids, batch_first=True)\n",
    "    h = torch.concat([hidden_states[i, :_len, :] for i, _len in enumerate(audio_lengths)], dim=0)\n",
    "    reduced_hidden_states = segment_means(h, torch.tensor(phoneme_counts, device=hidden_states.device))\n",
    "\n",
    "    if debug:\n",
    "        e = time.time()\n",
    "        print(\n",
    "            \"reduce feat output:\", reduced_hidden_states.shape, reduced_audio_lengths.shape, reduced_phoneme_ids.shape\n",
    "        )\n",
    "        print(\"reduce feat time:\", e - s)\n",
    "    return reduced_hidden_states, reduced_audio_lengths, reduced_phoneme_ids\n",
    "\n",
    "def get_id_based_frame_res(_dense_res, _full_unit_res):\n",
    "    reduced_hidden_states, reduced_audio_lengths, reduced_phoneme_ids = reduce_feat_by_phonemes(\n",
    "        hidden_states=torch.tensor(_dense_res),\n",
    "        audio_lengths=torch.tensor([149] * len(_dense_res)),\n",
    "        phoneme_ids=torch.tensor(_full_unit_res),\n",
    "        debug=0,\n",
    "    )\n",
    "    print(reduced_audio_lengths)\n",
    "\n",
    "    split_res = torch.split(reduced_hidden_states, tuple(reduced_audio_lengths.numpy()))\n",
    "\n",
    "    id_based_frame_res = torch.stack([x.mean(0) for x in split_res])\n",
    "    return id_based_frame_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_id_based_frame_res(res.quantized, res.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src/transformers/models/hubert/modeling_hubert.py#L945\n",
    "\n",
    "\n",
    "```python\n",
    "layers = quantization_model.model.encoder.layers\n",
    "hidden_states = res.quantized.transpose(1, 2) # (B, T, C) -> (B, C, T)\n",
    "for layer in layers[9:12]:\n",
    "    layer_outputs = layer(hidden_states)\n",
    "    hidden_states = layer_outputs[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # Define a linear layer with input dimension `input_dim` and output dimension `1`\n",
    "        self.classifier = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, vq_res: QuantizedResult):\n",
    "\n",
    "        codes = vq_res.codes\n",
    "        quantized = vq_res.quantized\n",
    "\n",
    "        # Pass the input through the linear layer\n",
    "        feat = quantized.mean(-1)\n",
    "        output = self.classifier(feat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.quantizer = AudioQuantization()\n",
    "        self.classifier = LinearClassifier(input_dim=self.quantizer.dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor, train_quantizer=True):\n",
    "        \n",
    "        if x.ndim == 3 and x.size(1) == 1:\n",
    "            x = x[:, 0, :]\n",
    "        \n",
    "        \n",
    "        if train_quantizer:\n",
    "            vq_res = self.quantizer(x) # (B, C, T)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                vq_res = self.quantizer(x) # (B, C, T)\n",
    "        codes = vq_res.codes # (B, T)\n",
    "        \n",
    "        \n",
    "        layers = self.quantizer.model.encoder.layers\n",
    "        hidden_states = vq_res.quantized.transpose(1, 2) # (B, C, T) -> (B, T, C)\n",
    "        \n",
    "        \n",
    "        ## use transformer layers to process the hidden states\n",
    "        ## The final hidden states is with shape of (B, T, C)\n",
    "        for layer in layers[9:12]:\n",
    "            layer_outputs = layer(hidden_states)\n",
    "            hidden_states = layer_outputs[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # feat = hidden_states.mean(1)\n",
    "        feat = get_id_based_frame_res(hidden_states, codes)\n",
    "        \n",
    "        logit = self.classifier.classifier(feat)\n",
    "        \n",
    "        # logit = self.classifier(vq_res)\n",
    "        \n",
    "        return {\"logit\" : logit, \"vq_res\":vq_res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
