{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResidualVectorQuantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantization import ResidualVectorQuantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@dataclass\n",
    "class QuantizedResult:\n",
    "    quantized: torch.Tensor\n",
    "    codes: torch.Tensor\n",
    "    bandwidth: torch.Tensor  # bandwidth in kb/s used, per batch item.\n",
    "    penalty: tp.Optional[torch.Tensor] = None\n",
    "    metrics: dict = field(default_factory=dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvq = ResidualVectorQuantizer(dimension=256, bins=128, n_q=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/Coding2/0-Deepfake/2-Audio/models/OurModels/RVQ/quantization/core_vq.py:307: UserWarning: When using RVQ in training model, first check https://github.com/facebookresearch/encodec/issues/25 . The bug wasn't fixed here for reproducibility.\n",
      "  warnings.warn('When using RVQ in training model, first check '\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 256, 149)\n",
    "res = rvq(x, frame_rate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 256, 149]), torch.Size([8, 32, 149]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res.quantized.shape, res.codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0613,  0.0208, -0.5809,  ..., -0.3119, -0.1753,  0.4991],\n",
       "         [ 0.2622,  0.0356, -0.3620,  ..., -0.6320, -0.0275, -0.2161],\n",
       "         [ 0.0835, -0.2980,  0.1818,  ...,  0.0466,  0.4261,  0.4125],\n",
       "         ...,\n",
       "         [ 0.2573,  0.0555, -0.0285,  ..., -0.2614, -0.1493,  0.2594],\n",
       "         [-0.4015, -0.5352, -0.1730,  ...,  0.2641,  0.0719, -0.1478],\n",
       "         [ 0.8366, -0.0354,  0.4255,  ...,  0.1570,  0.2005,  0.0879]],\n",
       "\n",
       "        [[ 0.2560, -0.2072, -0.5095,  ..., -0.2815, -1.6602,  0.3065],\n",
       "         [-0.1715,  0.5112,  0.2353,  ...,  0.1503,  0.3380,  0.5557],\n",
       "         [-0.3384, -0.0547,  0.2531,  ...,  0.5671,  0.7006,  0.5481],\n",
       "         ...,\n",
       "         [-0.8936, -0.6395, -0.1753,  ...,  0.7905, -1.9702,  0.1477],\n",
       "         [ 0.3633, -0.1222, -0.1304,  ...,  0.3106,  0.4887, -0.0433],\n",
       "         [ 0.4290,  0.2813,  0.5465,  ..., -0.1524,  1.1987,  0.0202]],\n",
       "\n",
       "        [[-0.3873,  1.4404,  0.2338,  ..., -0.5036,  0.2913,  0.4075],\n",
       "         [ 0.4622, -0.8767,  0.1305,  ..., -0.0040,  0.0170,  0.3739],\n",
       "         [ 0.0246, -2.6977,  0.4571,  ...,  0.0698,  0.0833, -0.2528],\n",
       "         ...,\n",
       "         [ 1.2515, -0.1559,  0.0537,  ..., -0.4953, -0.2481, -0.2689],\n",
       "         [-0.4608,  0.1783,  0.3089,  ...,  0.3109, -0.1932, -0.5216],\n",
       "         [-0.4341, -0.8195,  0.4551,  ...,  0.6150, -0.2970,  0.3612]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0624,  0.1812, -1.8197,  ...,  0.2167,  0.1958, -0.0896],\n",
       "         [-0.0599, -0.3517,  0.9438,  ..., -0.3323,  0.4298, -0.3328],\n",
       "         [-0.2036,  0.3085,  0.5488,  ..., -0.1218, -0.8940, -0.0257],\n",
       "         ...,\n",
       "         [ 0.0312, -0.0198, -0.3957,  ..., -0.2648,  0.7433, -0.2783],\n",
       "         [-0.6055,  0.1832,  0.9651,  ..., -0.2183, -0.5124,  0.5804],\n",
       "         [ 0.2172,  0.3831, -0.2098,  ...,  0.2643,  0.5247, -0.0165]],\n",
       "\n",
       "        [[ 0.8074, -0.2504,  0.3264,  ..., -0.1970,  0.2428,  0.3713],\n",
       "         [-0.0326,  0.3235,  0.3553,  ...,  0.1052, -0.3890,  0.2851],\n",
       "         [-0.9008,  0.4330,  0.9690,  ...,  0.1262,  0.1558,  0.3396],\n",
       "         ...,\n",
       "         [ 1.9147,  0.1972, -0.7872,  ..., -0.1102,  0.3115, -0.0328],\n",
       "         [ 0.3241, -0.1439, -1.2575,  ...,  0.1929, -0.0462,  0.2579],\n",
       "         [-1.2195, -0.0820, -0.1043,  ..., -0.2379,  0.2146, -0.6572]],\n",
       "\n",
       "        [[-0.6889,  0.4595, -0.0954,  ..., -1.0930, -0.5691, -0.8916],\n",
       "         [-0.0322,  0.6486,  0.4727,  ..., -1.0278, -0.1548,  1.2634],\n",
       "         [ 0.1779, -1.1782,  0.1543,  ..., -0.4061, -0.2864, -0.4498],\n",
       "         ...,\n",
       "         [-0.1762, -0.3593, -0.0162,  ..., -0.8966, -0.1322,  0.6272],\n",
       "         [-0.7010, -0.0353,  0.6851,  ...,  0.7226, -0.2097, -0.7419],\n",
       "         [ 0.3407,  0.0398,  0.2319,  ...,  0.7630,  0.1184, -1.0169]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bdb96a040f4f64a2503f30ce143bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/561 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe60a08fc7f4899b817698c715315f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c21585a888e491d98d27d79acbe2a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import HubertModel, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Load HuBERT model\n",
    "model_name = \"facebook/hubert-base-ls960\"\n",
    "model = HubertModel.from_pretrained(model_name)\n",
    "\n",
    "# Load feature extractor for HuBERT model (if you need audio processing)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "16000/np.prod(model.config.conv_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(16, 48000)\n",
    "y = model(x, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 149, 768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.hidden_states[9].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
