{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7febc91f-0e6f-4932-b958-848bd2be0c52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T10:33:16.131406Z",
     "iopub.status.busy": "2024-07-28T10:33:16.130942Z",
     "iopub.status.idle": "2024-07-28T10:33:16.139805Z",
     "shell.execute_reply": "2024-07-28T10:33:16.137643Z",
     "shell.execute_reply.started": "2024-07-28T10:33:16.131359Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc549b40-d2fe-40ad-8a05-5c6461a770cb",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "在音素识别时，一个音素可能会跨多个时间帧，因此需要合并连续且相同的音素，构造为一个音素，从而预测整句话的音素，并与ground truth进行比较。\n",
    ":::\n",
    "\n",
    "那使用音素特征识别时，那么也许合并连续且相同的音素特征帧。设音素特征为`(B, T, 768)`，那么合并后的特征为`(B, T', 768)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2189d86-d048-4952-b038-c6c071928a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T10:43:14.561460Z",
     "iopub.status.busy": "2024-07-28T10:43:14.560819Z",
     "iopub.status.idle": "2024-07-28T10:43:14.576470Z",
     "shell.execute_reply": "2024-07-28T10:43:14.574220Z",
     "shell.execute_reply.started": "2024-07-28T10:43:14.561398Z"
    }
   },
   "outputs": [],
   "source": [
    "def segment_means(x: torch.Tensor, segment_sizes: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      tensor: torch.Tensor: a 2D tensor with shape of `(L, C)`\n",
    "      segment_sizes: torch.Tensor: a 1D tensor that its sum is equal to the length `L` of tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.Tenosr: the tensor with reduce length `(L', C)`, where $L'=len(segment_sizes)$\n",
    "    \"\"\"\n",
    "    assert x.size(0) == segment_sizes.sum(), \"Sum of segment sizes must equal the tensor's first dimension size.\"\n",
    "\n",
    "    # Create an indices tensor that maps each row in the tensor to its corresponding segment\n",
    "    indices = torch.repeat_interleave(torch.arange(len(segment_sizes), device=x.device), segment_sizes)\n",
    "\n",
    "    # Create a tensor to hold the sum of each segment\n",
    "    segment_sums = torch.zeros(len(segment_sizes), x.size(1), device=x.device)\n",
    "\n",
    "    # Scatter and sum the inputs into the segment_sums tensor\n",
    "    segment_sums.scatter_add_(0, indices.unsqueeze(1).expand(-1, x.size(1)), x)\n",
    "\n",
    "    # Calculate the mean of each segment\n",
    "    segment_means = segment_sums / segment_sizes.unsqueeze(1)\n",
    "\n",
    "    return segment_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b57ed3-b459-4f84-8c28-bccd5c8b89f7",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-07-28T10:31:46.171386Z",
     "iopub.status.busy": "2024-07-28T10:31:46.170696Z",
     "iopub.status.idle": "2024-07-28T10:31:46.184382Z",
     "shell.execute_reply": "2024-07-28T10:31:46.182755Z",
     "shell.execute_reply.started": "2024-07-28T10:31:46.171323Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2530],\n",
      "        [-0.3817],\n",
      "        [ 0.1770],\n",
      "        [ 0.8541],\n",
      "        [ 1.2558]]) \n",
      " tensor([[ 0.2530],\n",
      "        [-0.1024],\n",
      "        [ 1.0549]])\n"
     ]
    }
   ],
   "source": [
    "## 验证\n",
    "x = torch.randn(5, 1)\n",
    "segments = torch.tensor([1, 2, 2])\n",
    "res = segment_means(x, segments)\n",
    "print(x, \"\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19dc8a21-651e-4727-9416-dfe2028a71cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T10:48:28.602673Z",
     "iopub.status.busy": "2024-07-28T10:48:28.602149Z",
     "iopub.status.idle": "2024-07-28T10:48:28.611861Z",
     "shell.execute_reply": "2024-07-28T10:48:28.610877Z",
     "shell.execute_reply.started": "2024-07-28T10:48:28.602643Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduce_feat_by_phonemes(hidden_states: Tensor, audio_lengths: Tensor, phoneme_ids: Tensor, debug: bool = False) -> Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    For each audio, combine continuous phonemes to reduce the temporal dimension of audio features.\n",
    "    For example, the audio with 10 frames, which phoneme ids will change from\n",
    "    ```python\n",
    "    [0, 0, 0, 1, 1, 1, 2, 2, 0, 0] -> [0, 1, 2, 0]\n",
    "    ```\n",
    "    and the hidden_states will also changed by this way.\n",
    "\n",
    "\n",
    "    Args:\n",
    "      hidden_states:Tensor: a 3D tensor with shape of (B, T, C), where T is audio frames\n",
    "      audio_lengths:Tensor: a 1D tensor with shape of (B,) that represent the legnth of each audio\n",
    "      phoneme_ids:Tensor: a 2D tensor with shape of (B, T) that represents the phoneme ids in each audio frame\n",
    "      debug:bool: determine whether to debug tensor info.\n",
    "\n",
    "    Returns:\n",
    "        the reduced hidden_states with shape (B*L', C), reduced audio lengths, reduced phoneme ids.\n",
    "    \"\"\"\n",
    "\n",
    "    reduced_hidden_states = []\n",
    "    reduced_audio_lengths = []\n",
    "    reduced_phoneme_ids = []\n",
    "    phoneme_counts = []\n",
    "\n",
    "    \n",
    "    if debug:\n",
    "        s = time.time()\n",
    "        print('reduce feat input:', hidden_states.shape, audio_lengths.shape, phoneme_ids.shape)\n",
    "\n",
    "    for i in range(len(audio_lengths)):\n",
    "        _phoneme_ids = phoneme_ids[i, : audio_lengths[i]]\n",
    "        unique_ids, _phoneme_counts = _phoneme_ids.unique_consecutive(return_counts=True)\n",
    "        phoneme_counts += _phoneme_counts.tolist()\n",
    "\n",
    "        reduced_audio_lengths.append(len(unique_ids))\n",
    "        reduced_phoneme_ids.append(unique_ids)\n",
    "\n",
    "    reduced_audio_lengths = torch.tensor(reduced_audio_lengths)\n",
    "    reduced_phoneme_ids = torch.nn.utils.rnn.pad_sequence(reduced_phoneme_ids, batch_first=True)\n",
    "    h = torch.concat([hidden_states[i, :_len, :] for i, _len in enumerate(audio_lengths)], dim=0)\n",
    "    reduced_hidden_states = segment_means(h, torch.tensor(phoneme_counts, device=hidden_states.device))\n",
    "\n",
    "    if debug:\n",
    "        e = time.time()\n",
    "        print('reduce feat output:',  reduced_hidden_states.shape,reduced_audio_lengths.shape, reduced_phoneme_ids.shape)\n",
    "        print('reduce feat time:', e - s)\n",
    "    return reduced_hidden_states, reduced_audio_lengths, reduced_phoneme_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0c74525-3f2e-47b6-aff6-9e7e06652502",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-07-28T10:48:29.339076Z",
     "iopub.status.busy": "2024-07-28T10:48:29.337695Z",
     "iopub.status.idle": "2024-07-28T10:48:29.466530Z",
     "shell.execute_reply": "2024-07-28T10:48:29.465180Z",
     "shell.execute_reply.started": "2024-07-28T10:48:29.339010Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce feat input: torch.Size([64, 149, 768]) torch.Size([64]) torch.Size([64, 149])\n",
      "reduce feat output: torch.Size([7379, 768]) torch.Size([64]) torch.Size([64, 125])\n",
      "reduce feat time: 0.010946512222290039\n",
      "torch.Size([64]) torch.Size([64, 125]) torch.Size([7379, 768])\n"
     ]
    }
   ],
   "source": [
    "B = 64\n",
    "T = 149\n",
    "hidden_states = torch.randn(B, T, 768).cuda().requires_grad_(True)\n",
    "phoneme_ids = torch.randint(0, 5, (B, T)).cuda()\n",
    "audio_lengths = torch.randint(140, 149, (B,)).cuda()\n",
    "\n",
    "reduced_hidden_states, reduced_audio_lengths, reduced_phoneme_ids = reduce_feat_by_phonemes(\n",
    "    hidden_states, audio_lengths, phoneme_ids, debug=1\n",
    ")\n",
    "print(reduced_audio_lengths.shape, reduced_phoneme_ids.shape, reduced_hidden_states.shape)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
