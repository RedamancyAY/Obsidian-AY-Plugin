{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fb20e1cd-542b-4170-94fc-ba1870f1a77e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T07:50:41.534229Z",
     "iopub.status.busy": "2024-08-13T07:50:41.533518Z",
     "iopub.status.idle": "2024-08-13T07:50:41.613549Z",
     "shell.execute_reply": "2024-08-13T07:50:41.611099Z",
     "shell.execute_reply.started": "2024-08-13T07:50:41.534162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "910a0d69-fffd-4a4b-add2-7485bad0e0ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T07:50:42.259723Z",
     "iopub.status.busy": "2024-08-13T07:50:42.258991Z",
     "iopub.status.idle": "2024-08-13T07:50:42.344832Z",
     "shell.execute_reply": "2024-08-13T07:50:42.342900Z",
     "shell.execute_reply.started": "2024-08-13T07:50:42.259663Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from ay2.tools import freeze_modules\n",
    "from ay2.torch.nn import LambdaFunctionModule\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3806ff31-8724-43a1-9355-950918107dc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T07:50:43.400870Z",
     "iopub.status.busy": "2024-08-13T07:50:43.399909Z",
     "iopub.status.idle": "2024-08-13T07:50:43.445785Z",
     "shell.execute_reply": "2024-08-13T07:50:43.444180Z",
     "shell.execute_reply.started": "2024-08-13T07:50:43.400825Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "04ca189f-8b60-4c56-9511-5035ffe56343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T07:50:44.071003Z",
     "iopub.status.busy": "2024-08-13T07:50:44.069969Z",
     "iopub.status.idle": "2024-08-13T07:50:44.131824Z",
     "shell.execute_reply": "2024-08-13T07:50:44.129949Z",
     "shell.execute_reply.started": "2024-08-13T07:50:44.070946Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from ...Aaasist.Aaasist.load_model import get_model as load_AASIST\n",
    "    from ...WaveLM.wavlm import BaseLine as WavLM\n",
    "except ImportError:\n",
    "    sys.path.append(\"../../Aaasist\")\n",
    "    sys.path.append(\"../../WaveLM\")\n",
    "    from Aaasist.load_model import get_model as load_AASIST\n",
    "    from wavlm import BaseLine as WavLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d49c16-fb32-4e46-a208-24db75a27f13",
   "metadata": {},
   "source": [
    "# 1D models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff2d721-98c6-4119-9539-bf146437a93c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T01:25:22.475384Z",
     "iopub.status.busy": "2024-08-11T01:25:22.475118Z",
     "iopub.status.idle": "2024-08-11T01:25:22.510149Z",
     "shell.execute_reply": "2024-08-11T01:25:22.508676Z",
     "shell.execute_reply.started": "2024-08-11T01:25:22.475364Z"
    }
   },
   "outputs": [],
   "source": [
    "class WavLM_1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model1D = WavLM()\n",
    "        self.n_dim = 768\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 3:\n",
    "            x = x[:, 0, :]\n",
    "        feature = self.model1D.pretrain_model(x)[self.model1D.pretrain_feat]  # (B, T, 768)\n",
    "        return feature.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf08127-725c-4827-b57c-6e6d0d8c7d9f",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-08-11T01:25:22.513198Z",
     "iopub.status.busy": "2024-08-11T01:25:22.513003Z",
     "iopub.status.idle": "2024-08-11T01:25:23.130657Z",
     "shell.execute_reply": "2024-08-11T01:25:23.129098Z",
     "shell.execute_reply.started": "2024-08-11T01:25:22.513182Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "style-solution",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/torch/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at /usr/local/ay_data/0-model_weights/microsoft_wavlm-base were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at /usr/local/ay_data/0-model_weights/microsoft_wavlm-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model1D \u001b[38;5;241m=\u001b[39m WavLM_1D()\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m48000\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m(x)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model1D = WavLM_1D()\n",
    "x = torch.randn(2, 1, 48000)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f66d9b-95de-4c1c-9191-6a81ad30db7c",
   "metadata": {},
   "source": [
    "## 分解 特征提取过程　"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ed5f4-d520-41cd-a1d5-af1e7e1607a6",
   "metadata": {},
   "source": [
    "### 使用1D CNN 提取特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c8065-c8b3-4b94-b088-ca4c352d2c45",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T01:25:23.131546Z",
     "iopub.status.idle": "2024-08-11T01:25:23.131812Z",
     "shell.execute_reply": "2024-08-11T01:25:23.131693Z",
     "shell.execute_reply.started": "2024-08-11T01:25:23.131681Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2, 48000)\n",
    "feat = model1D.pretrain_model.feature_extractor(x)\n",
    "\n",
    "extract_features = model1D.pretrain_model.feature_extractor(x)\n",
    "extract_features = extract_features.transpose(1, 2)\n",
    "\n",
    "# 输出的extract_features其实就是输入的layer norm\n",
    "hidden_states, extract_features = model1D.pretrain_model.feature_projection(extract_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159bfd1-d800-45e9-99dc-a980113db6d1",
   "metadata": {},
   "source": [
    "### 使用 Transformer 提取特征　"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89013e72-5171-492a-85c0-440e3ccba877",
   "metadata": {},
   "source": [
    "在transformer中，layers共有12层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fb520-0100-4da9-954a-9723b6816787",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T01:25:23.133241Z",
     "iopub.status.idle": "2024-08-11T01:25:23.133504Z",
     "shell.execute_reply": "2024-08-11T01:25:23.133389Z",
     "shell.execute_reply.started": "2024-08-11T01:25:23.133376Z"
    }
   },
   "outputs": [],
   "source": [
    "len(model1D.pretrain_model.encoder.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc0a91-711a-4242-8ad1-9be063240ae8",
   "metadata": {},
   "source": [
    "直接提取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239270e8-692d-43d4-9057-cfded0099aae",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T01:25:23.134618Z",
     "iopub.status.idle": "2024-08-11T01:25:23.134885Z",
     "shell.execute_reply": "2024-08-11T01:25:23.134767Z",
     "shell.execute_reply.started": "2024-08-11T01:25:23.134755Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_outputs = model1D.pretrain_model.encoder(\n",
    "    hidden_states,\n",
    "    attention_mask=None,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    return_dict=False,\n",
    ")\n",
    "hidden_states2 = encoder_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac69a5-9e3a-4c77-b333-2a0391146b36",
   "metadata": {},
   "source": [
    "提取的详细过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e1083-8856-4baa-a4df-b4e9a72e38d5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T01:25:23.135846Z",
     "iopub.status.idle": "2024-08-11T01:25:23.136113Z",
     "shell.execute_reply": "2024-08-11T01:25:23.135993Z",
     "shell.execute_reply.started": "2024-08-11T01:25:23.135980Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layerdrop = 0.05  ## default is 0.05\n",
    "position_embeddings = model1D.pretrain_model.encoder.pos_conv_embed(hidden_states)\n",
    "hidden_states = hidden_states + position_embeddings\n",
    "hidden_states = model1D.pretrain_model.encoder.layer_norm(hidden_states)\n",
    "hidden_states = model1D.pretrain_model.encoder.dropout(hidden_states)\n",
    "position_bias = None\n",
    "for i, layer in enumerate(model1D.pretrain_model.encoder.layers):\n",
    "    dropout_probability = np.random.uniform(0, 1)\n",
    "    skip_the_layer = model1D.pretrain_model.encoder.training and i > 0 and (dropout_probability < layerdrop)\n",
    "    if not skip_the_layer:\n",
    "        layer_outputs = layer(\n",
    "            hidden_states,\n",
    "            attention_mask=None,\n",
    "            position_bias=position_bias,\n",
    "            output_attentions=False,\n",
    "            index=i,\n",
    "        )\n",
    "        hidden_states, position_bias = layer_outputs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3a1b6-a76b-449c-bd17-03c9b7667043",
   "metadata": {},
   "source": [
    "经验证，上述过程和直接提取结果是一样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa81441-7016-486d-9ef6-1aa83b1a4246",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T01:25:23.137249Z",
     "iopub.status.idle": "2024-08-11T01:25:23.137520Z",
     "shell.execute_reply": "2024-08-11T01:25:23.137397Z",
     "shell.execute_reply.started": "2024-08-11T01:25:23.137385Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.sum(torch.abs(hidden_states2 - hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5138c-e9d2-479d-b942-ab08780af742",
   "metadata": {},
   "source": [
    "那么，可以将这12层分组，分成多个stage计算：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b25827-508a-4a81-ba43-e68fc144b8ca",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T01:25:23.138645Z",
     "iopub.status.idle": "2024-08-11T01:25:23.138882Z",
     "shell.execute_reply": "2024-08-11T01:25:23.138772Z",
     "shell.execute_reply.started": "2024-08-11T01:25:23.138760Z"
    }
   },
   "outputs": [],
   "source": [
    "class WavLM_1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model1D = WavLM()\n",
    "        self.n_dim = 768\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 3:\n",
    "            x = x[:, 0, :]\n",
    "        feature = self.model1D.pretrain_model(x)[self.model1D.pretrain_feat]  # (B, T, 768)\n",
    "        return feature.mean(1)\n",
    "\n",
    "    def compute_stage1(self, x):\n",
    "        if x.ndim == 3:\n",
    "            x = x[:, 0, :]\n",
    "        feat = self.model1D.pretrain_model.feature_extractor(x)\n",
    "        extract_features = self.model1D.pretrain_model.feature_extractor(x)\n",
    "        extract_features = extract_features.transpose(1, 2)\n",
    "        # 输出的extract_features其实就是输入的layer norm\n",
    "        hidden_states, extract_features = self.model1D.pretrain_model.feature_projection(extract_features)\n",
    "        return hidden_states\n",
    "\n",
    "    def compute_transformer_layers(self, hidden_states, s, e):\n",
    "        for i in range(s, e):\n",
    "            layer = self.model1D.pretrain_model.encoder.layers[i]\n",
    "            dropout_probability = np.random.uniform(0, 1)\n",
    "            skip_the_layer = (\n",
    "                self.model1D.pretrain_model.encoder.training\n",
    "                and i > 0\n",
    "                and (dropout_probability < self.model1D.pretrain_model.encoder.layerdrop)\n",
    "            )\n",
    "            if not skip_the_layer:\n",
    "                layer_outputs = layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=None,\n",
    "                    position_bias=self.position_bias,\n",
    "                    output_attentions=False,\n",
    "                    index=i,\n",
    "                )\n",
    "                hidden_states, self.position_bias = layer_outputs[:2]\n",
    "        return hidden_states\n",
    "\n",
    "    def compute_stage2(self, hidden_states):\n",
    "        position_embeddings = self.model1D.pretrain_model.encoder.pos_conv_embed(hidden_states)\n",
    "        hidden_states = hidden_states + position_embeddings\n",
    "        hidden_states = self.model1D.pretrain_model.encoder.layer_norm(hidden_states)\n",
    "        hidden_states = self.model1D.pretrain_model.encoder.dropout(hidden_states)\n",
    "        self.position_bias = None\n",
    "        hidden_states = self.compute_transformer_layers(hidden_states, 0, 3)\n",
    "        return hidden_states\n",
    "\n",
    "    def compute_stage3(self, hidden_states):\n",
    "        hidden_states = self.compute_transformer_layers(hidden_states, 3, 6)\n",
    "        return hidden_states\n",
    "\n",
    "    def compute_stage4(self, hidden_states):\n",
    "        hidden_states = self.compute_transformer_layers(hidden_states, 6, 9)\n",
    "        return hidden_states\n",
    "\n",
    "    def compute_latent_feature(self, hidden_states):\n",
    "        hidden_states = self.compute_transformer_layers(hidden_states, 9, 12)\n",
    "        hidden_states = hidden_states.mean(1)  # (B, C)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d377df-88b0-4883-859c-347e9e09c023",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T01:25:23.139501Z",
     "iopub.status.idle": "2024-08-11T01:25:23.139725Z",
     "shell.execute_reply": "2024-08-11T01:25:23.139621Z",
     "shell.execute_reply.started": "2024-08-11T01:25:23.139610Z"
    }
   },
   "outputs": [],
   "source": [
    "model1D = WavLM_1D()\n",
    "x = torch.randn(2, 1, 48000)\n",
    "hidden_states = model1D.compute_stage1(x)\n",
    "hidden_states = model1D.compute_stage2(hidden_states)\n",
    "hidden_states = model1D.compute_stage3(hidden_states)\n",
    "hidden_states = model1D.compute_stage4(hidden_states)\n",
    "hidden_states = model1D.compute_latent_feature(hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de2157-5dc2-41b9-afbb-bbf783a49999",
   "metadata": {},
   "source": [
    "经验证，上面的过程分解和直接计算的结果是一样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbad775-55dc-4ddf-af5c-55b73a028714",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T01:25:23.140576Z",
     "iopub.status.idle": "2024-08-11T01:25:23.140807Z",
     "shell.execute_reply": "2024-08-11T01:25:23.140700Z",
     "shell.execute_reply.started": "2024-08-11T01:25:23.140689Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.sum(torch.abs(hidden_states - model1D(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f1d20-2a1a-4da8-becc-b65e776260d1",
   "metadata": {},
   "source": [
    "# Feature Fusion　"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997bda7-abdf-4dcf-b34f-6d5b213ef8bf",
   "metadata": {},
   "source": [
    "## Expand：将1D wav特征转换为2D　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4bef5e56-16eb-4552-abdc-73d18d62a7ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:48:16.551456Z",
     "iopub.status.busy": "2024-08-11T08:48:16.551062Z",
     "iopub.status.idle": "2024-08-11T08:48:16.607584Z",
     "shell.execute_reply": "2024-08-11T08:48:16.606155Z",
     "shell.execute_reply.started": "2024-08-11T08:48:16.551424Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossAttention2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_dim,\n",
    "        spec_dim,\n",
    "        feature_dim,\n",
    "        num_heads=4,\n",
    "        dropout_rate=0.1,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=time_dim, out_channels=feature_dim, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=spec_dim, out_channels=feature_dim, kernel_size=1)\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def forward(self, waveform, spectrogram):\n",
    "        query = self.conv1(waveform).permute(0, 2, 3, 1)\n",
    "        key = self.conv2(spectrogram).permute(0, 2, 3, 1)\n",
    "        value = spectrogram.permute(0, 2, 3, 1)\n",
    "\n",
    "        # print(query.shape, key.shape, value.shape, torch.matmul(query, key.transpose(-2, -1)).shape)\n",
    "\n",
    "        attn_weights = self.softmax(torch.matmul(query, key.transpose(-2, -1)) / (self.feature_dim**0.5))\n",
    "        out = torch.matmul(attn_weights, value).permute(0, 3, 1, 2)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Expand(nn.Module):\n",
    "    def __init__(self, time_len=149, time_dim=768, spec_height=56, spec_width=56, spec_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_len = time_len\n",
    "        self.time_dim = time_dim\n",
    "        self.spec_height = spec_height\n",
    "        self.spec_width = spec_width\n",
    "        self.spec_dim = spec_dim\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=time_len, out_channels=spec_height * spec_width, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=time_dim, out_channels=spec_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(time_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(spec_dim)\n",
    "        self.attn = CrossAttention2D(time_dim=time_dim, spec_dim=spec_dim, feature_dim=spec_dim)\n",
    "\n",
    "    def compute_layernorm(self, feat, layer_norm):\n",
    "        feat = rearrange(feat, \"b c h w -> b h w c\")\n",
    "        feat = layer_norm(feat)\n",
    "        feat = rearrange(feat, \"b h w c -> b c h w\")\n",
    "        return feat\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.conv1(x)  # [B, spec_H * spec_W, time_dim]\n",
    "        x = rearrange(\n",
    "            x, \"b (h w) c -> b c h w\", h=self.spec_height, w=self.spec_width\n",
    "        )  ## [B, time_dim, spec_H, spec_W]\n",
    "        res = self.attn(self.compute_layernorm(x, self.layer_norm1), self.compute_layernorm(y, self.layer_norm2)) + y\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d5febbc-3d2c-4e39-9988-589e127bf58d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:48:17.613593Z",
     "iopub.status.busy": "2024-08-11T08:48:17.613124Z",
     "iopub.status.idle": "2024-08-11T08:48:17.751133Z",
     "shell.execute_reply": "2024-08-11T08:48:17.750431Z",
     "shell.execute_reply.started": "2024-08-11T08:48:17.613551Z"
    }
   },
   "outputs": [],
   "source": [
    "module = Expand()\n",
    "\n",
    "B = 3\n",
    "waveform = torch.rand((B, 149, 768))  # Replace with actual input\n",
    "spectrogram = torch.rand((B, 512, 56, 56))  # Replace with actual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e151b578-41d3-44e7-b248-ba185667232d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T08:48:18.399005Z",
     "iopub.status.busy": "2024-08-11T08:48:18.398523Z",
     "iopub.status.idle": "2024-08-11T08:48:18.476678Z",
     "shell.execute_reply": "2024-08-11T08:48:18.475571Z",
     "shell.execute_reply.started": "2024-08-11T08:48:18.398976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 56, 56])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(waveform, spectrogram).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1816f78f-d656-4736-ba1e-8abebc798903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T06:55:58.994602Z",
     "iopub.status.busy": "2024-08-11T06:55:58.994346Z",
     "iopub.status.idle": "2024-08-11T06:55:59.048555Z",
     "shell.execute_reply": "2024-08-11T06:55:59.046831Z",
     "shell.execute_reply.started": "2024-08-11T06:55:58.994577Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_dim,\n",
    "        spec_dim,\n",
    "        feature_dim,\n",
    "        num_heads=4,\n",
    "        dropout_rate=0.1,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert feature_dim % num_heads == 0, \"Feature dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.feature_dim = feature_dim\n",
    "        self.feature_dim_head = feature_dim // num_heads\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.key_linear = nn.Linear(spec_dim, feature_dim)\n",
    "        self.query_linear = nn.Linear(time_dim, feature_dim)\n",
    "        # self.value_linear = nn.Linear(spec_dim, feature_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, waveform, spectrogram):\n",
    "        batch_size = waveform.shape[0]\n",
    "\n",
    "        # Apply linear transformations\n",
    "        keys = self.key_linear(spectrogram)\n",
    "        queries = self.query_linear(waveform)\n",
    "        # values = self.value_linear(spectrogram)\n",
    "        values = spectrogram\n",
    "\n",
    "        # print(keys.shape, queries.shape, values.shape)\n",
    "        \n",
    "        # Split the last dimension into (heads, depth)\n",
    "        keys = keys.view(batch_size, -1, self.num_heads, self.feature_dim_head).permute(0, 2, 1, 3)\n",
    "        queries = queries.view(batch_size, -1, self.num_heads, self.feature_dim_head).permute(0, 2, 1, 3)\n",
    "        values = values.view(batch_size, -1, self.num_heads, self.feature_dim_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        keys = self.dropout(keys)\n",
    "        queries = self.dropout(queries)\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        score = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.feature_dim_head)\n",
    "        attn_weights = self.softmax(score)\n",
    "        out = torch.matmul(attn_weights, values).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Concatenate heads back to single head dimension\n",
    "        out = out.contiguous().view(batch_size, -1, self.feature_dim)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Expand(nn.Module):\n",
    "    def __init__(self, time_len=149, time_dim=768, spec_height=56, spec_width=56, spec_dim=512, num_heads=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_len = time_len\n",
    "        self.time_dim = time_dim\n",
    "        self.spec_height = spec_height\n",
    "        self.spec_width = spec_width\n",
    "        self.spec_dim = spec_dim\n",
    "\n",
    "        self.attn = MultiHeadCrossAttention2D(time_dim=time_dim, spec_dim=spec_dim, feature_dim=spec_dim, num_heads=num_heads)\n",
    "        self.norm_wave = nn.LayerNorm(time_dim)\n",
    "        self.norm_spec = nn.LayerNorm(spec_dim)\n",
    "\n",
    "        self.conv_wave = nn.Conv1d(in_channels=time_len, out_channels=spec_height * spec_width, kernel_size=1)\n",
    "        self.conv_spec = nn.Conv2d(in_channels=time_dim, out_channels=spec_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        self.positional_encoding_wave = nn.Parameter(torch.randn(1, spec_height * spec_width, time_dim), requires_grad=True)\n",
    "        self.positional_encoding_spec = nn.Parameter(torch.randn(1, spec_dim, spec_height, spec_width), requires_grad=True)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def compute_layernorm(self, feat, layer_norm):\n",
    "        feat = rearrange(feat, \"b c h w -> b h w c\")\n",
    "        feat = layer_norm(feat)\n",
    "        feat = rearrange(feat, \"b h w c -> b c h w\")\n",
    "        return feat\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: the waveform feature of size (B, time_len, time_dim)\n",
    "            y: the spectrogram feature of size (B, spec_dim, spec_height, spec_width)\n",
    "\n",
    "        Returns:\n",
    "            y + attn(x, y)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv_wave(x)  # [B, spec_H * spec_W, time_dim]\n",
    "        norm_x = self.norm_wave(x + self.positional_encoding_wave)  # [B, spec_H * spec_W, time_dim]\n",
    "\n",
    "        norm_y = self.compute_layernorm(y + self.positional_encoding_spec, self.norm_spec)\n",
    "        norm_y = rearrange(norm_y, \"b c h w -> b (h w) c\")\n",
    "\n",
    "        res = self.attn(norm_x, norm_y)\n",
    "        # res = norm_y\n",
    "        res = self.dropout(res)\n",
    "        res = rearrange(res, \"b (h w) c -> b c h w\", h=self.spec_height, w=self.spec_width)\n",
    "\n",
    "        return res + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7fab1382-ae81-4b4e-912b-1da88d79c4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T06:56:00.143380Z",
     "iopub.status.busy": "2024-08-11T06:56:00.142969Z",
     "iopub.status.idle": "2024-08-11T06:56:00.342861Z",
     "shell.execute_reply": "2024-08-11T06:56:00.341384Z",
     "shell.execute_reply.started": "2024-08-11T06:56:00.143342Z"
    }
   },
   "outputs": [],
   "source": [
    "module = Expand()\n",
    "\n",
    "B = 3\n",
    "waveform = torch.rand((B, 149, 768))  # Replace with actual input\n",
    "spectrogram = torch.rand((B, 512, 56, 56))  # Replace with actual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bf162d7a-b767-4e74-b0e5-304adbe91e58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T06:56:01.047581Z",
     "iopub.status.busy": "2024-08-11T06:56:01.047285Z",
     "iopub.status.idle": "2024-08-11T06:56:01.232151Z",
     "shell.execute_reply": "2024-08-11T06:56:01.231427Z",
     "shell.execute_reply.started": "2024-08-11T06:56:01.047553Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 56, 56])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module(waveform, spectrogram).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "71458de1-8f78-484d-b82c-01bd9b58254a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T06:44:23.174098Z",
     "iopub.status.busy": "2024-08-11T06:44:23.173763Z",
     "iopub.status.idle": "2024-08-11T06:44:23.372130Z",
     "shell.execute_reply": "2024-08-11T06:44:23.371190Z",
     "shell.execute_reply.started": "2024-08-11T06:44:23.174071Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 149, 768])\n"
     ]
    }
   ],
   "source": [
    "m = nn.MultiheadAttention(768, num_heads=4, batch_first=True)\n",
    "x = torch.randn(3, 149, 768)\n",
    "y = torch.randn(3, 3136, 768)\n",
    "res, _ = m(x,y, y, need_weights=False)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958e9eb-f5cf-4ab4-a5e8-401d177a65e3",
   "metadata": {},
   "source": [
    "### Squeeze：将2D spec squeeze 到1D　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9731782f-7a3d-4e3f-b0ee-1ca87985e667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T01:27:13.923325Z",
     "iopub.status.busy": "2024-08-11T01:27:13.923073Z",
     "iopub.status.idle": "2024-08-11T01:27:13.964646Z",
     "shell.execute_reply": "2024-08-11T01:27:13.963671Z",
     "shell.execute_reply.started": "2024-08-11T01:27:13.923306Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossAttention1D(nn.Module):\n",
    "    def __init__(self, time_dim, spec_dim, feature_dim):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.linear1 = nn.Linear(time_dim, feature_dim)\n",
    "        self.linear2 = nn.Linear(spec_dim, feature_dim)\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def forward(self, waveform, spectrogram):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            waveform: (B, time_len, time_dim)\n",
    "            spectrogram: (B, time_len, spec_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        key = self.linear1(waveform)  ##  (B, time_len, feature_dim)\n",
    "        query = self.linear2(spectrogram)  ##  (B, time_len, feature_dim)\n",
    "        value = waveform\n",
    "\n",
    "        attn_weights = self.softmax(torch.matmul(query, key.transpose(-2, -1)) / (self.feature_dim**0.5))\n",
    "        out = torch.matmul(attn_weights, value)  ##  (B, time_len, feature_dim)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1700f7b5-ba46-4fd5-95c6-2dfd3f122f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T01:27:14.682978Z",
     "iopub.status.busy": "2024-08-11T01:27:14.682445Z",
     "iopub.status.idle": "2024-08-11T01:27:14.719022Z",
     "shell.execute_reply": "2024-08-11T01:27:14.718153Z",
     "shell.execute_reply.started": "2024-08-11T01:27:14.682942Z"
    }
   },
   "outputs": [],
   "source": [
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, time_len=149, time_dim=768, spec_height=56, spec_width=56, spec_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_len = time_len\n",
    "        self.time_dim = time_dim\n",
    "        self.spec_height = spec_height\n",
    "        self.spec_width = spec_width\n",
    "        self.spec_dim = spec_dim\n",
    "\n",
    "        ### used to convert spec into waveform\n",
    "        self.linear = nn.Linear(spec_height * spec_width, time_len)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(time_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(spec_dim)\n",
    "        self.attn = CrossAttention1D(time_dim=time_dim, spec_dim=spec_dim, feature_dim=spec_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = rearrange(y, \"b c h w -> b c (h w)\")\n",
    "        y = self.linear(y)  ### # [B, time_len, spec_dim]\n",
    "        y = rearrange(y, \"b c l -> b l c\")\n",
    "\n",
    "        res = self.attn(self.layer_norm1(x), self.layer_norm2(y)) + x\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d95e8a60-e151-4f4e-b4ea-55eac59e500b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T01:27:15.646578Z",
     "iopub.status.busy": "2024-08-11T01:27:15.646113Z",
     "iopub.status.idle": "2024-08-11T01:27:15.784952Z",
     "shell.execute_reply": "2024-08-11T01:27:15.784431Z",
     "shell.execute_reply.started": "2024-08-11T01:27:15.646551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 149, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = Squeeze()\n",
    "\n",
    "B = 3\n",
    "waveform = torch.rand((B, 149, 768))  # Replace with actual input\n",
    "spectrogram = torch.rand((B, 512, 56, 56))  # Replace with actual input\n",
    "\n",
    "\n",
    "module(waveform, spectrogram).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1b023-83c8-4967-b39b-4f437fa9ade8",
   "metadata": {},
   "source": [
    "### 增强　"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6e457-4fa2-4262-9e68-9829906e9e7c",
   "metadata": {},
   "source": [
    "使用chatgpt改进的code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0444bee-2226-45c5-bf2a-b8fb1f554362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T01:33:13.537237Z",
     "iopub.status.busy": "2024-08-11T01:33:13.536571Z",
     "iopub.status.idle": "2024-08-11T01:33:13.609713Z",
     "shell.execute_reply": "2024-08-11T01:33:13.608648Z",
     "shell.execute_reply.started": "2024-08-11T01:33:13.537175Z"
    }
   },
   "outputs": [],
   "source": [
    "class CrossAttention1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_dim,\n",
    "        spec_dim,\n",
    "        feature_dim,\n",
    "        dropout_rate=0.1,\n",
    "        temperature=1.0,\n",
    "        num_heads=1,  # Placeholder for multi-head attention\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.linear1 = nn.Linear(time_dim, feature_dim)\n",
    "        self.linear2 = nn.Linear(spec_dim, feature_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.norm_key = nn.LayerNorm(feature_dim)\n",
    "        self.norm_query = nn.LayerNorm(feature_dim)\n",
    "\n",
    "        # Placeholder attributes for multi-head attention\n",
    "        self.num_heads = num_heads\n",
    "        self.feature_dim_head = feature_dim // num_heads\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "    def forward(self, waveform, spectrogram):\n",
    "        key = self.norm_key(self.linear1(waveform))\n",
    "        query = self.norm_query(self.linear2(spectrogram))\n",
    "        value = waveform  # Consider operation on value for multi-head attention\n",
    "\n",
    "        key = self.dropout(key)\n",
    "        query = self.dropout(query)\n",
    "\n",
    "        attn_weights = self.softmax(\n",
    "            torch.matmul(query, key.transpose(-2, -1)) / (self.temperature * math.sqrt(self.feature_dim_head))\n",
    "        )\n",
    "        out = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c4f7d-53a4-46fb-9d9e-ec5e6e1386c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T02:16:33.012825Z",
     "iopub.status.busy": "2024-08-11T02:16:33.012218Z",
     "iopub.status.idle": "2024-08-11T02:16:33.081018Z",
     "shell.execute_reply": "2024-08-11T02:16:33.079213Z",
     "shell.execute_reply.started": "2024-08-11T02:16:33.012767Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_dim,\n",
    "        spec_dim,\n",
    "        feature_dim,\n",
    "        num_heads=4,\n",
    "        dropout_rate=0.1,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert feature_dim % num_heads == 0, \"Feature dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.feature_dim = feature_dim\n",
    "        self.feature_dim_head = feature_dim // num_heads\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.key_linear = nn.Linear(time_dim, feature_dim)\n",
    "        self.query_linear = nn.Linear(spec_dim, feature_dim)\n",
    "        self.value_linear = nn.Linear(time_dim, feature_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, waveform, spectrogram):\n",
    "        batch_size = waveform.shape[0]\n",
    "\n",
    "        # Apply linear transformations\n",
    "        keys = self.key_linear(waveform)\n",
    "        queries = self.query_linear(spectrogram)\n",
    "        values = self.value_linear(waveform)\n",
    "\n",
    "        # Split the last dimension into (heads, depth)\n",
    "        keys = keys.view(batch_size, -1, self.num_heads, self.feature_dim_head).permute(0, 2, 1, 3)\n",
    "        queries = queries.view(batch_size, -1, self.num_heads, self.feature_dim_head).permute(0, 2, 1, 3)\n",
    "        values = values.view(batch_size, -1, self.num_heads, self.feature_dim_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        keys = self.dropout(keys)\n",
    "        queries = self.dropout(queries)\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        score = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.feature_dim_head)\n",
    "        attn_weights = self.softmax(score)\n",
    "        out = torch.matmul(attn_weights, values).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Concatenate heads back to single head dimension\n",
    "        out = out.contiguous().view(batch_size, -1, self.feature_dim)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, time_len=149, time_dim=768, spec_height=56, spec_width=56, spec_dim=512, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(spec_height * spec_width, time_len)\n",
    "        self.layer_norm1 = nn.LayerNorm(time_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(spec_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Add placeholders for positional encoding\n",
    "        self.positional_encoding_wave = nn.Parameter(torch.randn(1, time_len, time_dim), requires_grad=True)\n",
    "        self.positional_encoding_spec = nn.Parameter(torch.randn(1, time_len, spec_dim), requires_grad=True)\n",
    "\n",
    "        self.attn = MultiHeadCrossAttention1D(\n",
    "            time_dim=time_dim, spec_dim=spec_dim, feature_dim=time_dim, dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = rearrange(y, \"b c h w -> b c (h w)\")\n",
    "        y = self.linear(y)\n",
    "        y = rearrange(y, \"b c l -> b l c\")\n",
    "\n",
    "        # Apply positional encodings\n",
    "        norm_x = self.layer_norm1(x + self.positional_encoding_wave)\n",
    "        norm_y = self.layer_norm2(y + self.positional_encoding_spec)\n",
    "\n",
    "        res = self.attn(norm_x, norm_y)\n",
    "        res = self.dropout(res)\n",
    "        res += x  # Residual connection\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7524609-d1fb-448a-aee3-c00200a58102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T02:16:37.417546Z",
     "iopub.status.busy": "2024-08-11T02:16:37.416934Z",
     "iopub.status.idle": "2024-08-11T02:16:37.569571Z",
     "shell.execute_reply": "2024-08-11T02:16:37.568257Z",
     "shell.execute_reply.started": "2024-08-11T02:16:37.417489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 149, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = Squeeze()\n",
    "\n",
    "B = 3\n",
    "waveform = torch.rand((B, 149, 768))  # Replace with actual input\n",
    "spectrogram = torch.rand((B, 512, 56, 56))  # Replace with actual input\n",
    "\n",
    "\n",
    "module(waveform, spectrogram).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46275311-81a3-4d2e-9340-b8e88b57ee5d",
   "metadata": {},
   "source": [
    "# 重建损失　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44fc56-3334-4aeb-85e4-061b8f163495",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-11T01:25:23.147578Z",
     "iopub.status.idle": "2024-08-11T01:25:23.147808Z",
     "shell.execute_reply": "2024-08-11T01:25:23.147702Z",
     "shell.execute_reply.started": "2024-08-11T01:25:23.147690Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class WaveformToSpectrogram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WaveformToSpectrogram, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(149, 224, kernel_size=3, stride=1, padding=1)  # Adjust the channels\n",
    "        self.pool = nn.AdaptiveAvgPool1d(224)  # Adjust time dimension to match spectrogram\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # x: [B, 224, 768]\n",
    "        x = self.pool(x)  # x: [B, 224, 224]\n",
    "        return x\n",
    "\n",
    "class TimeFrequencyReconstructionLoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.module = WaveformToSpectrogram()\n",
    "        self.loss = nn.MSELoss()　\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.module(x)\n",
    "        loss = self.loss(x, y[:, 0, :, :])\n",
    "        return loss\n",
    "\n",
    "\n",
    "B = 3\n",
    "# Assuming the input waveform tensor\n",
    "waveform_input = torch.rand((B, 149, 768))  # Example input\n",
    "\n",
    "# Model and forward pass\n",
    "model = WaveformToSpectrogram()\n",
    "output = model(waveform_input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778e9eb-a8ed-4260-ab4f-756b25535d2d",
   "metadata": {},
   "source": [
    "# Gated fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "638653e0-ea41-4bb8-afdd-2df45289e2ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T07:54:16.357071Z",
     "iopub.status.busy": "2024-08-13T07:54:16.356502Z",
     "iopub.status.idle": "2024-08-13T07:54:16.396083Z",
     "shell.execute_reply": "2024-08-13T07:54:16.394764Z",
     "shell.execute_reply.started": "2024-08-13T07:54:16.357035Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GatedFusionLayer(nn.Module):\n",
    "    def __init__(self, waveform_dim, spectrogram_dim, combined_dim):\n",
    "        super(GatedFusionLayer, self).__init__()\n",
    "\n",
    "        self.proj = nn.Linear(waveform_dim, spectrogram_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(spectrogram_dim * 2, combined_dim)\n",
    "        self.gate_fc = nn.Linear(combined_dim, 2)\n",
    "\n",
    "    def forward(self, waveform_features, spectrogram_features):\n",
    "        waveform_features = self.proj(waveform_features)\n",
    "        # Flatten features if needed\n",
    "        waveform_features_flat = waveform_features.view(waveform_features.size(0), -1)\n",
    "        spectrogram_features_flat = spectrogram_features.view(spectrogram_features.size(0), -1)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([waveform_features_flat, spectrogram_features_flat], dim=-1)\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        combined_features = torch.relu(self.fc(combined_features))\n",
    "\n",
    "        # Compute gate weights\n",
    "        gate = torch.sigmoid(self.gate_fc(combined_features))\n",
    "\n",
    "        # Apply weights to the features\n",
    "        weight_waveform, weight_spectrogram = gate[:, 0], gate[:, 1]\n",
    "        weight_waveform = weight_waveform.view(-1, 1)\n",
    "        weight_spectrogram = weight_spectrogram.view(-1, 1)\n",
    "\n",
    "        weighted_waveform_features = waveform_features * weight_waveform\n",
    "        weighted_spectrogram_features = spectrogram_features * weight_spectrogram\n",
    "\n",
    "        # Fuse the features\n",
    "        fused_features = weighted_waveform_features + weighted_spectrogram_features\n",
    "\n",
    "        return fused_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8c872bdc-e811-44e7-b81f-eff72d331930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-13T07:54:22.595556Z",
     "iopub.status.busy": "2024-08-13T07:54:22.594818Z",
     "iopub.status.idle": "2024-08-13T07:54:22.691827Z",
     "shell.execute_reply": "2024-08-13T07:54:22.690547Z",
     "shell.execute_reply.started": "2024-08-13T07:54:22.595507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 768)\n",
    "y = torch.randn(3, 512)\n",
    "m = GatedFusionLayer(768, 512, 1024)\n",
    "m(x, y).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
