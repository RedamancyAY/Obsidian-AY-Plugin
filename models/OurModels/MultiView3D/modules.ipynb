{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218ea361-0220-4234-a6a4-2b1535fa3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from argparse import Namespace\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from ay2.tools import freeze_modules\n",
    "from ay2.torch.nn import LambdaFunctionModule\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a005b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ay/anaconda3/envs/audio_df/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, WavLMModel\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4851e74",
   "metadata": {},
   "source": [
    "# 1D modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924a3ca",
   "metadata": {},
   "source": [
    "We use the original WavLM as the backbone to extract general 1D features from 1D waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e142d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WavLM(nn.Module):\n",
    "    \"\"\"A PyTorch module for extracting features using the WavLM model.\n",
    "\n",
    "    This class wraps the WavLM model from the Hugging Face Transformers library and provides\n",
    "    functionality to extract either the last hidden state or the intermediate features from the model.\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        # Initialize the WavLM model\n",
    "        wavlm = WavLM(pretrain_feat=\"last_hidden_state\")\n",
    "\n",
    "        # Extract features from an input tensor\n",
    "        input_tensor = torch.randn(1, 16000)  # Example input tensor (batch_size, sequence_length)\n",
    "        features = wavlm(input_tensor)\n",
    "        ```\n",
    "\n",
    "    Args:\n",
    "        pretrain_path (str): The path to the pretrained WavLM model.\n",
    "        pretrain_feat (str): The type of feature to extract from the WavLM model.\n",
    "                                Must be either \"last_hidden_state\" or \"extract_features\".\n",
    "                                Defaults to \"last_hidden_state\". \"extract_features\" is the intermediate features from the 1D CNN, while \"last_hidden_state\" is the final output of the transformer.\n",
    "        **kwargs: Additional keyword arguments passed to the WavLMModel.from_pretrained method.\n",
    "\n",
    "    Attributes:\n",
    "        pretrain_feat (str): The type of feature to extract.\n",
    "        pretrain_model (WavLMModel): The pretrained WavLM model loaded from the specified path.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If `pretrain_feat` is not one of [\"last_hidden_state\", \"extract_features\"].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, pretrained_path:str, pretrain_feat: str = \"last_hidden_state\", **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert pretrain_feat in [\"last_hidden_state\", \"extract_features\"]\n",
    "        self.pretrain_feat = pretrain_feat\n",
    "\n",
    "        self.pretrain_model = WavLMModel.from_pretrained(pretrained_path)\n",
    "\n",
    "    def extract_feature(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract features from the input tensor using the WavLM model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The extracted features of shape (batch_size, channels, time_steps).\n",
    "        \"\"\"\n",
    "\n",
    "        if x.ndim == 3 and x.size(1) == 1:\n",
    "            _input = x[:, 0, :]\n",
    "        elif x.ndim == 2:\n",
    "            _input = x\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Input tensor for WavLM must be of shape (batch_size, sequence_length) or (batch_size, 1, sequence_length)., but got {x.shape}\"\n",
    "            )\n",
    "\n",
    "        feature = self.pretrain_model(_input)[self.pretrain_feat]\n",
    "        feature = torch.transpose(feature, 1, 2)  ## (B, T, C) -> (B, C, T)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the WavLM model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, sequence_length).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The extracted features of shape (batch_size, channels, time_steps).\n",
    "        \"\"\"\n",
    "        return self.extract_feature(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "450bd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model1D = WavLM(pretrained_path=\"/usr/local/ay_data/0-model_weights/microsoft_wavlm-base\",\n",
    "                             pretrain_feat=\"last_hidden_state\")\n",
    "        self.n_dim = 768\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        feat = self.model1D(x)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b165d6e8",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /usr/local/ay_data/0-model_weights/microsoft_wavlm-base were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at /usr/local/ay_data/0-model_weights/microsoft_wavlm-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768, 149])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_1D()\n",
    "x = torch.randn(2, 48000)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95afab",
   "metadata": {},
   "source": [
    "# 2D Modules\n",
    "\n",
    "We use the following modules as the backbone to extract general features from 2D spectrogram:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1524247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb99ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            \"hance-ai/audiomae\",\n",
    "            cache_dir=\"/usr/local/ay_data/0-model_weights\",\n",
    "            trust_remote_code=True,\n",
    "        ).encoder.to('cpu')\n",
    "        self.n_dim = 768\n",
    "\n",
    "    def extract_feature(self, x: torch.Tensor, profiler=None):\n",
    "\n",
    "        if x.ndim == 3 and x.size(1) == 1:\n",
    "            _input = x[:, 0, :]\n",
    "        elif x.ndim == 2:\n",
    "            _input = x\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Input tensor for WavLM must be of shape (batch_size, sequence_length) or (batch_size, 1, sequence_length)., but got {x.shape}\"\n",
    "            )\n",
    "\n",
    "        if profiler is None:\n",
    "            profiler = pl.profilers.PassThroughProfiler()\n",
    "\n",
    "        with profiler.profile(\"AudioMAE: generate melspec from input audio waveform\"):\n",
    "            melspec = [\n",
    "                self.model.waveform_to_melspec(_input[i][None])\n",
    "                for i in range(_input.shape[0])\n",
    "            ]\n",
    "            # (b, length, n_freq_bins) = (b, 1024, 128)\n",
    "            melspec = torch.stack(melspec, dim=0)\n",
    "            # (b, 1, length, n_freq_bins) = (b, 1, 1024, 128)\n",
    "            melspec = melspec.unsqueeze(1)\n",
    "\n",
    "        with profiler.profile(\"AudioMAE: generate spectrogram feature from encoder\"):\n",
    "            # melspec = self.model.waveform_to_melspec(x)  # (length, n_freq_bins) = (1024, 128)\n",
    "            # melspec = melspec[None,None,:,:]  # (1, 1, length, n_freq_bins) = (1, 1, 1024, 128)\n",
    "            z = self.model.forward_features(melspec)  # (b, 1+n, d); d=768\n",
    "            z = z[:, 1:, :]  # (b n d); remove [CLS], the class token\n",
    "\n",
    "        b, c, w, h = melspec.shape  # w: temporal dim; h:freq dim\n",
    "        wprime = round(\n",
    "            w / self.model.patch_embed.patch_size[0]\n",
    "        )  # width in the latent space\n",
    "        hprime = round(\n",
    "            h / self.model.patch_embed.patch_size[1]\n",
    "        )  # height in the latent space\n",
    "\n",
    "        # reconstruct the temporal and freq dims\n",
    "        z = rearrange(z, \"b (w h) c -> b c h w\", h=hprime)  # (b c h' w')\n",
    "        return z  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c1d410",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "model = AudioMAE()\n",
    "x = torch.randn(2, 48000)\n",
    "model.extract_feature(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a912103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model2D = AudioMAE()\n",
    "        self.n_dim = 768\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        feat = self.model2D.extract_feature(x)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1461aa7d",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768, 8, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_2D()\n",
    "x = torch.randn(2, 48000)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e8e63",
   "metadata": {},
   "source": [
    "# Text Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7734a5d",
   "metadata": {},
   "source": [
    "We use ASR modules to extract the text features.\n",
    "\n",
    "- Whisper from OpenAI: https://huggingface.co/models?search=openai/whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0fdcb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Text(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model1D = WavLM(pretrained_path=\"/usr/local/ay_data/0-model_weights/models--patrickvonplaten--wavlm-libri-clean-100h-base-plus\",\n",
    "                             pretrain_feat=\"last_hidden_state\")\n",
    "        self.n_dim = 768\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        feat = self.model1D(x)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b734b2",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMModel were not initialized from the model checkpoint at /usr/local/ay_data/0-model_weights/models--patrickvonplaten--wavlm-libri-clean-100h-base-plus and are newly initialized: ['wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768, 149])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_Text()\n",
    "x = torch.randn(2, 48000)\n",
    "model(x).shape"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "audio_df",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
