{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5894d87e-760a-4114-95ac-f4618f36f790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T01:41:28.109318Z",
     "iopub.status.busy": "2023-08-04T01:41:28.108743Z",
     "iopub.status.idle": "2023-08-04T01:41:28.137713Z",
     "shell.execute_reply": "2023-08-04T01:41:28.136453Z",
     "shell.execute_reply.started": "2023-08-04T01:41:28.109190Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb1b402-184a-4394-b7d2-5349758040a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T13:49:47.126957Z",
     "iopub.status.busy": "2023-08-05T13:49:47.126445Z",
     "iopub.status.idle": "2023-08-05T13:49:48.182531Z",
     "shell.execute_reply": "2023-08-05T13:49:48.181643Z",
     "shell.execute_reply.started": "2023-08-05T13:49:47.126909Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255bc8b1-462f-408d-bffb-9a8d88c07780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T01:41:33.011605Z",
     "iopub.status.busy": "2023-08-04T01:41:33.010969Z",
     "iopub.status.idle": "2023-08-04T01:41:34.018273Z",
     "shell.execute_reply": "2023-08-04T01:41:34.017465Z",
     "shell.execute_reply.started": "2023-08-04T01:41:33.011560Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from ay2.torch.deepfake_detection import DeepfakeAudioClassification\n",
    "from ay2.torch.lightning.callbacks import (\n",
    "    ACC_Callback,\n",
    "    APCallback,\n",
    "    AUC_Callback,\n",
    "    Color_progress_bar,\n",
    "    EarlyStop,\n",
    "    EER_Callback,\n",
    ")\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2bc66-9a9a-40be-be90-f984415e751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ._data import get_data\n",
    "from .wav2clip import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2475530-6746-4233-9333-75cf25519054",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb",
     "style-activity"
    ]
   },
   "outputs": [],
   "source": [
    "from wav2clip import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ecd9f5-6cd1-4a0d-a939-d48c8be8e21d",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-08-06T07:15:14.208592Z",
     "iopub.status.busy": "2023-08-06T07:15:14.207345Z",
     "iopub.status.idle": "2023-08-06T07:15:14.239414Z",
     "shell.execute_reply": "2023-08-06T07:15:14.237810Z",
     "shell.execute_reply.started": "2023-08-06T07:15:14.208540Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mOurs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from models.Ours.model import AudioModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad5caf-e1f8-4f12-8626-89f296d2e277",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d9d2bc0-f023-4a69-bbc7-c2dd52c1e328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T06:37:06.015632Z",
     "iopub.status.busy": "2023-07-26T06:37:06.014948Z",
     "iopub.status.idle": "2023-07-26T06:37:06.063869Z",
     "shell.execute_reply": "2023-07-26T06:37:06.063201Z",
     "shell.execute_reply.started": "2023-07-26T06:37:06.015584Z"
    }
   },
   "outputs": [],
   "source": [
    "class CLIPLoss1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPLoss1D, self).__init__()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.loss_image = nn.CrossEntropyLoss()\n",
    "        self.loss_text = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logit_scale * text_features @ image_features.t()\n",
    "\n",
    "        batch_size = image_features.shape[0]\n",
    "        ground_truth = torch.arange(\n",
    "            batch_size, dtype=torch.long, device=image_features.device\n",
    "        )\n",
    "        return (\n",
    "            self.loss_image(logits_per_image, ground_truth)\n",
    "            + self.loss_text(logits_per_text, ground_truth)\n",
    "        ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "23a26bb2-bab0-470a-b3a8-2008804519ef",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2023-07-26T07:23:41.166327Z",
     "iopub.status.busy": "2023-07-26T07:23:41.165800Z",
     "iopub.status.idle": "2023-07-26T07:23:41.229455Z",
     "shell.execute_reply": "2023-07-26T07:23:41.228709Z",
     "shell.execute_reply.started": "2023-07-26T07:23:41.166274Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Our_lit(DeepfakeAudioClassification):\n",
    "    def __init__(self, num_classes=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model1 = get_model()\n",
    "        self.model2 = AudioModel().feature_model\n",
    "        dim = self.model2.dims[-1]\n",
    "        self.mlp1 = nn.Sequential(\n",
    "                    nn.Linear(512, 512),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(512, dim),\n",
    "                )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "                    nn.Linear(dim, 512),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(512, 512),\n",
    "                )\n",
    "\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.class_head1 = nn.Linear(512, 309)\n",
    "        self.class_head2 = nn.Linear(dim, 309)\n",
    "\n",
    "        self.loss_fn = CLIPLoss1D()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def calcuate_loss(self, batch_res, batch):\n",
    "        res = {}\n",
    "        res[\"loss1\"] = self.loss_fn(\n",
    "            self.mlp1(batch_res[\"feat_org\"]), batch_res[\"feat_tar\"]\n",
    "        )\n",
    "        res[\"loss2\"] = self.loss_fn(\n",
    "            batch_res[\"feat_org\"], self.mlp2(batch_res[\"feat_tar\"])\n",
    "        )\n",
    "        res[\"ce_loss1\"] = self.ce_loss(batch_res[\"org_logit\"], batch[\"label\"])\n",
    "        res[\"ce_loss2\"] = self.ce_loss(batch_res[\"tar_logit\"], batch[\"label\"])\n",
    "        # return (loss1 + loss2) / 2\n",
    "        self.log_dict(res, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return (res[\"loss1\"] + res[\"loss2\"] + res[\"ce_loss1\"] + res[\"ce_loss2\"]) / 4\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        import itertools\n",
    "        optimizer = torch.optim.Adam(\n",
    "            itertools.chain(\n",
    "                self.model1.parameters(),\n",
    "                self.model2.parameters(),\n",
    "                self.mlp1.parameters(),\n",
    "                self.mlp2.parameters(),\n",
    "                self.class_head1.parameters(),\n",
    "                self.class_head2.parameters(),\n",
    "            ),\n",
    "            lr=0.0001, weight_decay=0.0001\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=0.8,\n",
    "            patience=3,\n",
    "            threshold=0.001,\n",
    "            threshold_mode=\"abs\",\n",
    "            min_lr=0.000001,\n",
    "            eps=1e-08,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val-loss\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _shared_pred(self, batch, batch_idx):\n",
    "        audio, sample_rate = batch[\"audio\"], batch[\"sample_rate\"]\n",
    "        # if len(audio.shape) == 3:\n",
    "        # audio = audio[:, 0, :]\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        feature_org = self.model1(audio[:, 0, :])\n",
    "\n",
    "        feature_tar = self.model2(audio)\n",
    "\n",
    "        res = {\"feat_org\": feature_org, \"feat_tar\": feature_tar}\n",
    "        res[\"org_logit\"] = self.class_head1(self.drop(feature_org))\n",
    "        res[\"tar_logit\"] = self.class_head2(self.drop(feature_tar))\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cbf711-bd0a-4338-a78d-d76b89b6592f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T01:06:40.852132Z",
     "iopub.status.busy": "2023-07-27T01:06:40.851539Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_callbacks():\n",
    "    callbacks = [\n",
    "        Color_progress_bar(),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=None,\n",
    "            save_top_k=1,\n",
    "            monitor=\"val-loss\",\n",
    "            mode=\"min\",\n",
    "            save_last=True,\n",
    "            filename=\"best-{epoch}-{val-loss:.2f}\",\n",
    "        ),\n",
    "        EarlyStop(\n",
    "            min_epochs=50,\n",
    "            monitor=\"val-loss\",\n",
    "            min_delta=0.001,\n",
    "            patience=5,\n",
    "            mode=\"min\",\n",
    "            stopping_threshold=0.1,\n",
    "            verbose=False,\n",
    "        ),\n",
    "        ACC_Callback(\n",
    "            batch_key=\"label\", output_key=\"org_logit\", num_classes=309, theme=\"org\"\n",
    "        ),\n",
    "        AUC_Callback(\n",
    "            batch_key=\"label\", output_key=\"org_logit\", num_classes=309, theme=\"org\"\n",
    "        ),\n",
    "        ACC_Callback(\n",
    "            batch_key=\"label\", output_key=\"tar_logit\", num_classes=309, theme=\"tar\"\n",
    "        ),\n",
    "        AUC_Callback(\n",
    "            batch_key=\"label\", output_key=\"tar_logit\", num_classes=309, theme=\"tar\"\n",
    "        ),\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a95f9-0247-42c5-84c8-fe9667591168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T02:41:03.975062Z",
     "iopub.status.busy": "2023-07-27T02:41:03.974469Z",
     "iopub.status.idle": "2023-07-27T02:41:04.122873Z",
     "shell.execute_reply": "2023-07-27T02:41:04.121515Z",
     "shell.execute_reply.started": "2023-07-27T02:41:03.975014Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/ay/data/DATA/1-model_save/0-Audio\"\n",
    "\n",
    "\n",
    "def start_distillation(gpu):\n",
    "    dl = get_data()\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=300,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[gpu],\n",
    "        logger=pl.loggers.CSVLogger(\n",
    "            ROOT_DIR,\n",
    "            name=\"distillation\",\n",
    "            version=None,\n",
    "        ),\n",
    "        check_val_every_n_epoch=1,\n",
    "        callbacks=make_callbacks(),\n",
    "        default_root_dir=ROOT_DIR,\n",
    "    )\n",
    "    model = Our_lit()\n",
    "\n",
    "    trainer.fit(model, dl.train, val_dataloaders=dl.val)\n",
    "\n",
    "    torch.save(model.model2.state_dict(), trainer.logger.log_dir + \"/model2.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
