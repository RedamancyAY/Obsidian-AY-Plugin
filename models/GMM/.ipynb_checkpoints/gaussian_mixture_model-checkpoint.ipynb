{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961bfcb3-bfa0-4702-886c-0260004e4c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T08:51:37.201119Z",
     "iopub.status.busy": "2023-06-20T08:51:37.200669Z",
     "iopub.status.idle": "2023-06-20T08:51:38.744236Z",
     "shell.execute_reply": "2023-06-20T08:51:38.743667Z",
     "shell.execute_reply.started": "2023-06-20T08:51:37.201091Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Type\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e6748-8e87-4688-a8b8-17ee91c5b142",
   "metadata": {},
   "source": [
    "# GMM models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f3d6be-2023-452a-9610-10adf4338b0b",
   "metadata": {},
   "source": [
    "## Excpetions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4895a2f4-32ce-4401-a1db-a0103d777e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T08:51:41.750921Z",
     "iopub.status.busy": "2023-06-20T08:51:41.750397Z",
     "iopub.status.idle": "2023-06-20T08:51:41.757033Z",
     "shell.execute_reply": "2023-06-20T08:51:41.756181Z",
     "shell.execute_reply.started": "2023-06-20T08:51:41.750894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GMMException(Exception):\n",
    "    \"\"\"Base class for GMM exceptions.\"\"\"\n",
    "\n",
    "\n",
    "class NotFittedException(GMMException):\n",
    "    \"\"\"Model not fitted.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Model not fitted, please call fit on data before inference!\")\n",
    "\n",
    "\n",
    "class UnsupportedCovarinaceType(GMMException):\n",
    "    \"\"\"Methods for covarinace estimation not supported.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Selected invalid method for estimating covarinace matrix!\")\n",
    "\n",
    "\n",
    "class UnsupportedInitilazationMethod(GMMException):\n",
    "    \"\"\"Methods for initializing Gaussian centers not supported.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Selected invalid method for initial Gaussian centers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f8278-7cfa-4997-80e1-2ead7cec1a0f",
   "metadata": {},
   "source": [
    "## GMM Model\n",
    "\n",
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996381fd-da88-436d-8a11-7c86b1e48811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T08:51:44.096909Z",
     "iopub.status.busy": "2023-06-20T08:51:44.095985Z",
     "iopub.status.idle": "2023-06-20T08:51:44.117468Z",
     "shell.execute_reply": "2023-06-20T08:51:44.116619Z",
     "shell.execute_reply.started": "2023-06-20T08:51:44.096861Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GMMBase(nn.Module):\n",
    "    \"\"\"Base class for Gaussian Mixture Models (GMM).\n",
    "\n",
    "    Args:\n",
    "        k (int): The number of mixture components.\n",
    "        data (torch.Tensor): Data to initialize means from. Must provide more data points than clusters!\n",
    "        loc (torch.Tensor): The means of the Gaussian distributions.\n",
    "        cov (torch.Tensor): The covariance matrices of the Gaussian distributions.\n",
    "        covariance_type (str): Which covariance type to learn? Options: {full, diag}. Default: full.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            k: int,\n",
    "            data: torch.Tensor,\n",
    "            covariance_type: str = \"full\",\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the GMM model.\n",
    "\n",
    "        Args:\n",
    "            See class description.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.covariance_type = covariance_type\n",
    "\n",
    "        self._mix: D.distribution = None\n",
    "        self._comp: D.distribution = None\n",
    "\n",
    "        self._eps = torch.finfo(torch.float32).eps\n",
    "        self._fitted = False\n",
    "        self._initalize(data)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute the weighted log probabilities for each sample.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Data matrix (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            log_prob (torch.Tensor): Log probabilities of each data point in X. (n_sample, )\n",
    "        \"\"\"\n",
    "        if not self._fitted:\n",
    "            raise NotFittedException()\n",
    "\n",
    "        weighted_log_prob = self._comp.log_prob(X.unsqueeze(\n",
    "            1)) + torch.log_softmax(self._mix.logits, dim=-1)\n",
    "        return torch.logsumexp(weighted_log_prob, dim=1, keepdim=True)\n",
    "\n",
    "    def load_state_dict(self, *args, **kwargs):\n",
    "        super().load_state_dict(*args, **kwargs)\n",
    "\n",
    "        # only loc and cov are stored in the state dict, thus we have to build distributions afterwards\n",
    "        self._build_distributions()\n",
    "        self._fitted = True\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        self = super().to(*args, **kwargs)\n",
    "        self._build_distributions()\n",
    "        return self\n",
    "\n",
    "    def cuda(self, *args, **kwargs):\n",
    "        self = super().cuda(*args, **kwargs)\n",
    "        self._build_distributions()\n",
    "        return self\n",
    "\n",
    "    def cpu(self, *args, **kwargs):\n",
    "        self = super().cpu(*args, **kwargs)\n",
    "        self._build_distributions()\n",
    "        return self\n",
    "\n",
    "    def _build_distributions(self):\n",
    "        raise NotImplementedError(\n",
    "            \"_build_distributions must be implemented by subclass!\")\n",
    "\n",
    "    def _initalize(self, data: torch.Tensor):\n",
    "        # equal prior distribution\n",
    "        d = data.size(1)\n",
    "        pi = torch.full(fill_value=(1. / self.k),\n",
    "                        size=[self.k, ])\n",
    "\n",
    "        # draw from Gaussian distribution\n",
    "        loc = torch.randn(self.k, d)\n",
    "        prob = torch.ones(len(data)) / len(data)\n",
    "        loc = data[torch.multinomial(prob, num_samples=self.k)]\n",
    "\n",
    "        # simple covariance matrix\n",
    "        if self.covariance_type == \"full\":\n",
    "            cov = torch.stack([torch.eye(d)\n",
    "                               for _ in range(self.k)])\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            cov = torch.stack([torch.ones(d)\n",
    "                               for _ in range(self.k)])\n",
    "\n",
    "        self._initalize_parameters(pi, loc, cov)\n",
    "        self._build_distributions()\n",
    "\n",
    "    def _initalize_parameters(self, pi: torch.Tensor, loc: torch.Tensor, cov: torch.Tensor) -> None:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d5c3e2-4cd6-4e83-b167-b39900395562",
   "metadata": {},
   "source": [
    "### GMM EM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c71ea-f8b4-4cb0-a2ee-7d39c75d394e",
   "metadata": {},
   "source": [
    "train the model using the EM-Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f6ce4-420e-41fc-80bf-b17b286d64e9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GMMEM(GMMBase):\n",
    "    \"\"\"A Gaussian Mixture Model.\n",
    "\n",
    "    The model must be trained by calling the fit function.\n",
    "    It is implemented as a torch.nn.Module, so it can be easily used for creating adversarial\n",
    "    examples or gradient based attribution.\n",
    "\n",
    "    Note, in contrast to sklearn, we do not run full k-mean for initialization.\n",
    "    Instead we use the initilazation procedure from k-means++.\n",
    "\n",
    "    Args:\n",
    "        k (int): The number of mixture components.\n",
    "        data (torch.Tensor): Data to initialize means from.\n",
    "        d (int): Size of feature dimension.\n",
    "        loc (torch.Tensor): The means of the Gaussian distributions.\n",
    "        cov (torch.Tensor): The covariance matrices of the Gaussian distributions.\n",
    "        max_iter(int): Maximum amount of EM steps.\n",
    "        tol (float): The convergence threshold. EM iterations will stop when the\n",
    "            lower bound average gain is below this threshold.\n",
    "        covariance_type (str): Which covariance type to learn? Options: {full, diag}. Default: full.\n",
    "        training_runs (int): Amount of retraining the network, returns the network with the highest lower bound.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            k: int,\n",
    "            data: torch.Tensor,\n",
    "            max_iter: int = 10,\n",
    "            tol: float = 1e-3,\n",
    "            covariance_type: str = \"full\",\n",
    "            training_runs: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the GMM model.\n",
    "\n",
    "        Args:\n",
    "            See class description.\n",
    "        \"\"\"\n",
    "        super().__init__(k, data, covariance_type)\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.training_runs = training_runs\n",
    "        self._lower_bounds = []\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"Fit the model using the EM-Algorithm.\n",
    "\n",
    "        Args:\n",
    "            data (torch.Tensor): Data matrix (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            self (GMM): The fitted instance.\n",
    "        \"\"\"\n",
    "        self._fitted = True\n",
    "\n",
    "        best_params = (self.pi.clone(),  self.loc.clone(), self.cov.clone())\n",
    "        best_lower_bound = None\n",
    "\n",
    "        LOGGER.debug(f\"Starting training...\")\n",
    "        for i in range(1, self.training_runs + 1):\n",
    "            # reinitalize model each run\n",
    "            self._initalize(data)\n",
    "\n",
    "            LOGGER.debug(f\"Starting training run {i:03}...\")\n",
    "            counter = 0\n",
    "            prev_lower_bound = None\n",
    "\n",
    "            while True:\n",
    "                prev_loc = self.loc.clone()\n",
    "                prev_cov = self.cov.clone()\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "                # compute responsibilities\n",
    "                log_resp = self._compute_responsibilites(data)\n",
    "\n",
    "                # compute helper term Nk\n",
    "                resp = log_resp.exp()\n",
    "                Nk = torch.sum(resp, dim=0)  # sum over all points\n",
    "\n",
    "                # update means\n",
    "                self.loc = torch.matmul(resp.T, data)\n",
    "                self.loc = self.loc / (Nk.unsqueeze(1) + self._eps)\n",
    "\n",
    "                # update cov\n",
    "                if self.covariance_type == \"full\":\n",
    "                    cov = self._update_covariance_full(data, resp, Nk)\n",
    "                elif self.covariance_type == \"diag\":\n",
    "                    cov = self._update_covariance_diag(data, resp, Nk)\n",
    "                else:\n",
    "                    raise UnsupportedCovarinaceType()\n",
    "\n",
    "                self.cov = cov\n",
    "\n",
    "                self.pi = Nk / len(data)\n",
    "\n",
    "                # update distributions\n",
    "                self._build_distributions()\n",
    "\n",
    "                lower_bound = self.forward(data).mean()\n",
    "                if prev_lower_bound is None:\n",
    "                    prev_lower_bound = lower_bound\n",
    "                    continue\n",
    "\n",
    "                self._lower_bounds.append(lower_bound)\n",
    "                change = lower_bound - prev_lower_bound\n",
    "                if torch.abs(change) < self.tol or counter >= self.max_iter:\n",
    "                    break\n",
    "\n",
    "                prev_lower_bound = lower_bound\n",
    "\n",
    "                LOGGER.debug(\n",
    "                    f\"Finished {counter: 3} iteration(s): Current lower bound: {lower_bound}!\")\n",
    "\n",
    "            LOGGER.debug(\n",
    "                f\"Finished run {i:04} after {counter} iterations (maximum: {self.max_iter})!\")\n",
    "\n",
    "            if best_lower_bound is None or lower_bound > best_lower_bound:\n",
    "                best_lower_bound = lower_bound\n",
    "                best_params = (self.pi.clone(),\n",
    "                               self.loc.clone(), self.cov.clone())\n",
    "\n",
    "        # restore best model\n",
    "        self.pi, self.loc, self.cov = best_params\n",
    "        self._build_distributions()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _initalize_parameters(self, pi, loc, cov):\n",
    "        self.register_buffer(\"pi\", pi)\n",
    "        self.register_buffer(\"loc\", loc)\n",
    "        self.register_buffer(\"cov\", cov)\n",
    "\n",
    "    def _compute_responsibilites(self, data):\n",
    "        weighted_log_prob = self._comp.log_prob(data.unsqueeze(\n",
    "            1)) + torch.log_softmax(self._mix.logits, dim=-1)\n",
    "        log_prob_norm = torch.logsumexp(weighted_log_prob, dim=1, keepdim=True)\n",
    "\n",
    "        return weighted_log_prob - log_prob_norm\n",
    "\n",
    "    def _update_covariance_full(self, data, resp, Nk):\n",
    "        cov = torch.zeros_like(self.cov)\n",
    "\n",
    "        # TODO: updateme when torch.vmap is a thing\n",
    "        for i in range(self.k):\n",
    "            respk = resp[:, i]\n",
    "            nk = Nk[i]\n",
    "\n",
    "            # update covariances\n",
    "            diff = data - self.loc[i]\n",
    "            cov[i] = torch.matmul(respk.unsqueeze(\n",
    "                0) * diff.T, diff) / (nk + self._eps)\n",
    "\n",
    "        return cov\n",
    "\n",
    "    def _update_covariance_diag(self, data, resp, Nk):\n",
    "        # calculate diagonal vectors\n",
    "        avg_X2 = torch.matmul(resp.T, data * data)\n",
    "        avg_X2 = avg_X2 / (Nk.unsqueeze(1) + self._eps)\n",
    "        avg_means2 = self.loc ** 2\n",
    "        avg_X_means = self.loc * \\\n",
    "            torch.matmul(resp.T, data) / (Nk.unsqueeze(1) + self._eps)\n",
    "        cov_diags = avg_X2 - 2 * avg_X_means + avg_means2\n",
    "\n",
    "        return cov_diags\n",
    "\n",
    "    def _build_distributions(self):\n",
    "        # create mutlivariate gaussian\n",
    "        if self.covariance_type == \"full\":\n",
    "            cov = self.cov\n",
    "            self._comp = D.MultivariateNormal(self.loc, cov)\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            cov = torch.stack([torch.diag(c) for c in self.cov])\n",
    "            self._comp = D.MultivariateNormal(self.loc, cov)\n",
    "        else:\n",
    "            raise UnsupportedCovarinaceType()\n",
    "\n",
    "        # create mixing weights\n",
    "        self._mix = D.Categorical(self.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82976b7-143e-42ec-901a-ef002913aae0",
   "metadata": {},
   "source": [
    "### GMM Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "232f4387-2687-4eca-8017-e85020dd79ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T08:51:50.610614Z",
     "iopub.status.busy": "2023-06-20T08:51:50.610029Z",
     "iopub.status.idle": "2023-06-20T08:51:50.622319Z",
     "shell.execute_reply": "2023-06-20T08:51:50.621507Z",
     "shell.execute_reply.started": "2023-06-20T08:51:50.610577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GMMDescent(GMMBase):\n",
    "    \"\"\"A Gaussian Mixture Model.\n",
    "\n",
    "    The model is designed to be trained by gradient descent, minimizing the negative log likelihood of the data.\n",
    "\n",
    "    Args:\n",
    "        k (int): The number of mixture components.\n",
    "        data (torch.Tensor): Data to initialize means from.\n",
    "        loc (torch.Tensor): The means of the Gaussian distributions.\n",
    "        cov (torch.Tensor): The covariance matrices of the Gaussian distributions.\n",
    "        max_iter(int): Maximum amount of EM steps.\n",
    "        tol (float): The convergence threshold. EM iterations will stop when the\n",
    "            lower bound average gain is below this threshold.\n",
    "        covariance_type (str): Which covariance type to learn? Options: {full, diag}. Default: full.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k: int, data: torch.Tensor, covariance_type: str = \"full\"):\n",
    "        \"\"\"Initialize the GMM model.\n",
    "\n",
    "        Args:\n",
    "            See class description.\n",
    "        \"\"\"\n",
    "        super().__init__(k, data, covariance_type)\n",
    "\n",
    "    def _initalize_parameters(self, pi: torch.Tensor, loc: torch.Tensor, cov: torch.Tensor):\n",
    "        # resize and copy inplace\n",
    "        self.pi = torch.nn.Parameter(pi)\n",
    "        self.loc = torch.nn.Parameter(loc)\n",
    "\n",
    "        # keep diagonal in log space\n",
    "        if self.covariance_type == \"diag\":\n",
    "            # obtain cholesky decomposition of the diagonal\n",
    "            # convert to diag matrix, obtain decomposition, only keep diagonal\n",
    "            before = cov.shape\n",
    "            cov = torch.stack([torch.diagonal(torch.cholesky(torch.diag(cov[i])))\n",
    "                               for i in range(self.k)])\n",
    "\n",
    "            assert cov.shape == before\n",
    "\n",
    "            # keep in log space\n",
    "            cov = torch.log(cov)\n",
    "        self.cov = torch.nn.Parameter(cov)\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "    def _build_distributions(self):\n",
    "        # create mutlivariate gaussian\n",
    "        if self.covariance_type == \"full\":\n",
    "            cov = self.cov\n",
    "            self._comp = D.MultivariateNormal(self.loc, cov)\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            # since we keep diagonal in log space\n",
    "            cov = torch.stack([torch.diag(torch.exp(c)) for c in self.cov])\n",
    "\n",
    "            # we use tril matrix\n",
    "            self._comp = D.MultivariateNormal(self.loc, scale_tril=cov)\n",
    "        else:\n",
    "            raise UnsupportedCovarinaceType()\n",
    "\n",
    "        # create mixing weights\n",
    "        self._mix = D.Categorical(F.softmax(self.pi, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5001b4a-d1cf-4c2c-b09b-ebcb0ab3aad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-20T08:52:13.088105Z",
     "iopub.status.busy": "2023-06-20T08:52:13.087326Z",
     "iopub.status.idle": "2023-06-20T08:52:13.119864Z",
     "shell.execute_reply": "2023-06-20T08:52:13.119277Z",
     "shell.execute_reply.started": "2023-06-20T08:52:13.088058Z"
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30])\n",
      "torch.Size([30, 128])\n",
      "torch.Size([30, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "model = GMMDescent(k=30, data = torch.rand(35, 128))\n",
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
